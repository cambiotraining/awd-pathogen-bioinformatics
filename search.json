[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Bioinformatics for AWD-related Pathogens",
    "section": "Overview",
    "text": "Overview\nAccording to the World Health Organisation (WHO), outbreaks of acute watery diarrhea (AWD) related diseases are likely to occur, unless surveillance systems are in place to rapidly detect their associated pathogen(s) and respond accordingly with public health measures. These surveillance systems can also help to determine the source of transmission, ensure implementation of control measures in the affected area and determine the microbial etiology associated with the outbreak.\nThese materials cover how genome analysis can be used for pathogen surveillance, detailing the bioinformatic analysis workflow to go from raw sequencing data, to the assembly of bacterial genomes, identification of pathogenic strains and screening for antibiotic resistance genes. We use cholera as a case study, however the tools and concepts covered also apply to other bacterial pathogens.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nDescribe how genome sequencing data can be used in the surveillance of bacterial pathogens.\nUnderstand how sequencing data is generated and the most common file formats and conventions used in the bioinformatics field.\nUse the command line to run software tools for the bioinformatic analysis of sequencing data.\nPerform bacterial genome assembly from Oxford Nanopore Techologies (ONT) sequencing data.\nCharacterise the assembled genomes by identifying strains and lineages, phylogeny and presence of antibiotic resistance genes.\nProduce a report summarising the main findings of the analysis, to use for public health decisions.\n\n\n\n\nTarget Audience\nThis course is primarily aimed at public health officials including doctors, lab workers and clinicians who work with waterborne or foodborne diseases (in particular cholera) and would like to get started in using genomic and bioinformatics approaches for the surveillance of their causative bacterial pathogens. We assume little or no prior experience in bioinformatics.\n\n\nPrerequisites\n\nBasic understanding of microbiology.\nA working knowledge of the UNIX command line will be advantageous, but not required as we will give a brief introduction as part of the course."
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Bioinformatics for AWD-related Pathogens",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nBajuna Salehe \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - original content; conceptualisation; coding; data curation\nHugo Tavares  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - original content; conceptualisation; coding; data curation\nAngelika Kritz\nAffiliation: New Variant Assessment Platform, UK Health Security Agency\nRoles: writing - review; conceptualisation; code review\nSam Sims\nAffiliation: New Variant Assessment Platform, UK Health Security Agency\nRoles: coding; code review\nAntoine Abou Fayad\nAffiliation: American University of Beirut\nRoles: resources\nLuke William Meredith\nAffiliation: World Health Organisation (EMRO)\nRoles: project administration; funding acquisition; resources\nMatt Castle\nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: project administration; funding acquisition; resources\nBabak Afrough\nAffiliation: New Variant Assessment Platform, UK Health Security Agency\nRoles: project administration; funding acquisition; resources"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Bioinformatics for AWD-related Pathogens",
    "section": "Citation",
    "text": "Citation\n\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in Salehe & Tavares et al. (2023).”.\n\nYou can cite these materials as:\n\nSalehe B, Tavares H, Kritz A, Sims S, Fayad AA, Meredith LW, Castle M & Afrough B (2023) “cambiotraining/awd-pathogen-bioinformatics: Bioinformatics for AWD-related Pathogens”, https://cambiotraining.github.io/awd-pathogen-bioinformatics"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bioinformatics for AWD-related Pathogens",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nWe thank Andries van Tonder (Department of Veterinary Medicine, University of Cambridge), Katy Brown (Department of Pathology, University of Cambridge) and Sebastian Bruchmann (Department of Medicine, University of Cambridge) for critical discussions and advice on these materials, as well as their work as lead trainers in live workshops."
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "2  Data & Setup",
    "section": "Software",
    "text": "Software\n\nInstall Linux\n\nUbuntuWindows WSLVirtual machine\n\n\nThe recommendation for bioinformatic analysis is to have a dedicated computer running a Linux distribution. The kind of distribution you choose is not critical, but we recommend Ubuntu if you are unsure.\nYou can follow the installation tutorial on the Ubuntu webpage.\n\n\n\n\n\n\nWarning\n\n\n\nInstalling Ubuntu on the computer will remove any other operating system you had previously installed, and can lead to data loss.\n\n\n\n\nThe Windows Subsystem for Linux (WSL2) runs a compiled version of Ubuntu natively on Windows.\nThere are detailed instructions on how to install WSL on the Microsoft documentation page. But briefly:\n\nClick the Windows key and search for Windows PowerShell, right-click on the app and choose Run as administrator.\nAnswer “Yes” when it asks if you want the App to make changes on your computer.\nA terminal will open; run the command: wsl --install.\n\nThis should start installing “ubuntu”.\nIt may ask for you to restart your computer.\n\nAfter restart, click the Windows key and search for Ubuntu, click on the App and it should open a new terminal.\nFollow the instructions to create a username and password (you can use the same username and password that you have on Windows, or a different one - it’s your choice).\nYou should now have access to a Ubuntu Linux terminal. This (mostly) behaves like a regular Ubuntu terminal, and you can install apps using the sudo apt install command as usual.\n\nAfter WSL is installed, it is useful to create shortcuts to your files on Windows. Your C:\\ drive is located in /mnt/c/ (equally, other drives will be available based on their letter). For example, your desktop will be located in: /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Desktop/. It may be convenient to set shortcuts to commonly-used directories, which you can do using symbolic links, for example:\n\nDocuments: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Documents/ ~/Documents\n\nIf you use OneDrive to save your documents, use: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/OneDrive/Documents/ ~/Documents\n\nDesktop: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Desktop/ ~/Desktop\nDownloads: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Downloads/ ~/Downloads\n\n\n\nAnother way to run Linux within Windows (or macOS) is to install a Virtual Machine. However, this is mostly suitable for practicing and not suitable for real data analysis.\nDetailed instructions to install an Ubuntu VM using Oracle’s Virtual Box is available from the Ubuntu documentation page.\nNote: In the step configuring “Virtual Hard Disk” make sure to assign a large storage partition (at least 100GB).\n\n\n\n\nUpdate Ubuntu\nAfter installing Ubuntu (through either of the methods above), open a terminal and run the following commands to update your system and install some essential packages:\nsudo apt update && sudo apt upgrade -y && sudo apt autoremove -y\nsudo apt install -y git\nsudo apt install -y default-jre\n\n\n\nConda/Mamba\nWe recommend using the Conda package manager to install your software. In particular, the newest implementation called Mamba.\nTo install Mamba, run the following commands from the terminal:\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\nbash Mambaforge-$(uname)-$(uname -m).sh -b\nrm Mambaforge-$(uname)-$(uname -m).sh\nRestart your terminal (or open a new one) and confirm that your shell now starts with the word (base). Then run the following commands:\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge\nconda config --set remote_read_timeout_secs 1000\n\n\nSoftware environments\nDue to the complexities of the different tools we will use, there are several software dependency incompatibilities between them. Therefore, rather than creating a single software environment with all the tools, we will create separate environments for different applications.\n\nMash\nmamba create -y -n mash mash\n\n\nAssembly\nmamba create -y -n assembly flye rasusa bakta medaka\n\n\n\nCheckM2\nmamba create -y -n checkm2 checkm2\n\n\nTyping\nmamba create -y -n typing mlst perl blast\n\n\nPhylogeny\nmamba create -y -n phylogeny panaroo iqtree figtree snp-sites\n\n\nNextflow\nmamba create -y -n nextflow nextflow\nAlso run these commands to set Nextflow correctly (copy/paste this entire code):\nmkdir -p $HOME/.nextflow\necho \"\nconda {\n  conda.enabled = true\n  singularity.enabled = false\n  docker.enabled = false\n  useMamba = true\n  createTimeout = '4 h'\n  cacheDir = \\\"$HOME/.nextflow-conda-cache/\\\"\n}\nsingularity {\n  singularity.enabled = true\n  conda.enabled = false\n  docker.enabled = false\n  pullTimeout = '4 h'\n  cacheDir = \\\"$HOME/.nextflow-singularity-cache/\\\"\n}\ndocker {\n  docker.enabled = true\n  singularity.enabled = false\n  conda.enabled = false\n}\n\" &gt;&gt; $HOME/.nextflow/config\n\n\n\nBandage\nGenerally, this software does not require installation, it can be simply downloaded from the website, unzipped and run. However, we provide command-line instructions which will place the executable on the Desktop for easy access.\n\nUbuntuWindows WSLVirtual machine\n\n\nFrom the command line:\n# install dependencies\nsudo apt-get install -y qt5-default\n\n# download the executable\nwget -O bandage.zip \"https://github.com/rrwick/Bandage/releases/download/v0.8.1/Bandage_Ubuntu_dynamic_v0_8_1.zip\"\nunzip bandage.zip -d bandage\nmv bandage/Bandage ~/Desktop/\nrm -r bandage.zip bandage\n\n\nFrom the WSL command line:\nwget -O bandage.zip \"https://github.com/rrwick/Bandage/releases/download/v0.8.1/Bandage_Windows_v0_8_1.zip\"\nunzip bandage.zip -d bandage\nmv bandage/Bandage ~/Desktop/\nrm -r bandage.zip bandage\n\n\nYou can follow the same instructions as for “Ubuntu”.\n\n\n\n\n\nSingularity\nWe recommend that you install Singularity and use the -profile singularity option when running Nextflow pipelines. On Ubuntu/WSL2, you can install Singularity using the following commands:\nsudo apt install -y runc cryptsetup-bin uidmap\nCODENAME=$(lsb_release -cs)\nwget -O singularity.deb https://github.com/sylabs/singularity/releases/download/v3.11.4/singularity-ce_3.11.4-${CODENAME}_amd64.deb\nsudo dpkg -i singularity.deb\nrm singularity.deb\nIf you have a different Linux distribution, you can find more detailed instructions on the Singularity documentation page.\nIf you have issues running Nextflow pipelines with Singularity, then you can follow the instructions below for Docker instead.\n\n\nDocker\nAn alternative for software management when running Nextflow pipelines is to use Docker.\n\nUbuntuWindows WSLVirtual machine\n\n\nFor Ubuntu Linux, here are the installation instructions:\nsudo apt install curl\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh ./get-docker.sh\nsudo groupadd docker\nsudo usermod -aG docker $USER\nAfter the last step, you will need to restart your computer. From now on, you can use -profile docker when you run Nextflow.\n\n\nWhen using WSL2 on Windows, running Nextflow pipelines with -profile singularity sometimes doesn’t work.\nAs an alternative you can instead use Docker, which is another software containerisation solution. To set this up, you can follow the full instructions given on the Microsoft Documentation: Get started with Docker remote containers on WSL 2.\nWe briefly summarise the instructions here (but check that page for details and images):\n\nDownload Docker for Windows.\nRun the installer and install accepting default options.\nRestart the computer.\nOpen Docker and go to Settings &gt; General to tick “Use the WSL 2 based engine”.\nGo to Settings &gt; Resources &gt; WSL Integration to enable your Ubuntu WSL installation.\n\nOnce you have Docker set and installed, you can use -profile docker when running your Nextflow command.\n\n\nYou can follow the same instructions as for “Ubuntu”.\n\n\n\n\n\nVisual Studio Code\n\nUbuntuWindows WSLVirtual machine\n\n\n\nGo to the Visual Studio Code download page and download the installer for your Linux distribution. Install the package using your system’s installer.\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for your operating system. Double-click the downloaded file to install the software, accepting all the default options.\nAfter completing the installation, go to your Windows Menu, search for “Visual Studio Code” and launch the application.\nGo to File &gt; Preferences &gt; Settings, then select Text Editor &gt; Files on the drop-down menu on the left. Scroll down to the section named “EOL” and choose “\\n” (this will ensure that the files you edit on Windows are compatible with the Linux operating system).\nClick Ctrl + Shift + X, which will open an “Extensions” panel on the left.\nSearch for “WSL” and click “Install”.\n\nFrom now on, you can open VS code directly from a WSL terminal by typing code ..\n\n\nYou can follow the same instructions as for “Ubuntu”."
  },
  {
    "objectID": "setup.html#data",
    "href": "setup.html#data",
    "title": "2  Data & Setup",
    "section": "Data",
    "text": "Data\nThe data used in these materials is provided as a set of zip files. We provide instructions to download and uncompress the data via the command line, which is the recommended way to make sure you have the correct directory structure. However, we also provide the direct links to the zip files, in case you prefer to download them manually.\nFirst create a directory to store the files. Here, we create a directory for the workshop in the “Documents” folder (you can change this if you want to):\n# create variable for working directory - change this if you want\nworkdir=\"$HOME/Documents/awd_bioinfo\"\nmkdir $workdir\n\nResources\nWe provide files for databases and public genomes used in different parts of the analysis. These files are required in addition to any other datasets. In summary, this contains four directories:\n\nmash_db - database for the software Mash, covered in the Read content chapter.\nbakta_db - database for the software Bakta, covered in the Genome assembly chapter.\nCheckM2_database - database for the CheckM2 program covered in the Assembly quality chapter.\nvibrio_genomes - public genomes downloaded from NCBI and used in the Phylogenetics chapter.\n\nWe recommend downloading this file once and then creating a symbolic link (shortcut) to this folder from each of the analysis directories. This will reduce the storage space required for analysis.\nDownload this file using the command line:\n# make sure you are in the workshop folder\ncd $workdir\n\n# download and unzip\nwget -O resources.zip \"https://www.dropbox.com/sh/t8ivljixrg0z1qz/AAD9fGRSyQHrCizxrBU1VMB-a?dl=1\"\nunzip resources.zip -d resources\nrm resources.zip  # remove original zip file to save space\nIf you want to download this file manually:  download resources.\n\n\nAmbroise 2023\nThis dataset includes 5 samples sequenced on an ONT platform, and published in Ambroise et al. 2023. Here are the details about these data:\n\nNumber of samples: 5\nOrigin: samples from cholera patients from the Democratic Republic of the Congo.\nSample preparation: stool samples were collected and used for plate culture in media appropriate to grow Vibrio species; ONT library preparation and barcoding were done using standard kits.\nSequencing platform: MinION\nBasecalling: Guppy version 6 in high accuracy (“hac”) mode (this information is not actually specified in the manuscript, but we are making this assumption, just as an example).\n\nTo download the data, you can run the following commands:\n# make sure you are in the workshop folder\ncd $workdir\n\n# download and unzip\nwget -O ambroise.zip \"https://www.dropbox.com/sh/xytht4upehuo4c3/AABeYpICT2uAQzGBy4IzsKKwa?dl=1\"\nunzip ambroise.zip -d ambroise2023\nrm ambroise.zip  # remove original zip file to save space\n\n# create link to resources directory\nln -s $PWD/resources/ $PWD/ambroise2023/resources\nIf you want to download this file manually:  download Ambroise 2023.\n\n\nScripts only\nWe also provide a folder containing only the scripts used in the exercises. This is useful if you want to use your own data.\nHere are the commands to download these data:\n# make sure you are in the workshop folder\ncd $workdir\n\n# download and unzip\nwget -O minimal.zip \"https://www.dropbox.com/sh/f421dkyos4us4ty/AABmomHwzL1miVvStaDQA4gma?dl=1\"\nunzip minimal.zip -d minimal\nrm minimal.zip  # remove original zip file to save space\n\n# create link to resources directory\nln -s $PWD/resources/ $PWD/minimal/resources\nIf you want to download this file manually:  download scripts only."
  },
  {
    "objectID": "materials/01-introduction/01-awd_genomic_surveillance.html#genomic-sequencing-for-pathogen-surveillance",
    "href": "materials/01-introduction/01-awd_genomic_surveillance.html#genomic-sequencing-for-pathogen-surveillance",
    "title": "3  Pathogen genomic surveillance",
    "section": "3.1 Genomic sequencing for pathogen surveillance",
    "text": "3.1 Genomic sequencing for pathogen surveillance\nIn genomic surveillance, one crucial technique is genome sequencing. It involves examining the DNA (or RNA) of pathogens to understand their spread in communities, their role in illnesses, and their development of resistance to medications and vaccines.\nWithin pathogen genomic surveillance, a key method is whole genome sequencing (WGS). Here, the aim is to reveal the complete order of the bases in the pathogen’s genome. Today, WGS has become the standard approach for quickly detecting and investigating outbreaks. Public health labs primarily use it to promptly identify outbreak sources and assess antimicrobial resistance.\nBroadly speaking, genome sequencing involves DNA extraction, library preparation, sequencing and bioinformatic analysis. Typically, WGS produces millions of DNA fragments, termed sequence reads, using sequencing technologies such as Illumina, Oxford Nanopore, or PacBio.\n\n\n\nFigure 3.2: Overview of whole genome sequencing applied to outbreak detection. Source: US CDC\n\n\nFollowing WGS, the subsequent step involves bioinformatic and downstream analysis, encompassing:\n\nChecking and filtering sequence read quality\nGenome assembly\nEvaluating genome assembly quality\nIdentifying strain types and their resistance to antimicrobial agents.\n\nDetailed discussions of these analytical steps are presented in the following sections."
  },
  {
    "objectID": "materials/01-introduction/01-awd_genomic_surveillance.html#acute-watery-diarrhoea-awd",
    "href": "materials/01-introduction/01-awd_genomic_surveillance.html#acute-watery-diarrhoea-awd",
    "title": "3  Pathogen genomic surveillance",
    "section": "3.2 Acute Watery Diarrhoea (AWD)",
    "text": "3.2 Acute Watery Diarrhoea (AWD)\nAcute Watery Diarrhea (AWD) is a rapid-onset gastrointestinal condition characterized by frequent and loose, watery stools. It typically lasts for a short duration and is often caused by bacterial, viral, or parasitic infections, leading to dehydration if left untreated. Cholera, a specific type of AWD, is primarily caused by the bacterium Vibrio cholerae. It can manifest as severe diarrhea and vomiting and can swiftly lead to dehydration and electrolyte imbalances. Cholera outbreaks are of particular concern due to their potential to spread rapidly, especially in areas with limited access to clean water and sanitation facilities. Effective treatment involves rehydration therapy and, in severe cases, antibiotics. Controlling cholera outbreaks often requires a combination of public health measures, including improved hygiene and sanitation practices, as well as vaccination campaigns in some instances.\nThe ability to detect if a spike in AWD cases is caused by a pathogenic strain of Vibrio cholerae is therefore critical for public health action. Genomic surveillance can help in addressing if the patient infected with a pathogenic strain of Vibrio cholerae and whether it is resistant to antimicrobial agents.\n\n3.2.1 Vibrio cholerae\nVibrio cholerae is a gram-negative bacterium primarily found in water, making it highly susceptible to waterborne transmission, particularly through contaminated water sources and food. This bacterium exhibits notable genomic plasticity, characterized by the acquisition of various genetic elements that contribute to its pathogenicity. The history of V. cholerae’s evolution involves the integration of these elements, resulting in the emergence of different pathogenic strains (Figure 3.3).\nStrains of V. cholerae are typically classified through serotyping. The most prevalent serogroup, O1, includes many pathogenic strains. Further subdivisions occur based on serotypes, determined by the methylation status of the terminal lipopolysaccharide in the O-antigen. Additionally, V. cholerae is categorized into biotypes, with El Tor 7PET strains being one example.\nV. cholerae has been associated with several pandemic outbreaks, each featuring distinct prevalent strains that evolved through the acquisition of diverse genetic components. Notably, this evolutionary process often involves the incorporation of genetic elements such as the Vibrio pathogenicity island-1 and cholera toxin phage (CTX). There are multiple versions of this phage, with the current one being the El Tor phage. In summary, V. cholerae possesses a dynamic genome shaped by the acquisition of new genetic elements through horizontal gene transfer mechanisms like conjugation and phage infection, which has implications for understanding antimicrobial resistance (AMR).\n\n\n\nFigure 3.3: Strain classification of Vibrio cholerae and its associated evolution. Source: Fig. 1 in Montero et al. 2023\n\n\nIn addition to serotyping, extensive genomic analysis of numerous Vibrio genomes offers a more refined approach to clustering strains into lineages, enhancing our comprehension of the origins of local outbreaks and transmission pathways. These comprehensive genomic investigations unveil the species’ remarkable diversity, with local strains frequently harboring distinct variants of the cholera toxin and displaying specific transmission dynamics (Weill et al. 2017, Weill et al. 2019, Domman et al. 2017).\nThe Vibrio cholerae genome consists of two chromosomes, for a total of aproximately 4 Mb (million basepairs) (Figure 3.4). Besides encoding various genes responsible for the bacterium’s structure and metabolism, its genome may also contain additional elements such as plasmids, phages, and pathogenicity islands that contribute to its ability to cause disease.\n\n\n\nFigure 3.4: Schematic representation of the V. cholerae genome. Source: Fig. 1 in Pant et al. 2020\n\n\nFor an excellent overview of cholera global epidemiology and the role of genomics in this area, we recommend the following lecture by Prof. Nick Thomson:"
  },
  {
    "objectID": "materials/01-introduction/01-awd_genomic_surveillance.html#key-genomic-surveillance-questions",
    "href": "materials/01-introduction/01-awd_genomic_surveillance.html#key-genomic-surveillance-questions",
    "title": "3  Pathogen genomic surveillance",
    "section": "3.3 Key genomic surveillance questions",
    "text": "3.3 Key genomic surveillance questions\nDuring genomic surveillance of outbreaks, especially when investigating cases of acute watery diarrhoea (AWD), several critical public health questions arise:\n\nCan Vibrio cholerae be detected in the collected and sequenced samples?\nIf so, is it a recognized pathogenic strain or an unknown variant?\nIf it’s a known strain, then is it likely to be resistant to any antibiotics?\nIf resistance is suspected, then which resistance genes have been detected and which antibiotics are predicted to be affected?\n\nConsequently, one of the primary goals of these materials is to understand how genomic sequencing can be applied to reconstruct the genome of Vibrio cholerae samples collected from patients exhibiting cholera symptoms, including AWD, to address these questions."
  },
  {
    "objectID": "materials/01-introduction/01-awd_genomic_surveillance.html#summary",
    "href": "materials/01-introduction/01-awd_genomic_surveillance.html#summary",
    "title": "3  Pathogen genomic surveillance",
    "section": "3.4 Summary",
    "text": "3.4 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nGenomic surveillance is a crucial tool in tracking and monitoring the spread of pathogens. It helps identify disease outbreaks, track transmission routes, and understand the genetic diversity of pathogens.\nGenomic sequencing involves determining the complete genetic code of an organism. Steps include DNA extraction, library preparation, sequencing, and bioinformatics analysis.\nAWD is characterized by sudden-onset diarrhea and is often caused by infections. Vibrio cholerae is a bacterium responsible for cholera, a severe form of AWD.\nThe Vibrio cholerae genome is composed of two circular chromosomes. It encodes various genes related to structure, metabolism, and pathogenicity.\nDifferent strains of Vibrio cholerea may carry variations in virulence and antibiotic resistance genes.\nPathogen genomic surveillance aims to answer questions like the presence of pathogens, strain identification, antibiotic resistance, and transmission dynamics during outbreaks."
  },
  {
    "objectID": "materials/01-introduction/02-unix_intro.html#the-command-prompt",
    "href": "materials/01-introduction/02-unix_intro.html#the-command-prompt",
    "title": "4  The Unix command line",
    "section": "4.1 The command prompt",
    "text": "4.1 The command prompt\nWhen you open a terminal you are presented with a command prompt, waiting for you to input a command. It will look something like this:\nusername@computer-name:~$ |\nIt gives you information about:\n\nYour username\nThe name of your computer\nThe location in your filesystem (~ indicates your home directory)\nA separator, usually $ symbol\nThe prompt (often blinking) waiting for your command input"
  },
  {
    "objectID": "materials/01-introduction/02-unix_intro.html#navigating-the-filesystem",
    "href": "materials/01-introduction/02-unix_intro.html#navigating-the-filesystem",
    "title": "4  The Unix command line",
    "section": "4.2 Navigating the filesystem",
    "text": "4.2 Navigating the filesystem\nThe location of files in Unix is represented as a file path. For example:\n/home/participant/Documents\nIndicates the “Documents” folder of a user called “participant”. The first / at the beginning of the path indicates the root (or start) of the filesystem.\nPaths can be specific in two ways:\n\nAbsolute path: specify the full path starting from the root. These paths always start with /.\nRelative path: specify the path starting from your current location. For example, if you are located in /home/participant, the path Documents/resources would be equivalent to /home/participant/Documents/resources. Relative paths never start with /.\n\nHere are some key commands to navigate the filesystem:\n\npwd prints your current directory\ncd changes directory\nls lists files and folders\n* is known as a “wildcard” and can be used to match multiple files\n\nFor example:\npwd\n/home/participant\nChange to the “resources” folder, located within “Documents”:\ncd Documents/resources\nList the files within that folder\nls\nCheckM2_database  bakta_db  mash_db  reference  vibrio_genomes\nCommands have options that can change their behaviour, for example:\nls -l reference\n-rwxr--r-- 1 ubuntu ubuntu 2269140 Sep  7 09:34 annotation.gff\n-rwxr--r-- 1 ubuntu ubuntu     139 Sep  7 09:34 count_proteins.sh\n-rwxr--r-- 1 ubuntu ubuntu 4098588 Sep  7 09:34 genome.fasta\n-rwxr--r-- 1 ubuntu ubuntu 1413435 Sep  7 09:34 proteins.fasta\nThe -l option lists the files in a long format. We also specified that we wanted to list the files inside the reference folder (instead of the default, which lists files in the current directory).\nYou can see all the options available to a program by looking at its help/manual page: man ls or ls --help.\nThe wildcard * can be used to match files that share part of their name. For example:\nls reference/*.fasta\nreference/genome.fasta  reference/proteins.fasta\nOnly matches the files with .fasta extension."
  },
  {
    "objectID": "materials/01-introduction/02-unix_intro.html#files-and-folders",
    "href": "materials/01-introduction/02-unix_intro.html#files-and-folders",
    "title": "4  The Unix command line",
    "section": "4.3 Files and folders",
    "text": "4.3 Files and folders\nHere are some key commands to create directories and investigate the content of text files:\n\nmkdir creates a directory\nhead prints the top lines of a file\ntail prints the bottom lines of a file\nless opens the file in a viewer\nwc counts lines, words and characters in a file\ngrep prints lines that match a specified text pattern\n\nTo create a directory called “test” you can run:\nmkdir test\nTo look at the top lines of a file you can use:\nhead genome.fasta\n&gt;NZ_CP028827.1 Vibrio cholerae strain N16961 chromosome 1, complete sequence\nGTGTCATCTTCGCTATGGTTGCAATGTTTGCAACGGCTTCAGGAAGAGCTACCTGCCGCAGAATTCAGTATGTGGGTGCG\nTCCGCTTCAAGCGGAGCTCAATGACAATACTCTCACTTTATTCGCCCCGAACCGCTTTGTGTTGGATTGGGTACGCGATA\nAGTACCTCAATAACATCAATCGTCTGCTGATGGAATTCAGTGGCAATGATGTGCCTAATTTGCGCTTTGAAGTGGGGAGC\nCGCCCTGTGGTGGCGCCAAAACCCGCGCCTGTACGTACGGCTGCGGATGTCGCGGCGGAATCGTCGGCGCCTGCGCAATT\nGGCGCAGCGTAAACCTATCCATAAAACCTGGGATGATGACAGTGCTGCGGCTGATATTACTCACCGCTCAAATGTGAACC\nCGAAACACAAGTTCAACAACTTCGTGGAAGGTAAATCTAACCAGTTAGGTCTGGCCGCGGCTCGCCAAGTCTCTGATAAC\nCCAGGTGCGGCGTATAACCCCCTCTTTTTGTATGGCGGCACCGGTTTGGGTAAAACGCACTTGCTGCATGCGGTGGGTAA\nCGCGATTGTTGATAACAACCCGAACGCTAAAGTGGTGTACATGCACTCTGAGCGTTTCGTGCAAGACATGGTAAAAGCCC\nTGCAGAACAACGCGATTGAAGAATTCAAACGCTACTATCGCAGTGTAGATGCCTTGTTGATCGACGATATTCAATTCTTT\nYou can print only ‘N’ lines of the file using the following option:\nhead -n 2 genome.fasta\n&gt;NZ_CP028827.1 Vibrio cholerae strain N16961 chromosome 1, complete sequence\nGTGTCATCTTCGCTATGGTTGCAATGTTTGCAACGGCTTCAGGAAGAGCTACCTGCCGCAGAATTCAGTATGTGGGTGCG\nThe tail command works similarly, but prints the bottom lines of a file.\nTo open the file in a viewer, you can use:\nless genome.fasta\nYou can use ↑ and ↓ arrows on your keyboard to browse the file. When you want to exit you can press Q (quit).\nTo count the lines in a text file you can use:\nwc -l genome.fasta\n50601 genome.fasta\nTo print the lines that match a pattern in a file you can use:\ngrep \"&gt;\" genome.fasta\n&gt;NZ_CP028827.1 Vibrio cholerae strain N16961 chromosome 1, complete sequence\n&gt;NZ_CP028828.1 Vibrio cholerae strain N16961 chromosome 2, complete sequence"
  },
  {
    "objectID": "materials/01-introduction/02-unix_intro.html#combining-commands",
    "href": "materials/01-introduction/02-unix_intro.html#combining-commands",
    "title": "4  The Unix command line",
    "section": "4.4 Combining commands",
    "text": "4.4 Combining commands\nYou can chain multiple commands together using the pipe operator. For example:\ngrep \"&gt;\" genome.fasta | wc -l\n2\n\nFirst find and print the lines that match “&gt;”\nAnd then count the number of lines from the output of the previous step\n\nIn this case, the wc command took its input from the pipe."
  },
  {
    "objectID": "materials/01-introduction/02-unix_intro.html#summary",
    "href": "materials/01-introduction/02-unix_intro.html#summary",
    "title": "4  The Unix command line",
    "section": "4.5 Summary",
    "text": "4.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nThe Unix command line is essential for bioinformatic analysis because it is widely used in the field and allows for efficient data manipulation, automation, and reproducibility.\nThe location of files and folders from the command line using either absolute or relative paths.\n\nAbsolute paths always start with / (the root of the filesystem)\nSubsequent directory names are separated by /.\n\nKey commands to navigate the filesystem include: cd and ls\nKey commands to investigate the content of files include: head, tail, less, grep and wc.\nThe wildcard * can be used to match multiple files sharing part of their name."
  },
  {
    "objectID": "materials/01-introduction/03-ngs_intro.html#next-generation-sequencing",
    "href": "materials/01-introduction/03-ngs_intro.html#next-generation-sequencing",
    "title": "5  Introduction to NGS",
    "section": "5.1 Next Generation Sequencing",
    "text": "5.1 Next Generation Sequencing\nThe sequencing of genomes has become more routine due to the rapid drop in DNA sequencing costs seen since the development of Next Generation Sequencing (NGS) technologies in 2007. One main feature of these technologies is that they are high-throughput, allowing one to more fully characterise the genetic material in a sample of interest.\nThere are three main technologies in use nowadays, often referred to as 2nd and 3rd generation sequencing:\n\nIllumina’s sequencing by synthesis (2nd generation)\nOxford Nanopore Technologies, shortened ONT (3rd generation)\nPacific Biosciences, shortened PacBio (3rd generation)\n\nThe video below from the iBiology team gives a great overview of these technologies.\n\n\n\n\n\n5.1.1 Illumina Sequencing\nIllumina’s technology has become a widely popular method, with many applications to study transcriptomes (RNA-seq), epigenomes (ATAC-seq, BS-seq), DNA-protein interactions (ChIP-seq), chromatin conformation (Hi-C/3C-Seq), population and quantitative genetics (variant detection, GWAS), de-novo genome assembly, amongst many others.\nAn overview of the sequencing procedure is shown in the animation video below. Generally, samples are processed to generate so-called sequencing libraries, where the genetic material (DNA or RNA) is processed to generate fragments of DNA with attached oligo adapters necessary for the sequencing procedure (if the starting material is RNA, it can be converted to DNA by a step of reverse transcription). Each of these DNA molecule is then sequenced from both ends, generating pairs of sequences from each molecule, i.e. paired-end sequencing (single-end sequencing, where the molecule is only sequenced from one end is also possible, although much less common nowadays).\nThis technology is a type of short-read sequencing, because we only obtain short sequences from the original DNA molecules. Typical protocols will generate 2x50bp to 2x250bp sequences (the 2x denotes that we sequence from each end of the molecule).\n\n\n\n\nThe main advantage of Illumina sequencing is that it produces very high-quality sequence reads (error rate &lt;1%) at a low cost. However, we only get very short sequences, which is a limitations when it comes to resolving problems such as long sequence repeats (e.g. around centromeres or transposon-rich areas of the genome), distinguishing gene isoforms (in RNA-seq), or resolving haplotypes (combinations of variants in each copy of an individual’s diploid genome).\nIn summary, Illumina:\n\nUtilizes sequencing-by-synthesis chemistry.\nOffers short read lengths.\nKnown for high accuracy with low error rates (&lt;1%).\nWell-suited for applications like DNA resequencing and variant detection.\nScalable and cost-effective for large-scale projects.\nLimited in sequencing long DNA fragments.\nExpensive to set up.\n\n\n\n5.1.2 Nanopore Sequencing\nNanopore sequencing, a form of long-read sequencing technology, has distinct advantages in the field of genomics. It excels in its ability to sequence exceptionally lengthy DNA molecules, including those reaching megabase sizes, thus effectively addressing that limitation of short-read sequencing. The portability of some nanopore sequencing devices is another advantageous feature; some are designed to operate via a simple USB connection to a standard laptop, making it exceptionally adaptable for on-the-go applications, including fieldwork.\n\n\n\nOverview of Nanopore sequencing showing the highly-portable MinION device. The device contains thousands of nanopores embedded in a membrane where current is applied. As individual DNA molecules pass through these nanopores they cause changes in this current, which is detected by sensors and read by a dedicated computer program. Each DNA base causes different changes in the current, allowing the software to convert this signal into base calls. Source: Fig. 1 in Wang et al. 2021\n\n\nHowever, optimising this technology presents some challenges, notably in the production of sequencing libraries containing high molecular weight and intact DNA. It’s important to note that nanopore sequencing historically exhibited higher error rates, approximately 5% for older chemistries, compared to Illumina sequencing. However, significant advancements have emerged, enhancing the accuracy of nanopore sequencing technology, now achieving accuracy rates exceeding 99%.\nIn summary, ONT:\n\nOperates on the principle of nanopore technology.\nProvides long read lengths, ranging from thousands to tens of thousands of base pairs.\nIdeal for applications requiring long-range information, such as de novo genome assembly and structural variant analysis.\nPortable, enabling fieldwork and real-time sequencing.\nExhibits higher error rates (around 5%), with improvements in recent versions.\nCosts can be higher per base, compared to Illumina for certain projects.\n\n\n\n\n\n\n\nWhich technology to choose?\n\n\n\nBoth of these platforms have been widely popular for bacterial sequencing. They can both generate data with high-enough quality for the assembly and analysis for most of the pathogen genomic surveillance. Mostly, which one you use will depend on what sequencing facilities you have access to.\nWhile Illumina provides the cheapest option per sample of the two, it has a higher setup cost, requiring access to the expensive sequencing machines. On the other hand, Nanopore is a very flexible platform, especially its portable MinION devices. They require less up-front cost allowing getting started with sequencing very quickly in a standard molecular biology lab."
  },
  {
    "objectID": "materials/01-introduction/03-ngs_intro.html#sec-file-formats",
    "href": "materials/01-introduction/03-ngs_intro.html#sec-file-formats",
    "title": "5  Introduction to NGS",
    "section": "5.2 Bioinformatics file formats",
    "text": "5.2 Bioinformatics file formats\nBioinformatics relies on various standard file formats for storing diverse types of data. In this section, we’ll discuss some of the key ones we’ll encounter, although there are numerous others. You can refer to the “Common file formats” appendix for a more comprehensive list.\n\n5.2.1 FAST5\nFAST5 is a proprietary format developed by ONT and serves as the standard format generated by its sequencing devices. It is based on the hierarchical data format HDF5, designed for storing extensive and intricate data. Unlike text-based formats like FASTA and FASTQ, FAST5 files are binary, necessitating specialized software for opening and reading.\nWithin these files, you’ll find a Raw/ field containing the original raw current signal measurements. Additionally, tools like basecallers can add Analyses/ fields, converting signals into standard FASTQ data (e.g., Guppy basecaller).\nTypically, manual inspection of these files is unnecessary, as specialized software is used for processing them. For more in-depth information about this format, you can refer to this resource.\n\n\n5.2.2 FASTQ\nFASTQ files are used to store nucleotide sequences along with a quality score for each nucleotide of the sequence. These files are the typical format obtained from NGS sequencing platforms such as Illumina and Nanopore (after basecalling). Common file extensions used for this format include .fastq and .fq.\nThe file format is as follows:\n@SEQ_ID                   &lt;-- SEQUENCE NAME\nAGCGTGTACTGTGCATGTCGATG   &lt;-- SEQUENCE\n+                         &lt;-- SEPARATOR\n%%).1***-+*''))**55CCFF   &lt;-- QUALITY SCORES\nIn FASTQ files each sequence is always represented across 4 lines. The quality scores are encoded in a compact form, using a single character. They represent a score that can vary between 0 and 40 (see Illumina’s Quality Score Encoding). The reason single characters are used to encode the quality scores is that it saves space when storing these large files. Software that work on FASTQ files automatically convert these characters into their score, so we don’t have to worry about doing this conversion ourselves.\nThe quality value in common use is called a Phred score and it represents the probability that the base is an error. For example, a base with quality 20 has a probability \\(10^{-2} = 0.01 = 1\\%\\) of being an error. A base with quality 30 has \\(10^{-3} = 0.001 = 0.1\\%\\) chance of being an error. Typically, a Phred score threshold of &gt;20 or &gt;30 is used when applying quality filters to sequencing reads.\nBecause FASTQ files tend to be quite large, they are often compressed to save space. The most common compression format is called gzip and uses the extension .gz. To look at a gzip file, we can use the command zcat, which decompresses the file and prints the output as text.\nFor example, we can use the following command to count the number of lines in a compressed FASTQ file:\nzcat sequences.fq.gz | wc -l\nIf we want to know how many sequences there are in the file, we can divide the result by 4 (since each sequence is always represented across four lines).\n\n\n5.2.3 FASTA\nFASTA files are used to store nucleotide or amino acid sequences. Common file extensions used for this format include .fasta, .fa, .fas and .fna.\nThe general structure of a FASTA file is illustrated below:\n&gt;sample01                 &lt;-- NAME OF THE SEQUENCE\nAGCGTGTACTGTGCATGTCGATG   &lt;-- SEQUENCE ITSELF\nEach sequence is represented by a name, which always starts with the character &gt;, followed by the actual sequence.\nA FASTA file can contain several sequences, for example:\n&gt;sample01\nAGCGTGTACTGTGCATGTCGATG\n&gt;sample02\nAGCGTGTACTGTGCATGTCGATG\nEach sequence can sometimes span multiple lines, and separate sequences can always be identified by the &gt; character. For example, this contains the same sequences as above:\n&gt;sample01      &lt;-- FIRST SEQUENCE STARTS HERE\nAGCGTGTACTGT\nGCATGTCGATG\n&gt;sample02      &lt;-- SECOND SEQUENCE STARTS HERE\nAGCGTGTACTGT\nGCATGTCGATG\nTo count how many sequences there are in a FASTA file, we can use the following command:\ngrep \"&gt;\" sequences.fa | wc -l\nIn two steps:\n\nfind the lines containing the character “&gt;”, and then\ncount the number of lines of the result.\n\nFASTA files are commonly used to store genome sequences, after they have been assembled. We will see FASTA files several times throughout these materials, so it’s important to be familiar with them.\n\n\n5.2.4 GFF3\nThe GFF3 (Generic Feature Format version 3) is a standardized file format used in bioinformatics to describe genomic features and annotations. It primarily serves as a structured and human-readable way to represent information about genes, transcripts, and other biological elements within a genome. Common file extensions used for this format include .gff and .gff3.\nKey characteristics of the GFF3 format include:\n\nTab-delimited columns: GFF3 files consist of tab-delimited columns, making them easy to read and parse.\nHierarchical structure: the format supports a hierarchical structure, allowing the description of complex relationships between features. For instance, it can represent genes containing multiple transcripts, exons, and other elements.\nNine standard columns: this includes information such as the sequence identifier (e.g. chromosome), feature type (e.g. gene, exon), start and end coordinates, strand and several attributes.\nAttributes field: the ninth column, known as the “attributes” field, contains additional information in a key-value format. This field is often used to store details like gene names, IDs, and functional annotations.\nComments: GFF3 files can include comment lines starting with a “#” symbol to provide context or documentation.\n\nGFF3 is widely supported by various bioinformatics tools and databases, making it a versatile format for storing and sharing genomic annotations.\n\n\n5.2.5 CSV/TSV\nComma-separated values (CSV) and tab-separated values (TSV) files are text-based formats commonly used to store tabular data. While strictly not specific to bioinformatics, they are commonly used as the output of bioinformatic software. CSV files usually have .csv extension, while TSV files often have .tsv or the more generic .txt extension.\nIn both cases, the data is organized into rows and columns. Rows are represented across different lines of the file, while the columns are separated using a delimiting character: a command , in the case of CSV files and a tab space (tab ↹) for TSV files.\nFor example, for this table:\n\n\n\nsample\ndate\nstrain\n\n\n\n\nVCH001\n2023-08-01\nO1 El Tor\n\n\nVCH002\n2023-08-02\nO1 Classical\n\n\nVCH003\n2023-08-03\nO139\n\n\nVCH004\n2023-08-04\nNon-O1 Non-O139\n\n\n\nThis would be its representation as a CSV file:\nsample,date,strain\nVCH001,2023-08-01,O1 El Tor\nVCH002,2023-08-02,O1 Classical\nVCH003,2023-08-03,O139\nVCH004,2023-08-04,Non-O1 Non-O139\nAnd this is its representation as a TSV file (the space between columns is a tab ↹):\nsample    date        strain\nVCH001    2023-08-01  O1 El Tor\nVCH002    2023-08-02  O1 Classical\nVCH003    2023-08-03  O139\nVCH004    2023-08-04  Non-O1 Non-O139\nCSV and TSV files are human-readable and can be opened and edited using basic text editors or spreadsheet software like Microsoft Excel."
  },
  {
    "objectID": "materials/01-introduction/03-ngs_intro.html#summary",
    "href": "materials/01-introduction/03-ngs_intro.html#summary",
    "title": "5  Introduction to NGS",
    "section": "5.3 Summary",
    "text": "5.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nHigh-throughput sequencing technologies, often called next-generation sequencing (NGS), enable rapid and cost-effective genome sequencing.\nProminent NGS platforms include Illumina, Oxford Nanopore Technologies (ONT) and Pacific Biosciences (PacBio).\nEach platform employs distinct mechanisms for DNA sequencing, leading to variations in read length, error rates, and applications.\nIllumina sequencing:\n\nUses sequencing-by-synthesis chemistry, produces short read lenghts and has high accuracy with low error rates (&lt;1%).\nWhile it is scalable and cost-effective for large-scale projects, it is expensive to set up and limited in sequencing long DNA fragments.\n\nNanopore sequencing:\n\nUses nanopore technology, provides long read lengths, making it ideal for applications such as de novo genome assembly.\nAlthough the costs can be higher per base, it is cheaper to set up.\nExhibits higher error rates (around 5%), but with significant improvements in recent versions (1%).\n\nCommon file formats in bioinformatics include FASTQ, FASTA and GFF. These are all text-based formats.\nFASTQ format (.fastq or .fq):\n\nDesigned to store sequences along with quality scores.\nContains a sequence identifier, sequence data, a separator line and quality scores.\nWidely used for storing sequence reads generated by NGS platforms.\n\nFASTA format (.fasta, .fa, .fas, .fna):\n\nIs used for storing biological sequences, including DNA, RNA, and protein.\nIt Comprises a sequence identifier (often preceded by “&gt;”) and the sequence data.\nCommonly used for sequence storage and exchange of genome sequences.\n\nGFF format (.gff or .gff3):\n\nA structured, tab-delimited format for describing genomic features and annotations.\nConsists of nine standard columns, including sequence identifier, feature type, start and end coordinates, strand information, and attributes.\nFacilitates the representation of genes, transcripts, and other genomic elements, supporting hierarchical structures and metadata.\nCommonly used for storing and sharing genomic annotation data in bioinformatics.\n\nCSV (.csv) and TSV (.tsv):\n\nPlain text formats to store tables.\nThe columns in the CSV format are delimited by comma, whereas in the TSV format by a tab.\nThese files can be opened in standard spreadsheet software such as Excel."
  },
  {
    "objectID": "materials/02-assembly/01-preparing_data.html#data-overview",
    "href": "materials/02-assembly/01-preparing_data.html#data-overview",
    "title": "6  Preparing data",
    "section": "6.1 Data overview",
    "text": "6.1 Data overview\nIn the examples used throughout these materials we will use sequencing data for the cholera bacterium Vibrio cholerae. The material was obtained from cultured samples, so each sequencing library was prepared from DNA extracted from a single plate colony. We have two datasets, as summarised below.\n\nDataset 1Dataset 2\n\n\nThis dataset is used for the examples shown in the main text. These data are not available for download (see dataset 2 instead).\n\nNumber of samples: 10\nOrigin: unspecified country (due to privacy concerns), but all patients showed AWD symptoms, suspected to be due to a local cholera outbreak.\nSample preparation: stool samples were collected and used for plate culture in media appropriate to grow Vibrio species; DNA was prepared using the Zymobiomics Fungal/Bacterial DNA miniprep kit; ONT library preparation and barcoding were done using standard ONT kits.\nSequencing platform: MinION\nBasecalling: Guppy version 6 in “fast” mode\n\n\n\nThis dataset is used for the exercises and can be downloaded (see Setup & Data). This is part of a public dataset from Ambroise et al. 2003 (see publication for further details).\n\nNumber of samples: 5\nOrigin: samples from cholera patients from the Democratic Republic of the Congo.\nSample preparation: stool samples were collected and used for plate culture in media appropriate for growing Vibrio species; ONT library preparation and barcoding were done using standard kits.\nSequencing platform: MinION, using FLO-MIN112 (R10 version) flowcells.\nBasecalling: Guppy version 6 in “fast” mode (this information is not actually specified in the manuscript, but we are making this assumption just as an example).\n\n\n\n\n\n\n\n\n\n\nUsing your data\n\n\n\nIf you are attending one of our workshops that includes lab training, you can use the data produced during the workshop in the exercises."
  },
  {
    "objectID": "materials/02-assembly/01-preparing_data.html#setting-up-directories-and-preparing-files",
    "href": "materials/02-assembly/01-preparing_data.html#setting-up-directories-and-preparing-files",
    "title": "6  Preparing data",
    "section": "6.2 Setting up directories and preparing files",
    "text": "6.2 Setting up directories and preparing files\nFor convenience and reproducibility of any bioinformatic analysis, it is good practice to set up several directories before starting your analysis. It is also convenient to download any required files from public databases, such as reference genomes.\nYou can do this from your file browser or using the command line. We recommend starting with a new directory that will be used to store all the data and scripts used during the analysis. For example, let’s say we are working from a directory in our Documents called awd_workshop. Here are some recommended directories that you should create inside it:\n\ndata - for storing raw sequencing data (fastq files).\nscripts - for storing all scripts for running analysis at different stages.\nresults - for storing results of the pipeline.\nreports - for storing reports of the analysis.\nresources - for storing files from public repositories, such as reference genomes and other databases we will require during our analysis.\n\n\n\n\n\n\n\nNote for workshop attendees\n\n\n\nIf you’re attending one of our live workshops, we’ve already prepared the data for you to save time in downloading and preparing the files. However, you can read this section to understand where the data came from.\n\n\nFrom the command line, you can create directories using the command mkdir. In our example, we will be working from our Documents folder, which is located in ~/Documents/ (remember that ~ is a shortcut to your home directory).\nWe start by moving into that directory:\ncd ~/Documents\nAnd then we create a folder for our project. We call this folder awd_workshop:\nmkdir awd_workshop\nWe can then move into that folder using cd again:\ncd awd_workshop\nFinally, we create all the sub-directories to save our different pieces of data:\n# create several directories simultaneously with a single command\nmkdir data scripts results resources reports"
  },
  {
    "objectID": "materials/02-assembly/01-preparing_data.html#sequencing-files",
    "href": "materials/02-assembly/01-preparing_data.html#sequencing-files",
    "title": "6  Preparing data",
    "section": "6.3 Sequencing files",
    "text": "6.3 Sequencing files\nOur bioinformatics analysis will start by first looking at the FASTQ files generated by the basecalling software called Guppy. This software converts the Nanopore electrical signal to sequence calls and stores the results in a directory named fastq_pass, which contains a subdirectory for each sample barcode used.\nWe have copied/pasted the fastq_pass folder from Guppy into our data directory simply using our  file browser. We can use the command ls to view what’s inside it:\nls data/fastq_pass\nbarcode25  barcode27  barcode29  barcode31  barcode33\nbarcode26  barcode28  barcode30  barcode32  barcode34\nIn our example, we had 10 samples with the barcodes shown (yours might look different).\nIf you wanted to quickly look at how many reads you have in each file, you could use some command line tricks:\n\nfirst combine all the FASTQ files from a barcode using zcat (we use the z* variant of the cat command because our files are compressed)\npipe the output to the wc -l to count the number of lines in the combined files\nthen divide that number by 4, because each sequence is represented in 4 lines in FASTQ files\n\nFor example, for barcode25 we could do:\nzcat data/fastq_pass/barcode25/*.fastq.gz | wc -l\n484416\nIf we divide that value by 4, we can determine that we have 121,104 reads in this sample.\n\n\n\n\n\n\nAdvanced: automatically determining read number across barcodes\n\n\n\n\n\nThe following code is more advanced, but it allows us to determine how many reads we have in each barcode without having to type each command individually. Instead, we use a for loop to automatically perform our task of counting reads for each barcode.\nIf you feel confortable using the command line, you can try it out.\nfor barcode in data/fastq_pass/*\ndo\n  # count total lines in all files within a barcode folder\n  nlines=$(zcat $barcode/* | wc -l)\n  \n  # divide the value by 4 (each sequence is represented in 4 lines in FASTQ files)\n  nreads=$(( $nlines / 4 ))\n  \n  # print the result\n  echo \"Reads in ${barcode}: ${nreads}\"\ndone\nReads in data/fastq_pass/barcode25: 121104\nReads in data/fastq_pass/barcode26: 202685\nReads in data/fastq_pass/barcode27: 247162\nReads in data/fastq_pass/barcode28: 262453\nReads in data/fastq_pass/barcode29: 157356\nReads in data/fastq_pass/barcode30: 286582\nReads in data/fastq_pass/barcode31: 187090\nReads in data/fastq_pass/barcode32: 120555\nReads in data/fastq_pass/barcode33: 121991\nReads in data/fastq_pass/barcode34: 102589\nTo learn more about for loops see our Unix course materials."
  },
  {
    "objectID": "materials/02-assembly/01-preparing_data.html#metadata",
    "href": "materials/02-assembly/01-preparing_data.html#metadata",
    "title": "6  Preparing data",
    "section": "6.4 Metadata",
    "text": "6.4 Metadata\nHaving metadata (data about our raw FASTQ files) is important in order to have a clear understanding on how the samples and raw data were generated. Two key pieces of information for genomic surveillance are the date of sample collection and the geographic location of that sample. This information can be used to understand which strains of a pathogen are circulating in an area at any given time. Information like the protocol used for preparing the samples (e.g. metagenomic or culture-based) and the sequencing platform used (Illumina or ONT) are also crucial for the bioinformatic analysis steps. Metadata is thus important for downstream analyses, results interpretation and reporting.\nPrivacy concerns need to be considered when collecting and storing sensitive data. However, it should be noted that sensitive data can still be collected, even if it is not shared publicly. Such sensitive information may still be useful for the relevant public health authorities, who may use that sensitive information for a finer analysis of the data. For example, epidemiological analysis will require individual-level metadata (“person, place, time”) to be available, in order to track the dynamics of transmission within a community.\nThe most general advice when it comes to metadata collection is: record as much information about each sample as possible!\nSome of this information can be stored in a CSV file, created with a spreadsheet software such as Excel."
  },
  {
    "objectID": "materials/02-assembly/01-preparing_data.html#sec-public-genomes",
    "href": "materials/02-assembly/01-preparing_data.html#sec-public-genomes",
    "title": "6  Preparing data",
    "section": "6.5 Public genomes",
    "text": "6.5 Public genomes\nIn our example, we are working with cultured samples of the Vibrio cholerae bacteria, the causative pathogen of the cholera disease, of which AWD is a major symptom. Therefore, we have downloaded public genomes for this pathogen, which will later be used to understand how our strains relate to others previously sequenced.\nThere are many complete genomes available for V. cholerae, which can be downloaded from the NCBI public database. The first full genome was published in 2000 from the clinical isolate N16961, which belongs to serogroup O1, serotype Inaba, biotype El Tor. This is a typical strain from the current pandemic, often referred to as ‘7PET’ (7th pandemic El Tor).\nThe Vibriowatch website (which we will detail more about in a later chapter) also provides a set of 17 ‘reference genomes’, 14 of which belong to the current pandemic (7PET) lineages. Their reference genomes are named as ‘Wi_Tj’ where ‘W’ stands for a Wave and ‘i’ is its number, and ‘T’ stands for Transmission event and ‘j’ its number. For instance, a W1_T1 strain means “wave one transmission event one”.\nFor example, we can look at a public strain related to ‘W3_T13’, associated with a cholera outbreak in Yemen in 2019. This sample is available from the following link: https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_937000105.1/.\nFrom the link above you can see information about the genome assembly of this strain, as shown below. The assembly is complete with 99.69% of the genome recovered.\n\nIn that same page, you will see a “Download” button. When you click that button, the pop-up window will be displayed as illustrated in the image below.\n\nThe reference genome sequence can be selected by ticking ‘Genome sequences (FASTA)’ and the gene annotation by ticking ‘Annotation features (GFF)’ formats. You can change the name of the file that will be downloaded and finally, click the ‘Download’ button to download the reference genome assembly.\nThe downloaded file is compressed as a Zip file, which you can uncompress and copy the files to your project folder. For this workshop, we have downloaded 31 samples and saved them in the directory resources/vibrio_genomes. We will later use these genomes for our phylogenetic analysis.\nWe can check all our genomes with the ls command:\nls resources/vibrio_genomes\nGCF_004328575.1_ASM432857v1_genomic.fna   GCF_013462495.1_ASM1346249v1_genomic.gff  GCF_021431865.1_ASM2143186v1_genomic.fna\nGCF_004328575.1_ASM432857v1_genomic.gff   GCF_015482825.1_ASM1548282v1_genomic.fna  GCF_021431865.1_ASM2143186v1_genomic.gff\nGCF_009763665.1_ASM976366v1_genomic.fna   GCF_015482825.1_ASM1548282v1_genomic.gff  GCF_021431945.1_ASM2143194v1_genomic.fna\nGCF_009763665.1_ASM976366v1_genomic.gff   GCF_017948285.1_ASM1794828v1_genomic.fna  GCF_021431945.1_ASM2143194v1_genomic.gff\n\n... more output ommitted ...\nYou can see that for each sample we have an .fna file (FASTA format) and a .gff file (GFF format). See Section 5.2 for a recap of these file formats.\nFor each of these samples we also obtained some metadata, which is stored in a tab-delimited (TSV) file:\nhead resources/vibrio_genomes/public_genomes_metadata.tsv\nname                                  display_name     clade       mlst  biotype    serogroup\nGCF_009763665.1_ASM976366v1_genomic   GCF_009763665.1  Env_Sewage  1258  NA         NA\nGCF_023169825.1_ASM2316982v1_genomic  GCF_023169825.1  Env_Sewage  555   NA         NA\nGCF_937000115.1_CNRVC190247_genomic   GCF_937000115.1  Env_Sewage  555   NA         NA\nGCF_004328575.1_ASM432857v1_genomic   GCF_004328575.1  M66         178   NA         NA\nGCF_019458465.1_ASM1945846v1_genomic  GCF_019458465.1  M66         1257  NA         NA\nGCF_021431945.1_ASM2143194v1_genomic  GCF_021431945.1  M66         1092  NA         NA\nGCF_026013235.1_ASM2601323v1_genomic  GCF_026013235.1  M66         167   NA         NA\nGCF_009763945.1_ASM976394v1_genomic   GCF_009763945.1  W1_T2       69    O1 El Tor  O1\nGCF_013085075.1_ASM1308507v1_genomic  GCF_013085075.1  W1_T2       69    O1 El Tor  O1\nThis provides information for each sample, which we will discuss in later sections."
  },
  {
    "objectID": "materials/02-assembly/01-preparing_data.html#summary",
    "href": "materials/02-assembly/01-preparing_data.html#summary",
    "title": "6  Preparing data",
    "section": "6.6 Summary",
    "text": "6.6 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nProper file and folder organization ensures clarity, reproducibility, and efficiency throughout your bioinformatic analysis.\nOrganising files by project, and creating directories for data, scripts and results helps prevent data mix-ups and confusion.\nCommand line tools like mkdir allow you to create directory structures efficiently.\nDirectories should have meaningful names to reflect their contents and purpose.\nFASTQ files containing the raw sequencing data, can be quickly investigated using standard command line tools, for example to count how many reads are available.\nMetadata provides context to biological data, including sample information, experimental conditions, and data sources.\nIn pathogen surveillance, metadata helps trace the origin and characteristics of samples, aiding outbreak investigation.\nPublic genomic databases like NCBI and platforms such as Pathogenwatch provide a vast collection of sequenced genomes.\nAccess to these resources allows researchers to retrieve reference genomes for comparative analyses."
  },
  {
    "objectID": "materials/02-assembly/02-read_content.html#read-content",
    "href": "materials/02-assembly/02-read_content.html#read-content",
    "title": "7  Assessing read content",
    "section": "7.1 Read content",
    "text": "7.1 Read content\nAs detailed in the previous section, our example data was obtained from Vibrio cholerae colonies from plate cultures. Therefore, we expect our sequencing reads to contain only V. cholerae sequences and nothing else. If we had used a metagenomic approach, we would have expected other organisms to also be contained in our reads.\nTherefore, before attempting to assemble genomes from our samples, it is a good idea to screen which species can be detected in our sequencing reads. For plate-based samples like ours, this is a good quality check for our samples, as we can confirm that they contain V. cholerae only and are not contaminated with other organisms. For metagenomic samples, where we expect a mixture of organisms (including Human), we can get an idea of whether V. cholerae is present in the sample and at what fraction.\nGenerally speaking, assessing read content can be done by comparing our sequencing reads against a database of sequences from known organisms. Two popular software options for this task are:\n\nKraken2, which categorises each sequencing read to the highest possible taxonomic level. Kraken2 is commonly used in metagenomic analysis, especially when combined with Braken, which allows estimating species abundance in a mixed sample. However, it can be computationally intensive.\nMash is a faster alternative for assessing species content in reads. It doesn’t assign a taxonomy to individual reads but reports the sample’s content based on matches with its database sequences. The drawback is that it doesn’t provide precise species abundance estimation.\n\nFor our case study, which uses cultured samples, we choose to use Mash for its speed."
  },
  {
    "objectID": "materials/02-assembly/02-read_content.html#mash",
    "href": "materials/02-assembly/02-read_content.html#mash",
    "title": "7  Assessing read content",
    "section": "7.2 Mash",
    "text": "7.2 Mash\nTo screen our reads for known bacterial species we will use the software Mash, which implements an algorithm for fast screening raw sequencing reads against a database. The Mash software, primarily designed for metagenomic samples with multiple species, can also be applied to culture-based samples to confirm the presence of the expected organism and detect potential contaminants. Mash is known for its speed in assessing the species content of sequencing reads without requiring genome assembly.\n\n\n\nFigure 7.1: Schematic view of the Mash algorithm approach. In this example, the 4th species is likely to be what’s contained in our sample, as there were matches to all its sub-sequences (shown as coloured bars). Source: adapted from Fig. 2 in Ondov et al. 2019.\n\n\nTo run Mash, a pre-built database file is needed, readily available online from the developers. This database is constructed from bacterial organisms found in public databases like NCBI’s RefSeq. For efficiency, the developers break down each reference sequence into smaller sub-sequences, as shown in Figure 7.1. Occasionally, different organisms may share similar sub-sequences, but Mash can differentiate them by examining other sub-sequences. When your reads are matched against these sub-sequences, Mash counts the hits for each species. The species with the most matches in the database is likely the main species in your sample. If you have multiple organisms, as in a metagenomic sample, different species may show a high fraction of matched sub-sequences.\n\n7.2.1 Mash database\nBefore screening our reads, we must first download the most up-to-date database used by Mash. There are several pre-compiled databases available from the Mash website. We will use one that includes both bacterial genomes and plasmids (see example tutorial).\nIf you are attending our workshop, we have already downloaded these files for you, but here are the commands we used to achieve this:\n# create subdirectory for mash database\nmkdir -p resources/mash_db \n\n# download the database file\nwget -O resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh --no-check-certificate https://gembox.cbcb.umd.edu/mash/refseq.genomes%2Bplasmid.k21s1000.msh\nAs you can see, because this is a public resource, we saved it in our resources folder.\n\n\n7.2.2 Mash screen\nThe main step of our analysis can be run using the mash screen command. This command takes the Mash database and checks how well its sequences are contained in our reads, for each organism. If our sequencing reads cover the entire genome of V. cholerae, then we expect both a high sequence identity and a high percentage of shared sequences with that organism.\nTo start our analysis, we first create a folder for our output:\nmkdir results/mash\nTo run the analysis on a single sample, we can run the following command:\nmash screen -w -p 8 resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh data/fastq_pass/barcode25/*.fastq.gz | sort -n -r &gt; results/mash/barcode25_screen.tsv\nThere are several things to note about this command:\n\nWe use the option -w, which according to the help (mash screen -h) refers to the “winner-take-all strategy”, which removes much of the redundancy in the results to make interpretation of the results easier.\nWe use the -p option to use more CPU cores, to speed the computation by running the analysis in parallel.\nWe use the * wildcard to match all the FASTQ files within the fastq_pass/barcode25 data folder (ONT often outputs multiple FASTQ files per barcode).\nWe pipe the output of mash to the sort command to order the results in descending order of sequence identity against the Mash database. This will make sure that we get the hits with the highest identity at the top of the file.\nThe output from Mash is a tab-delimited file, so we save our output with .tsv extension to indicate this.\n\nWe can either open this file in a spreadsheet program (such as Excel), or use the program less to open it directly in the terminal:\nless -S results/mash/barcode25_screen.tsv\n0.99957   991/1000  46  0  GCF_001187225.1_ASM118722v1_genomic.fna.gz     [97 seqs] NZ_LGNX01000001.1 Vibrio cholerae O1 strain NHCC-079 Contig1, whole genome shotgun sequence [...]\n0.999377  987/1000  36  0  GCF_000893195.1_ViralProj63437_genomic.fna.gz  NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.994519  891/1000  63  0  ref|NC_004982.1|                               Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n0.993445  871/1000  90  0  ref|NC_008613.1|                               Photobacterium damselae subsp. piscicida plasmid pP91278, complete sequence\n0.967069  495/1000  74  0  ref|NZ_CP013143.1|                             Alcaligenes faecalis strain ZD02 plasmid pZD02, complete sequence\n0.962791  451/1000  83  0  ref|NZ_CP007487.1|                             Salmonella enterica subsp. enterica strain SA972816 plasmid p972816 sequence\n0.959749  422/1000  96  0  ref|NZ_CP007486.1|                             Salmonella enterica subsp. enterica strain SA972816 plasmid p972816 sequence\n0.952766  362/1000  96  0  ref|NZ_CP007485.1|                             Salmonella enterica subsp. enterica strain SA972816 plasmid p972816 sequence\n0.952135  357/1000  99  0  ref|NC_011511.1|                               Klebsiella pneumoniae plasmid p169, complete sequence\n0.95188   355/1000  32  0  ref|NC_017172.1|                               Acinetobacter baumannii MDR-ZJ06 plasmid pMDR-ZJ06, complete sequence\nThe output file contains one row for each organism (or plasmid) from the Mash database that was found in our sequencing reads. The columns in the output file are in the following order:\n\nidentity - the percentage of sequence similarity between the database and our reads.\nshared-hashes - a score referring to how many sequences in the database for that organism were matched to our reads.\nmedian-multiplicity - the average number of times each database sequence is found in our reads (this is a very rough proxy for genome coverage).\np-value - a statistical measure of the significance of the distance between the sequences in the database and our own sequencing reads. Low values (close to zero) indicate that it would be very unlikely to observe such similarity by chance alone.\nquery-id - the name of the sequence in the Mash database that was used to report these matches.\nquery-comment - the description of the query sequence.\n\nFrom the results above, we can see that our reads appear to be clearly related to V. cholerae. The second hit is “Vibrio phage CTX”, which refers to the phage encoding the cholera toxin, CTXφ.\nThis analysis initially answers two critical questions with regards to cholera genomic surveillance, which are:\n\nIs this V.cholerae? Yes, it appears that our samples contain V. cholerae sequences.\nIs this a pathogenic strain? Yes, it appears that our samples belong to a pathogenic serogroup, because they contain the CTXφ prophage.\n\nThe second question will be confirmed during downstream analyses."
  },
  {
    "objectID": "materials/02-assembly/02-read_content.html#screening-multiple-samples",
    "href": "materials/02-assembly/02-read_content.html#screening-multiple-samples",
    "title": "7  Assessing read content",
    "section": "7.3 Screening multiple samples",
    "text": "7.3 Screening multiple samples\nIn the above analysis, we only screened one sample. To analyse the other samples we could re-run the command, replacing the barcode number each time. However, if we have dozens of samples, this can become very tedious and prone to error, as it requires a lot of copy/paste. Therefore, we could automate the analysis for each barcode by using a for loop, which is demonstrated in the code below.\n#!/bin/bash\n\n# create output directory\nmkdir results/mash/\n\n# loop through each barcode\nfor filepath in data/fastq_pass/*\ndo\n    # get the barcode name\n    barcode=$(basename $filepath)\n    \n    # print a message\n    echo \"Processing ${barcode}\"\n    \n    # run mash command\n    mash screen -w -p 8 resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh ${filepath}/*.fastq.gz | sort -n -r &gt; results/mash/${barcode}_screen_sorted.tsv\ndone\nThe result in this case will be individual TSV files for each barcode, which we could open individually to see what the top hits for each of them were. You could also use the head command to look at the top few lines of each file in one go, as exemplified here:\n# look at the top 3 lines of every screen file\nhead -n 3 results/mash/barcode*.tsv\n==&gt; results/mash/barcode25_screen_sorted.tsv &lt;==\n0.999473        989/1000        12      0       GCF_000893195.1_ViralProj63437_genomic.fna.gz   NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.999135        982/1000        11      0       GCF_000348385.2_ASM34838v2_genomic.fna.gz       [86 seqs] NZ_KB662481.1 Vibrio cholerae O1 str. NHCC-004A genomic scaffold vcoNHCC004A.contig.0, whole genome shotgun sequence [...]\n0.994092        883/1000        13      0       ref|NC_004982.1|        Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n\n==&gt; results/mash/barcode26_screen_sorted.tsv &lt;==\n0.99957 991/1000        46      0       GCF_001187225.1_ASM118722v1_genomic.fna.gz      [97 seqs] NZ_LGNX01000001.1 Vibrio cholerae O1 strain NHCC-079 Contig1, whole genome shotgun sequence [...]\n0.999377        987/1000        36      0       GCF_000893195.1_ViralProj63437_genomic.fna.gz   NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.994519        891/1000        63      0       ref|NC_004982.1|        Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n\n==&gt; results/mash/barcode27_screen_sorted.tsv &lt;==\n0.999522        990/1000        44      0       GCF_000348385.2_ASM34838v2_genomic.fna.gz       [86 seqs] NZ_KB662481.1 Vibrio cholerae O1 str. NHCC-004A genomic scaffold vcoNHCC004A.contig.0, whole genome shotgun sequence [...]\n0.999377        987/1000        36      0       GCF_000893195.1_ViralProj63437_genomic.fna.gz   NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.995206        904/1000        68      0       ref|NC_004982.1|        Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n\n... some output ommitted to save space ...\nWe can see that the results for individual barcodes (here we show 3 barcodes as an example) are similar to each other, which we expect as all these samples come from the same outbreak event and so should be related to each other."
  },
  {
    "objectID": "materials/02-assembly/02-read_content.html#exercises",
    "href": "materials/02-assembly/02-read_content.html#exercises",
    "title": "7  Assessing read content",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data.\n\n\n\n\n\n\nMash screening\n\n\n\n\n\n\n\nScreen the sequencing reads for known organisms, by running Mash on one of the barcodes of your choice.\n\nMake sure you start from your analysis directory: cd ~/Documents/YOUR_FOLDER.\nCreate an output directory for the results called results/mash.\nActivate the software environment: conda activate mash.\nRun the mash screen command on a barcode of your choice:\n\nSave the results to a file called results/mash/barcodeXX_screen.tsv (replace “XX” with the barcode number you are analysing).\nThe Mash database is already available at resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh\n\n\nThe command might take some time to run (maybe 10 minutes). Once it completes, examine the output file to investigate if your reads contained the organism you were expecting.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nRemember that to create a directory from the command line you can use the mkdir command.\nThe general command to run mash is:\nmash screen -w -p 8  PATH_TO_DB_FILE  PATH_TO_INPUT_FASTQS  |  sort -n -r &gt;  OUTPUT_FILE\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThese solutions are for the “Ambroise 2023” dataset. We first made sure that were were on that folder:\ncd ~/Documents/ambroise2023\nWe then created an output directory and activated our software environment:\nmkdir results/mash\nmamba activate mash\nWe could confirm the software environment was active as our terminal showed (mash) at the start.\nFinally, we ran the mash screen command for “barcode01”:\nmash screen -w -p 8  resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh  data/fastq_pass/barcode01/*.fastq.gz  |  sort -n -r &gt;  results/mash/barcode01_screen.tsv\nThe output file is in tab-delimited (TSV) format. We can open this file in Excel, or directly from the command line:\n# look at the top 3 lines of the file\nhead -n 3 results/mash/barcode01_screen.tsv\n1         1000/1000  42  0  GCF_000279245.1_ASM27924v1_genomic.fna.gz      [23 seqs] NZ_ALDE01000001.1 Vibrio cholerae CP1041(14) vcoCP1041.contig.0, whole genome shotgun sequence [...]\n0.999618  992/1000   39  0  GCF_000893195.1_ViralProj63437_genomic.fna.gz  NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.995779  915/1000   73  0  ref|NC_004982.1|                               Vibrio cholerae strain O395P plasmid pTLC, complete sequence\nIndeed, we can confirm that our reads contain Vibrio cholerae sequences. The second hit is to the CTX phage, which indicates this is a pathogenic strain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScreening multiple samples\n\n\n\n\n\n\n\nIn the previous exercise, you ran the analysis for a single sample. However running the analysis individually is impractical when you have many samples/barcodes to process.\nIn the folder scripts (within your analysis directory) you will find a script named 01-mash.sh. This script contains the code to use a programatic technique known as a for loop to automatically repeat the analysis for each barcode.\n\nOpen the script and examine its code to see if you can understand what it is doing. The script is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. You shouldn’t have to change these settings, but you can if your data is located in a different folder than what we specified by default.\n#### Analysis #### this is where the Mash analysis is run on each sample. You should not change the code in this section, although examining it is a good way to learn about Bash programming.\n\nRun the script using bash scripts/01-mash.sh. If the script is running successfully it should print a message on the screen for each barcode that it processes. Depending on how many barcodes you have, this will take quite a while to finish. \nOne the analysis is over, examime the output files to see if all barcodes contain the organism you were expecting.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nWe ran the script as instructed using:\nbash scripts/01-mash.sh\nWhile it was running it printed a message on the screen:\nProcessing barcode01\nLoading resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh...\n   20679266 distinct hashes.\nStreaming from data/fastq_pass/barcode01/ERR10146532.fastq.gz...\n   Estimated distinct k-mers in mixture: 210848305\nSumming shared...\nReallocating to winners...\nComputing coverage medians...\nWriting output...\nProcessing barcode02\nLoading resources/mash_db/refseq.genomes_and_plasmids.k21s1000.msh...\n   20679266 distinct hashes.\nStreaming from data/fastq_pass/barcode02/ERR10146551.fastq.gz...\nAfter it finished we see several files in the output folder:\nls results/mash\nbarcode01_screen_sorted.tsv  barcode04_screen_sorted.tsv  barcode07_screen_sorted.tsv\nbarcode02_screen_sorted.tsv  barcode05_screen_sorted.tsv  barcode08_screen_sorted.tsv\nbarcode03_screen_sorted.tsv  barcode06_screen_sorted.tsv  barcode09_screen_sorted.tsv\nWe examined the first 10 lines of every file using the head command and the * to help us select all files at once:\nhead -n 10 results/mash/*_screen_sorted.tsv\n==&gt; results/bkp-mash/barcode01_screen_sorted.tsv &lt;==\n1       1000/1000       42      0       GCF_000279245.1_ASM27924v1_genomic.fna.gz       [23 seqs] NZ_ALDE01000001.1 Vibrio cholerae CP1041(14) vcoCP1041.contig.0, whole genome shotgun sequence [...]\n0.999618        992/1000        39      0       GCF_000893195.1_ViralProj63437_genomic.fna.gz   NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.995779        915/1000        73      0       ref|NC_004982.1|        Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n0.977192        616/1000        33      0       ref|NC_010072.1|        Yersinia pestis plasmid pIP1203, aph(3'')-Ib gene and aph(6)-Id gene for aminoglycoside phosphotransferases, APH(3'')-Ib and APH(6)-Id\n0.958764        413/1000        53      0       ref|NC_008690.1|        Vibrio sp. TC68 plasmid pTC68, complete sequence\n0.931825        227/1000        40      0       ref|NC_017329.1|        Shigella flexneri 2002017 plasmid pSFxv_3, complete sequence\n0.93163 226/1000        35      0       ref|NC_005862.1|        Salmonella enterica enterica sv Choleraesuis plasmid cryptic, complete sequence\n0.922553        184/1000        34      0       ref|NC_011378.1|        Pasteurella multocida plasmid pCCK1900, complete sequence\n0.904677        122/1000        37      0       ref|NC_020280.1|        Edwardsiella ictaluri plasmid pEI3, complete sequence\n0.886148        79/1000 38      2.69788e-223    ref|NC_010912.1|        Avibacterium paragallinarum strain A14 plasmid pYMH5, complete sequence\n\n==&gt; results/bkp-mash/barcode02_screen_sorted.tsv &lt;==\n1       1000/1000       35      0       GCF_000893195.1_ViralProj63437_genomic.fna.gz   NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.999761        995/1000        35      0       GCF_000763075.1_ASM76307v1_genomic.fna.gz       [451 seqs] NZ_JPOP01000001.1 Vibrio cholerae strain RND81 contig_1, whole genome shotgun sequence [...]\n0.995882        917/1000        102     0       ref|NC_004982.1|        Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n0.977192        616/1000        36      0       ref|NC_010072.1|        Yersinia pestis plasmid pIP1203, aph(3'')-Ib gene and aph(6)-Id gene for aminoglycoside phosphotransferases, APH(3'')-Ib and APH(6)-Id\n0.958543        411/1000        55      0       ref|NC_008690.1|        Vibrio sp. TC68 plasmid pTC68, complete sequence\n0.932021        228/1000        34      0       ref|NC_005862.1|        Salmonella enterica enterica sv Choleraesuis plasmid cryptic, complete sequence\n0.931825        227/1000        42      0       ref|NC_017329.1|        Shigella flexneri 2002017 plasmid pSFxv_3, complete sequence\n0.922314        183/1000        40      0       ref|NC_011378.1|        Pasteurella multocida plasmid pCCK1900, complete sequence\n0.904677        122/1000        38      0       ref|NC_020280.1|        Edwardsiella ictaluri plasmid pEI3, complete sequence\n0.892135        91/1000 43      1.09991e-265    ref|NC_010912.1|        Avibacterium paragallinarum strain A14 plasmid pYMH5, complete sequence\n\n==&gt; results/bkp-mash/barcode03_screen_sorted.tsv &lt;==\n0.999905        998/1000        33      0       GCF_000893195.1_ViralProj63437_genomic.fna.gz   NC_015209.1 Vibrio phage CTX chromosome I, complete genome\n0.999618        992/1000        34      0       GCF_000234395.1_ASM23439v2_genomic.fna.gz       [15 seqs] NZ_AGUM01000015.1 Vibrio cholerae HC-23A1 vcoHC23A1.contig.14, whole genome shotgun sequence [...]\n0.995519        910/1000        53      0       ref|NC_004982.1|        Vibrio cholerae strain O395P plasmid pTLC, complete sequence\n0.977569        621/1000        33      0       ref|NC_010072.1|        Yersinia pestis plasmid pIP1203, aph(3'')-Ib gene and aph(6)-Id gene for aminoglycoside phosphotransferases, APH(3'')-Ib and APH(6)-Id\n0.958208        408/1000        43      0       ref|NC_008690.1|        Vibrio sp. TC68 plasmid pTC68, complete sequence\n0.931825        227/1000        40      0       ref|NC_017329.1|        Shigella flexneri 2002017 plasmid pSFxv_3, complete sequence\n0.931825        227/1000        29      0       ref|NC_005862.1|        Salmonella enterica enterica sv Choleraesuis plasmid cryptic, complete sequence\n0.922553        184/1000        36      0       ref|NC_011378.1|        Pasteurella multocida plasmid pCCK1900, complete sequence\n0.905377        124/1000        33      0       ref|NC_020280.1|        Edwardsiella ictaluri plasmid pEI3, complete sequence\n0.884515        76/1000 33      1.0071e-217     ref|NC_010912.1|        Avibacterium paragallinarum strain A14 plasmid pYMH5, complete sequence\n\n\n... more lines omitted to save space ...\nWe could confirm from this output that all our samples contained a high fraction of Vibrio cholerae sequences. This was confirmed by a high percentage of sequence identify (first column) and a high fraction of the database sequences matching our sequences (second column).\nBy the 4th or 5th entries in the results tables, although the sequence identity is relatively high (first column), there’s a much smaller fraction of matches (second column) suggesting these may be spurious, perhaps due to conserved sequences across species."
  },
  {
    "objectID": "materials/02-assembly/02-read_content.html#summary",
    "href": "materials/02-assembly/02-read_content.html#summary",
    "title": "7  Assessing read content",
    "section": "7.5 Summary",
    "text": "7.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nAssessing species content helps confirm the identity of the organisms present in your sequencing data.\nFor cultured samples it ensures that your data matches your expected species and helps detect contamination early in the analysis.\nMash is a tool that matches your reads against known genome sequences (stored in a database), allowing you to identify the closest known species to your sequencing reads.\nThe Mash analysis requires two steps:\n\nDownloading a suitable database for the organisms of interest (e.g. prokaryotes, eukaryotes, fungi). This only needs to be done once, or if the database is updated.\nPerforming the screening step, using the command mash screen.\n\nResults may indicate contamination if there are species present that shouldn’t be in your dataset, such as lab contaminants or environmental microbes.\nProper interpretation of results involves considering both the sequence identity and number of hits against the database.\nEarly screening is sometimes enough to assess that you are dealing with pathogenic strains of the microbe. For example, the presence of the CTX phage, common in O1 El Tor strains of Vibrio cholerae."
  },
  {
    "objectID": "materials/02-assembly/03-genome_assembly.html#genome-assembly",
    "href": "materials/02-assembly/03-genome_assembly.html#genome-assembly",
    "title": "8  Genome assembly",
    "section": "8.1 Genome Assembly",
    "text": "8.1 Genome Assembly\nThe next step in our analysis is genome assembly, which involves piecing together a complete genome from the sequencing data we’ve obtained. This is a critical process in our study of Vibrio cholerae samples, as it allows us to identify the specific strains, determine their pathogenicity, and detect toxigenic and antimicrobial resistance genes.\nThere are two main approaches to genome assembly:\n\nDe-novo assembly: this uses no prior information, attempting to reconstruct the genome from the sequencing reads only. It’s suitable when we’re unsure about the organism or when it’s genetically diverse (so there is no single reference genome that is representative of the full diversity observed in the species). However, it’s computationally intensive and challenging to achieve highly accurate results due to difficulties in handling gaps and repetitive regions.\n\nReference-based assembly: this method uses a known genome as a guide. It aligns the reads to the reference genome and identifies new variations from those reads. It’s less computationally demanding and works well when we have a good idea of the organism’s identity. Yet, it might not perform well if the organism is significantly different from the reference genome.\n\nIn our study of Vibrio cholerae, we have opted for the de novo approach. Although there are many high-quality genomes available on NCBI, this species is notorious for having a plastic genome, with the presence of several mobile genetic elements and gene exchange happening through conjugation and phage-mediated transduction (Montero et al. 2023 and Ramamurthy et al. 2019). Therefore, a reference-based assembly may not be the most suited for this species, as we might miss important genes from our samples, if they are not present in the reference genome that we choose.\nHere’s a breakdown of the steps we’ll take:\n\nDownsample the FASTQ files for a target depth of coverage of approximately 100x.\nAssemble the reads into contiguous fragments.\nPolish those fragments to correct systematic sequencing errors common in Nanopore data.\nAnnotate the assembled fragments, by identifying the position of known bacterial genes.\n\nWe’ll provide a detailed procedure for a single sample, and later, we’ll offer a script that automates these steps for multiple samples simultaneously.\n\n\n\n\n\n\nTerminology: genome coverage\n\n\n\nGenome coverage refers to the average number of times a specific base pair in a genome is read or sequenced during the process of DNA sequencing. It indicates how thoroughly the genome has been sampled by the sequencing technique. Higher coverage means that a base pair has been read multiple times, increasing the confidence in the accuracy of the sequencing data for that particular region of the genome. Genome coverage is an important factor in determining the quality of genome assemblies and the ability to detect variations (such as new mutations or sequence rearragements)."
  },
  {
    "objectID": "materials/02-assembly/03-genome_assembly.html#step-by-step-bacterial-assembly",
    "href": "materials/02-assembly/03-genome_assembly.html#step-by-step-bacterial-assembly",
    "title": "8  Genome assembly",
    "section": "8.2 Step-by-step bacterial assembly",
    "text": "8.2 Step-by-step bacterial assembly\nAs we mentioned earlier, de novo assembly of genomes is a challenging process. Ideally, we want to achieve a “perfect genome”, i.e. a complete representation of the individual’s genome, with no errors and no missing gaps. Although challenging, it is possible to achieve near-perfect genomes, in particular when multiple types of data are available: long reads for assembling more difficult regions and short reads for correcting the high error rates of long reads (Wick et al. 2023).\nIn this section, we introduce a simplified version of the tutorial “Assembling the perfect bacterial genome”, which is suitable for nanopore-only data.\nWe start by activating our software environment, to make all the necessary tools available:\nmamba activate assembly\n\n8.2.1 Sampling: rasusa\nThe first step in our analysis is to sample a fraction of our original reads to match a minimum specified genome coverage. This may seem counterintuitive, as we may expect that more data is always better. While this is generally the case, it turns out that for genome assembly there is a plateau at which the tradeoff between having too much data doesn’t outweight the computational costs that come with it. For example, assembling a Vibrio genome with an average depth of coverage of 500x might take 5h, while one with 100x might take 1h, with very similar results in the final assembly. The computational burden is especially relevant when we are running analysis on a local computer, rather than on a HPC cluster (where we can run analysis in parallel), as each sample will have to be processed sequentially.\nTo be able to run all analyses on a local computer, we’ll adjust the number of FASTQ reads we’re working with to reach a coverage depth of about 100 times, using a tool called Rasusa. This tool is designed to handle reads of various lengths, which is common with ONT data. It randomly picks reads to make sure the average coverage across a genome of a certain size (chosen by us) is achieved.\nThis process considers the lengths of the reads. For instance, if the average ONT read is longer, we’ll need fewer reads to cover the genome compared to when the average read is shorter. Imagine our genome is around 1 million bases long, and we aim for a 20-fold coverage. If our reads average around 1,000 bases, we’d need about 200,000 reads. But if the average read is 10,000 bases, we’d only need 2,000 reads instead. This is a simplified explanation, as our reads have various lengths, but it captures what Rasusa is working towards.\nTo run Rasusa on a single sample (in this example we are doing barcode26), the following commands can be used:\n# create output directory\nmkdir results/rasusa\n\n# temporarily combine FASTQ files from Guppy\ncat data/fastq_pass/barcode26/*.fastq.gz &gt; results/rasusa/combined_barcode26.fastq.gz\n\n# downsample files\nrasusa --input results/rasusa/combined_barcode26.fastq.gz --coverage 100 --genome-size 4m --output results/rasusa/barcode26_downsampled.fastq.gz\n\n# remove the combined FASTQ file to save space\nrm results/rasusa/combined_barcode26.fastq.gz\n\nDuring the basecalling procedure, Guppy will often generate multiple FASTQ files for each barcode. We first need to combine these into a single file, as rasusa only accepts a single file as input. We achieved this using the cat command.\nFor rasusa we used the following options:\n\n--input specifies the input FASTQ file.\n--coverage specifies our desired depth of coverage; 100x is a good minimum for genome assembly.\n--genome-size specifies the predicted genome size. Of course we don’t know the exact size of our genome (we’re assembling it after all), but we known approximately how big the Vibrio genome is based on other genomes available online, such as this genome from a 7PET strain, which is 4.2 Mb long.\n--output specifies the output FASTQ file.\n\nAt the end we removed the combined FASTQ file, to save space on the computer (these files can be quite large!).\n\nAfter rasusa runs, it informs us of how many reads it sampled:\n Target number of bases to subsample to is: 400000000\n Gathering read lengths...\n 202685 reads detected\n Keeping 167985 reads\n Actual coverage of kept reads is 100.00x\n Done 🎉\nNote that the sampling procedure is random, so you may get slightly different results every time you run it.\nSometimes, you may not have enough reads to achieve the desired coverage. In that case, rasusa instead keeps all the reads, with an appropriate message, for example for our barcode25 sample we got:\nRequested coverage (100.00x) is not possible as the actual coverage is 37.27x - output will be the same as the input\n\n\n\n\n\n\nLow genome coverage\n\n\n\nFor genome assembly, we do not recommend that you go much lower than 100x coverage. A low depth of coverage leads to difficulty in:\n\nDistinguishing errors from the true base; low coverage generally results in higher error rates.\nGenerating a contiguous genome assembly; low coverage generally results in a more fragmented genome.\n\n\n\n\n\n8.2.2 Assembly: flye\nNow that we have downsampled our reads, we are ready for the next step of the analysis, which is the genome assembly. We will use the software Flye, which is designed for de novo assembly of long-read sequencing data, particularly from technologies like Oxford Nanopore or PacBio. However, many other assembly tools are available, and you could explore alternatives.\nContinuing from our example barcode26 sample, here is the flye command that we could use:\n# create output directory\nmkdir results/flye\n\n# run the assembly - this step takes some time to run!\nflye --nano-raw results/rasusa/barcode26_downsampled.fastq.gz --threads 8 --out-dir results/flye/isolate2/ --asm-coverage 100 --genome-size 4m\n\n--nano-raw specifies the input FASTQ file with our reads.\n--threads specifies how many CPUs we have available for parallel computations.\n--out-dir specifies the output directory for our results; in this case we used a more friendly name that corresponds to our isolate ID.\n--asm-coverage and --genome-size specifies the coverage we want to consider for a certain genome size; in our case these options could be left out, as we already downsampled our FASTQ files to this depth; but we can keep the same values here that we used for rasusa.\n\nOne important thing to note about Flye is that it has two ways to specify input files: --nano-raw (which we used) and --nano-hq. As the ONT chemistry and basecalling software improve, the error rates of the reads also improve. If your reads are predicted to have &lt;5% error rate, then you can use --nano-hq option to specify your input. This is the case if you used Guppy version 5 or higher with basecalling in super accurate (SUP) mode. This basecalling mode takes substantially longer to run, but it does give you better assemblies, so it may be worth the extra time, if you want the highest-quality assemblies possible. In our example, the basecalling was done in “fast” mode, so we used the --nano-raw input option instead.\nAfter flye completes running, it generates several output files:\nls results/flye/isolate2\n00-assembly   40-polishing                assembly_graph.gfa  params.json\n10-consensus  assembly.fasta              assembly_graph.gv\n20-repeat     assembly.fasta.fai          assembly_info.txt\n30-contigger  assembly.fasta.map-ont.mmi  flye.log\nThe main file of interest to us is assembly.fasta, which is the genome assembly in the standard FASTA file format. Another file of interest is flye.log, which contains information at the bottom of the file about the resulting assembly:\ntail -n 8 results/flye/isolate2/flye.log\nTotal length:   4219745\nFragments:      3\nFragments N50:  3031589\nLargest frg:    3031589\nScaffolds:      0\nMean coverage:  98\nWe can see for our example run that we have a total assembly length of ~4.2 Mb, which matches our expected genome size. The number of final fragments was 3, and given that Vibrio cholerae has 2 chromosomes, this is not bad at all - we must have been able to assembly the two chromosomes nearly completely. The largest fragment is ~3 Mb, which again from our knowledge of other Vibrio genomes, probably corresponds to chromosome 1. And we can see the final depth of coverage of the genome is 98x, which makes sense, since we sampled our reads to achieve approximately 100x.\nSometimes you may end up with more fragmented genome assemblies, in particular if your coverage is not good. For the example we showed earlier, where rasusa reported our reads were only enough for ~37x coverage, our flye assembly resulted in more than 30 fragments. While this is worse than a near-complete assembly, it doesn’t mean that the genome assembly is useless. We should still have recovered a lot of the genes, and even a fragmented assembly can be used to identify sequence types and pathogenic and AMR genes.\n\n\n8.2.3 Polishing: medaka\nAs previously mentioned, ONT reads generally come with higher error rates compared to other technologies like the short-read Illumina sequencing, which often has less than 1% error rate. Even with the most recent ONT chemistry updates, the error rates are still around 5%, a significant figure that can potentially introduce incorrect base calls into our final assembly. While the flye software does address some of these errors, its error-correcting algorithm wasn’t tailored specifically to address the systematic errors observed in ONT data. Consequently, as an added step following assembly, we can review the FASTA file of our assembly and rectify any potential errors that may have been retained from the original reads. This process is commonly known as polishing the assembly.\nTo perform polishing, we can employ the Medaka software developed by ONT. More specifically, we can utilize the medaka_consensus tool, which is expressly designed to complement genome assemblies created using Flye. The approach involves aligning the original reads to the newly assembled genome. Then, it involves identifying the correct base for each position in the genome, determined by analyzing the “pileup” of reads that align to that specific position. This is facilitated by a machine learning model trained on ONT data, which accounts for the distinct error patterns and biases inherent in this type of sequencing information.\nContinuing our example for barcode26 / isolate2, we could run the following command:\nmedaka_consensus -t 8 -i results/rasusa/barcode26_downsampled.fastq.gz -d results/flye/isolate2/assembly.fasta -o results/medaka/isolate2 -m r941_min_fast_g507\n\n-t specifies the number of CPUs (threads) available to use for parallel computation steps.\n-i specifies the FASTQ file with the reads used for the assembly.\n-d specifies the FASTA file with the assembly we want to polish.\no specifies the output directory for our results.\n-m specifies the Medaka model to use for identifying variants from the aligned reads.\n\nThe last option requires some further explanation. The Medaka model name follows the structure {pore}_{device}_{mode}_{version}, as detailed in the medaka models documentation. For example, the Medaka model we specified was “r941_min_fast_g507”, which means we used R9.4.1 pores, sequenced on a MinION, basecalling in “fast” mode using Guppy version 5.0.7. A list of supported models can be found on the medaka GitHub repository. In reality, we used Guppy version 6, but for recent versions of Guppy (&gt;6) there is no exact matching model. The recommendation in that case is to use the model for the latest version available.\nThe output from this step generates several files:\nls results/medaka/isolate2\ncalls_to_draft.bam  calls_to_draft.bam.bai  consensus.fasta  consensus.fasta.gaps_in_draft_coords.bed  consensus_probs.hdf\nThe most important of which is the file consensus.fasta, which contains our final polished assembly.\n\n\n8.2.4 Annotation: bakta\nAlthough we now have a genome assembly, we don’t yet know which genes might be present in our assembly or where they are located. This process is called genome annotation, and for bacterial genomes we can do this using the software Bakta. This software takes advantage of the vast number of bacterial gene sequences in public databases, to aid in the identification of genes in our new assembly. Bakta achieves this by looking for regions of our genome that have high similarity with those public sequences.\n\nbakta database\nIn order to run bakta we first need to download the database that it will use to aid the annotation. This can be done with the bakta_db command. There are two versions of its database: “light” and “full”. The “light” database is smaller and results in a faster annotation runtime, at the cost of a less accurate/complete annotation. The “full” database is the recommended for the best possible annotation, but it is much larger and requires longer runtimes.\nFor the purposes of our tutorial, we will use the “light” database, but if you were running this through your own samples, it may be desirable to use the “full” database. To download the database you can run the following command (if you are attending our workshop, please don’t run this step, as we already did this for you):\n# make a directory for the database\nmkdir -p resources/bakta_db\n\n# download the \"light\" version of the database\nbakta_db download --output resources/bakta_db/ --type light\nAfter download you will see several newly created files in the output folder:\nls resources/bakta_db/db-light/\namrfinderplus-db  bakta.db                       ncRNA-genes.i1p    oric.fna  pfam.h3p   rRNA.i1p\nantifam.h3f       expert-protein-sequences.dmnd  ncRNA-regions.i1f  orit.fna  pscc.dmnd  rfam-go.tsv\nantifam.h3i       ncRNA-genes.i1f                ncRNA-regions.i1i  pfam.h3f  rRNA.i1f   sorf.dmnd\nantifam.h3m       ncRNA-genes.i1i                ncRNA-regions.i1m  pfam.h3i  rRNA.i1i   version.json\nantifam.h3p       ncRNA-genes.i1m                ncRNA-regions.i1p  pfam.h3m  rRNA.i1m\n\n\n\n\n\n\nImportant\n\n\n\nYou only need to download the bakta database once, and it may be a good idea to save it in a separate folder that you can use again for a new analysis. This should save you substantial time and storage space.\n\n\n\n\nbakta annotation\nTo run the annotation step we use the bakta command, which would be the following for our barcode26 / isolate2 sample:\n# create output directory\nmkdir results/bakta\n\n# run the annotation step\nbakta --db resources/bakta_db/db-light/ --output results/bakta/isolate2 --threads 8 results/medaka/isolate2/consensus.fasta\n\n--db is the path to the database folder we downloaded earlier.\n--output is the directory we want to output our results to.\n--threads is the number of CPUs (threads) we have available for parallel computation.\nAt the end of the command we give the path to the FASTA file that we want to annotate.\n\nSeveral results files are generated:\nls results/bakta/isolate2\nconsensus.embl  consensus.ffn  consensus.gbff  consensus.hypotheticals.faa  consensus.json  consensus.png  consensus.tsv\nconsensus.faa   consensus.fna  consensus.gff3  consensus.hypotheticals.tsv  consensus.log   consensus.svg  consensus.txt\nMany of these files contain the same information but in different file formats. The main file of interest is consensus.gff3, which we will use in a later chapter covering phylogenetic analysis. The tab-delimited file consensus.tsv can conveniently be opened in a spreadsheet program such as Excel.\nAlso, using the command line tool grep, we can quickly search for genes of interest. For example, we can see if the genes from the CT phage (CTX) are present in your samples (these include the toxin-encoding genes ctxA and ctxB):\ngrep -i \"ctx\" results/bakta/isolate2/consensus.tsv\ncontig_2        cds     2236513 2237289 +       LBGGEL_19385    ctxA    cholera enterotoxin catalytic subunit CtxA      BlastRules:WP_001881225, NCBIProtein:AAF94614.1\ncontig_2        cds     2237286 2237660 +       LBGGEL_19390    ctxB    cholera enterotoxin binding subunit CtxB        BlastRules:WP_000593522, NCBIProtein:AAF94613.1, SO:0001217, UniRef:UniRef50_P01556\nBoth subunits are found in our assembly, suggesting our isolate is a pathogenic strain of Vibrio cholerae.\nAnd this concludes our assembly steps: we now have an annotated genome assembly produced from our ONT reads."
  },
  {
    "objectID": "materials/02-assembly/03-genome_assembly.html#sec-workflow",
    "href": "materials/02-assembly/03-genome_assembly.html#sec-workflow",
    "title": "8  Genome assembly",
    "section": "8.3 Assembly workflow",
    "text": "8.3 Assembly workflow\nIn the previous section we applied our assembly workflow to a single sample. But what if we had 20 samples? It would be quite tedious and error-prone to have to copy/paste and modify these commands so many times.\nInstead, we can automate our analysis using some programming techniques, such as a for loop, to repeat the analysis across a range of samples. This is what we’ve done for you, having already prepared a shell script that runs through these steps sequentially for any number of samples (if you are interested, the full script is available here). Our script is composed of two parts:\n\nSettings: at the top of the script you will find a section #### Settings ####, where you can define several options to run the analysis (we will detail these below).\nPipeline: further down you find a section #### Assembly pipeline ####, which contains the commands to run the full analysis from raw sequences to an annotated assembly. The code we use looks more complex, as we make use of several (advanced) programming techniques, but the basic steps are the same as those we detailed step-by-step in the previous section.\n\nAs a user of our script, the most important part is the #### Settings ####. The following settings are available:\n\nsamplesheet → the path to a CSV file with at least two columns:\n\nsample - sample name of your choice for each barcode; for example “isolate1”, “isolate2”, etc.\nbarcode - barcode folder names of each samples; this should essentially match the folder names in the fastq_pass folder output by the Guppy basecaller, for example “barcode01”, “barcode02”, “barcode34”, etc.\nyou can include other columns in this sheet (such as metadata information), as long as the first two columns are the ones explained above.\n\nfastq_dir → the path to the directory where the FASTQ files are in. The pipeline expects to find individual barcode folders within this, so we use the fastq_pass directory generated by the Guppy basecaller.\noutdir → the path to the output directory where all the results files will be saved.\nthreads → how many CPUs are available for parallel processing.\ngenome_size → the predicted genome size for the bacterial we are assembling (4m is recommended for Vibrio).\ncoverage → the desired coverage for subsampling our reads to.\nmedaka_model → the medaka model to use for the polishing step (as explained above).\nbakta_db → the path to the bakta database folder.\n\nHere is an example of the samplesheet for our samples:\nsample,barcode\nisolate01,barcode25\nisolate02,barcode26\nisolate03,barcode27\nisolate04,barcode28\nisolate05,barcode29\nisolate06,barcode30\nisolate07,barcode31\nisolate08,barcode32\nisolate09,barcode33\nisolate10,barcode34\nWe have 10 isolates (number 1 to 10), corresponding to different barcodes. The samplesheet therefore makes the correspondence between each sample’s name and its respective barcode folder.\nOne we have this samplesheet and we edit all the options in the script, we can run the script using bash:\nbash scripts/02-assembly.sh\nOur script in this case was called 02-assembly.sh and we had saved it in a folder called scripts. Note that all the file paths specified in the “Settings” of the script are relative to the folder where we run the script from. In our case, we had a folder named awd_workshop, where we are running the analysis from, and that is where we ran the script from.\nThe script will take quite a while to run (up to 1h per sample, using 8 CPUs on our computers), so you may have to leave your computer doing the hard work overnight. As it runs, the script prints some progress messages on the screen:\nProcessing sample 'isolate01' with barcode 'barcode25'\n        2023-08-09 22:41:38      Concatenating reads...\n        2023-08-09 22:41:42      Subsampling reads with rasusa...\n        2023-08-09 22:42:47      Assembling with flye...\n        2023-08-09 22:55:37      Polishing with medaka...\n        2023-08-09 22:59:41      Annotating with bakta...\n        2023-08-09 23:24:47      Finished assembly pipeline for 'isolate01'.\n                                 Assembly file in: results/assemblies/isolate01.fasta\n                                 Annotation file in: results/assemblies/isolate01.gff\n\nProcessing sample 'isolate02' with barcode 'barcode26'\n        2023-08-09 23:24:47      Concatenating reads...\n        2023-08-09 23:24:56      Subsampling reads with rasusa...\n        2023-08-09 23:26:32      Assembling with flye...\n        2023-08-09 23:52:36      Polishing with medaka...\nSeveral output files are generated in the directory you specified as outdir. Here is what we have for our example data:\nls results/assemblies\n01-rasusa        isolate02.gff    isolate06.fasta  isolate09.gff\n02-flye          isolate03.fasta  isolate06.gff    isolate10.fasta\n03-medaka        isolate03.gff    isolate07.fasta  isolate10.gff\n04-bakta         isolate04.fasta  isolate07.gff    summary_metrics.csv\nisolate01.fasta  isolate04.gff    isolate08.fasta\nisolate01.gff    isolate05.fasta  isolate08.gff\nisolate02.fasta  isolate05.gff    isolate09.fasta\nWe get a folder with the results of each step of the analysis (this matches what we went through in detail in the step-by-step section) and two files per sample: a FASTA file with the polished assembly and a GFF file with the gene annotations. We also get a CSV file called summary_metrics.csv, which contains some useful statistics about our assemblies. We will look at these in the next chapter on quality control."
  },
  {
    "objectID": "materials/02-assembly/03-genome_assembly.html#sec-ex-assembly",
    "href": "materials/02-assembly/03-genome_assembly.html#sec-ex-assembly",
    "title": "8  Genome assembly",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data.\n\n\n\n\n\n\nRunning assembly script\n\n\n\n\n\n\n\nAs covered in the sections above, the genome assembly process involves several steps and software. We provide a script that performs the assemly pipeline for a set of samples specified by the user (using a for loop as detailed in Section 8.3).\n\nIn the folder scripts (within your analysis directory) you will find a script named 02-assembly.sh.\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. We already include default settings which are suitable for the “Ambroise 2023” data, but you may have to change some settings to suit your data.\n#### Analysis #### this is where the assembly workflow is run on each sample as detailed in Section 8.3. You should not change the code in this section, although examining it is a good way to learn about Bash programming (this one is quite advanced, so don’t worry if you don’t understand some of it).\n\nOne of the main inputs to the script is a CSV file with two columns specifying your sample IDs and their respective barcode folder name. Using Excel, create this file for your samples, as explained in Section 8.3. Save it as samplesheet.csv in your analysis directory.\nActivate the software environment: mamba activate assembly\nRun the script using bash scripts/02-assembly.sh. If the script is running successfully it should print a message on the screen as the samples are processed. Depending on how many barcodes you have, this will take quite a while to finish (up to 1h per sample). \nOne the analysis finishes you can confirm that you have several files in the output folder. We will analyse these files in the next chapter\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nWe opened the script 02-assembly.sh and these are the settings we used:\n\nsamplesheet=\"samplesheet.csv\" - the name of our samplesheet CSV file, detailed below.\nfastq_dir=\"data/fastq_pass\" - the name of the directory where we have our barcode folders from basecalling with Guppy.\noutdir=\"results/assemblies\" - the name of the directory where we want to save our results.\nthreads=\"16\" - the number of CPUs we have available for parallel computations. You can check how many CPUs you have using the command nproc --all.\ngenome_size=\"4m\" - the predicted genome size for the organism we are assembling. We use the Vibrio cholerae genome size.\ncoverage=\"100\" - the coverage we want to downsample our sequencing reads to.\nmedaka_model=\"r941_min_hac_g507\" - the model for the Medaka software. For the “Ambroise 2023” data basecalling was performed using the high accuracy (“hac”) mode, sequenced on a MinION platform using R9.4.1 pores. So, we chose the model that fits with this.\nbakta_db=\"resources/bakta_db/db-light/\" - the path to the Bakta database used for gene annotation. This was already pre-downloaded for us.\n\nOur samplesheet.csv file looked as follows:\nsample,barcode\nCTMA_1402,barcode01\nCTMA_1421,barcode02\nCTMA_1427,barcode05\nCTMA_1432,barcode06\nCTMA_1473,barcode09\nWe used the sample identifiers from the original publication, and their respective barcodes. If these were our own samples, we would have used identifiers that made sense to us.\nWe then ran the script using bash scripts/02-assembly.sh. The script prints a message while it’s running:\nProcessing sample 'CTMA_1402' with barcode 'barcode01'\n        2023-08-09 22:41:38      Concatenating reads...\n        2023-08-09 22:41:42      Subsampling reads with rasusa...\n        2023-08-09 22:42:47      Assembling with flye...\n        2023-08-09 22:55:37      Polishing with medaka...\n        2023-08-09 22:59:41      Annotating with bakta...\n        2023-08-09 23:24:47      Finished assembly pipeline for 'isolate01'.\n                                 Assembly file in: results/assemblies/isolate01.fasta\n                                 Annotation file in: results/assemblies/isolate01.gff\n\nProcessing sample 'CTMA_1421' with barcode 'barcode02'\n        2023-08-09 23:24:47      Concatenating reads...\n        2023-08-09 23:24:56      Subsampling reads with rasusa...\n        2023-08-09 23:26:32      Assembling with flye...\n        2023-08-09 23:52:36      Polishing with medaka...\nThis took a long time to run, it was around 5h for us.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for genes of interest\n\n\n\n\n\n\n\nThe varG gene is part of the antibiotic resistance var regulon in Vibrio cholerae (source).\nUsing the command line tool grep, search for this gene in the annotation files produced by Bakta.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nSee Section 8.2.4.2 for an example of how we did this for the ctxA and ctxB genes.\nThe pipeline script we used outputs the Bakta results to results/assemblies/04-bakta/SAMPLE/consensus.tsv (where “SAMPLE” is the sample name).\nRemember that you can use the * wildcard to match multiple file/directory names\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nTo search for this gene across all our samples, we did:\ngrep -i \"varG\" results/assemblies/04-bakta/*/consensus.tsv\nresults/assemblies/04-bakta/CTMA_1402/consensus.tsv:contig_2    cds     1452034 1453002 +       FHBCKO_14010    varG    VarG family subclass B1-like metallo-beta-lactamase     NCBIProtein:WP_000778180.1, SO:0001217, UniRef:UniRef50_A0A290TY11\nresults/assemblies/04-bakta/CTMA_1421/consensus.tsv:contig_4    cds     1469496 1470620 -       GBDILF_14250    varG    VarG family subclass B1-like metallo-beta-lactamase     NCBIProtein:WP_000778180.1, SO:0001217, UniRef:UniRef50_A0A290TY11\nresults/assemblies/04-bakta/CTMA_1427/consensus.tsv:contig_1    cds     1682677 1683801 -       CJLKCO_09705    varG    VarG family subclass B1-like metallo-beta-lactamase     NCBIProtein:WP_000778180.1, SO:0001217, UniRef:UniRef50_A0A290TY11\nresults/assemblies/04-bakta/CTMA_1432/consensus.tsv:contig_2    cds     1363185 1364309 +       BDJHLF_13895    varG    VarG family subclass B1-like metallo-beta-lactamase     NCBIProtein:WP_000778180.1, SO:0001217, UniRef:UniRef50_A0A290TY11\nresults/assemblies/04-bakta/CTMA_1473/consensus.tsv:contig_2    cds     1373351 1374475 +       DGGMCP_08610    varG    VarG family subclass B1-like metallo-beta-lactamase     NCBIProtein:WP_000778180.1, SO:0001217, UniRef:UniRef50_A0A290TY11\nWe can see that all 5 samples contain this gene. According to this gene’s description on CARD, this suggests that all of these isolates might be resistant to antimicrobial drugs using penicillins, carbapenems or cephalosporins."
  },
  {
    "objectID": "materials/02-assembly/03-genome_assembly.html#summary",
    "href": "materials/02-assembly/03-genome_assembly.html#summary",
    "title": "8  Genome assembly",
    "section": "8.5 Summary",
    "text": "8.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nDe novo genome assembly involves reconstructing a complete genome sequence without relying on a reference genome.\nGenome coverage refers to the average number of times each base in the genome is sequenced.\nHigher coverage provides more confident assembly, especially in repetitive regions, but it can be computationally intensive and expensive. Low coverage leads to more fragmented and less accurate assemblies.\nKey steps in Nanopore data assembly, along with relevant software, include::\n\nDownsample reads using rasusa to optimize coverage, typically aiming for around 100-150x.\nAssembly with flye, a tool specialised for long-read technologies.\nEnhance accuracy through polishing with medaka, which corrects systematic ONT errors.\nAnnotate genomes using bakta to identify genes and other genome features.\n\nAutomation scripts simplify assembly across multiple samples by executing the same commands for each, ensuring consistency and saving time.\nThe provided script requires a samplesheet (CSV file) containing sample IDs and barcodes. Additionally, selecting the appropriate medaka error-correction model is crucial for accurate results."
  },
  {
    "objectID": "materials/02-assembly/04-assembly_quality.html#assembly-quality",
    "href": "materials/02-assembly/04-assembly_quality.html#assembly-quality",
    "title": "9  Assembly quality",
    "section": "9.1 Assembly quality",
    "text": "9.1 Assembly quality\nThe quality of a genome assembly can be influenced by various factors that impact its accuracy and completeness, from sample collection, to sequencing, to the bioinformatic analysis. To assess the quality of an assembly, several key indicators can be examined:\n\nCompleteness: the extent to which the genome is accurately represented in the assembly, including both core and accessory genes.\nContiguity: refers to how long the sequences are without gaps. A highly contiguous assembly means that longer stretches are assembled without interruptions, the best being chromosome-level assemblies. A less contiguous assembly will be represented in more separate fragments.\nContamination: the presence of DNA from other species or sources in the assembly.\nAccuracy/correctness: how closely the assembled sequence matches the true sequence of the genome.\n\nEvaluating these factors collectively provides insights into the reliability and utility of the genome assembly for further analysis and interpretation.\nFor example, before we even did our assembly, we had already checked for signs of sample contamination from our sequencing reads using Mash. Since our samples were generated from cultured colonies with vibrio-selective medium (TCBS), we are mostly expecting V. cholerae DNA, but other certain other Vibrio or enterobacterial species could end up in our cultured samples too. Our analysis indicated no concerning signs of contamination, which would have affected the accuracy of our assemblies.\nLet’s now turn to some of the other metrics to help us assess our assemblies’ quality.\n\n\n\nImage source: Bretaudeau et al. (2023) Galaxy Training"
  },
  {
    "objectID": "materials/02-assembly/04-assembly_quality.html#contiguity",
    "href": "materials/02-assembly/04-assembly_quality.html#contiguity",
    "title": "9  Assembly quality",
    "section": "9.2 Contiguity",
    "text": "9.2 Contiguity\nThe assembly script we used in the previous chapter conveniently output a CSV file with some metrics of interest. This file can be found in results/assemblies/summary_metrics.csv. You can open it with a spreadsheet software such as Excel from our file browser . Alternatively you can print it directly on the terminal in a nice format using the column command, as shown here for the samples we’ve been using as an example:\ncolumn -t -s \",\" results/assemblies/summary_metrics.csv\nsample     total_reads  downsampled_reads  assembly_length  fragments  n50      largest  coverage\nisolate01  121104       121104             4188802          35         369202   841273   31\nisolate02  202685       168327             4240196          5          3031958  3031958  97\nisolate03  247162       226566             4071558          2          3022820  3022820  100\nisolate04  262453       151775             4228717          3          3040641  3040641  98\nisolate05  157356       157356             4045552          229        29056    295945   21\nisolate06  286582       286582             4240860          43         252998   416986   44\nisolate07  187090       187090             4186377          38         214593   669735   40\nisolate08  120555       120555             4083432          19         582736   885447   43\nisolate09  121991       121991             4195953          113        105571   179265   23\nisolate10  102589       102589             4214473          3          3026130  3026130  48\nThe columns are:\n\nsample - our sample ID.\ntotal_reads - the total number of reads in our FASTQ files.\ndownsampled_reads - the number of reads downsampled by rasusa to achive a coverage of 100x.\nassembly_length - the total length of our assembled fragments.\nfragments - the number of fragments into which the reads were assembled to by the software flye.\nn50 - a metric indicating the length of the shortest fragment, from the group of fragments that together represent at least 50% of the total genome. A higher N50 value suggests better contig lengths.\nlargest - the largest assembled fragment.\ncoverage - the achieved coverage in the final genome assembly (this might be slightly lower than the coverage we intended when downsampling the reads).\n\nTo interpret these statistics, it helps to compare them with other well-assembled Vibrio cholerae genomes. For example, let’s take the first O1 El Tor genome that was sequenced, N16961, as our reference for comparison. This genome is 4 Mb long and is composed of 2 chromosomes.\nWe can see that all of our assemblies reached a total length of around 4 to 4.2 Mb, which matches the expected length from our reference genome. This indicates that we managed to assemble most of the expected genome. However, we can see that there is a variation in the number of fragments in the final assemblies (i.e. their contiguity). Isolates 2, 3, 4 and 10 were assembled to a small number of fragments each, suggesting good assemblies. In fact, for “isolate03” we only have 2 fragments, which indicates that for this sample we achieved a chromosome-level assembly!\nFor several other isolates our assemblies were more fragmented, in particular isolate05 and isolate09, which had hundreds of fragments. This indicates less contiguous sequences. We can see why this might be: there is a clear correlation between the number of fragments and the sequencing coverage. The lower the coverage is (i.e. the fewer times we sequenced each position of the genome), the more fragmented the assembly is. Clearly, a depth of coverage of around 100x is sufficient to get highly contiguous assemblies, whereas coverages below 50x result in more fragmented assemblies.\nNote that there is no clear correlation between the number of reads and the final coverage. For example, isolates 4 and 5 had roughly the same number of reads used for the assembly (around 150k), but the latter was much more fragmented. This is likely because the library for that sample was composed of shorter reads (this would have to be verified from the quality report from the ONT sequencing platform).\nDespite some of our assemblies being more fragmented, it doesn’t mean they are of no use to us. In fact, we may have still recovered most of the genes expected for this species, and we can therefore use them to investigate their strain types and the presence of toxigenic and antimicrobial resistance genes.\n\n9.2.1 Assembly graph\nAnother approach to evaluate the continuity of our assembly is to examine the assembly graph generated by Flye. During genome assembly, Flye constructs a graph illustrating how it infers the various fragments connect to one another.\nFigure 9.1 presents examples of genome graphs. The initial example displays an assembly composed of two circular fragments, which is the optimal outcome for our species of interest, Vibrio cholerae, as it possesses two circular chromosomes. The next example portrays a more fragmented genome with numerous small, unconnected fragments, along with other fragments linked at different points within the graph. This suggests difficulties in bridging certain gaps during genome assembly. The final example exhibits a highly intricate graph featuring multiple loops, indicating an inability to establish a likely order for the fragments in the final assembly. Such complexity is more common in species with highly repetitive sequences, especially when working solely with short-read data.\n\n\n\nFigure 9.1: Assembly graph examples visualised using Bandage.\n\n\nThe software Bandage can be used to visualise the assembly graphs generated by Flye. It is a user-friendly graphical tool with the following steps:\n\nClick File &gt; Load graph.\nLocate the Flye output folder (using our script, this is located in results/assemblies/02-flye/SAMPLE-ID/assembly_graph.gfa).\nClick Draw graph."
  },
  {
    "objectID": "materials/02-assembly/04-assembly_quality.html#completeness",
    "href": "materials/02-assembly/04-assembly_quality.html#completeness",
    "title": "9  Assembly quality",
    "section": "9.3 Completeness",
    "text": "9.3 Completeness\nWe now turn to assessing genome completeness, i.e. whether we managed to recover most of the known Vibrio cholerae genome, or whether we have large fractions missing.\nWe can assess this by using CheckM2. This tool assesses the completeness of the assembled genomes based on other similar organisms in public databases, in addition to contamination scores. CheckM2 is mostly suited for prokaryotic species. For other organisms, the BUSCO software is a good alternative, as it supports all species and is also more stringent in determining completeness.\n\nCheckM2 was specifically designed for working with metagenome-assembled genomes (MAGs). These are genomes created from a mixture of DNA from various species. CheckM2 has two main roles: it determines whether a given genome is complete and checks for signs of contamination in the genome. Contamination might occur when different organisms accidentally get combined in the assembly due to shared DNA sequences. Even though CheckM2 is intended for MAGs, it can also evaluate the completeness of genomes assembled from samples that aren’t mixed (which is our case).\nThis software employs machine learning models that were trained using many extensively annotated public sequences from RefSeq. These models help CheckM2 estimate how complete and uncontaminated the provided genome assemblies are. In our case, we anticipate a minimal level of contamination, as confirmed by Mash. However, we’re still uncertain about the completeness of our genomes.\nWe start our analysis by activating our software environment, to make all the necessary tools available:\nmamba activate checkm\n\n9.3.1 checkm database\nAs part of its analysis, CheckM2 also does a rapid gene annotation. It does this by comparing our sequences to a database, using the super-fast local alignment software DIAMOND. This process enables CheckM2 to estimate the number of genes identified in our genomes, allowing us to make comparisons with the annotation of our organism’s reference genome.\nFor the gene annotation step, CheckM2 requires a DIAMOND database. This can be downloaded with the following command (don’t run this if you are attending our workshop, as we already did it for you):\ncheckm2 database --download --path resources/\nThis command will automatically download the database into a folder named CheckM2_database with a the database file inside it called uniref100.KO.1.dmnd. We use this file in the next step.\n\n\n\n\n\n\nImportant\n\n\n\nYou only need to download the database once. To save space (and time downloading it), it’s a good idea to save the database in a shared folder that you can use across multiple analysis.\n\n\n\n\n9.3.2 checkm predict\nOnce the database is available, running CheckM2 is done with the checkm2 predict command, exemplified in this shell script:\n#!/bin/bash\n\n# make output directory\nmkdir results/checkm2\n\n# run checkm\ncheckm2 predict \\\n  --input results/assemblies/*.fasta \\\n  --output-directory results/checkm2/ \\\n  --database_path resources/CheckM2_database/uniref100.KO.1.dmnd \\\n  --lowmem --threads 12\nThe options we used are:\n\n--input - to specify the input files. Several files can be given as input, so we used the * wildcard to match all the FASTA files output by our assembly workflow script.\n--output-directory - the name of the output directory to store the results.\n--database_path - is the path to the database file we downloaded.\n--lowmem - in case you run this on a computer with low RAM memory (such as a laptop), this option will make sure the analysis runs successfully.\n--threads - to use multiple CPUs for parallel processing, which you can adjust depending on how many CPUs you have available on your computer (you can check this with the command nproc --all).\n\nThe main output file from CheckM2 is a tab-delimited file called quality_report.tsv, which can be opened in a spreadsheet software such as Excel. Here is an example result (for brevity, we only show some columns of most interest):\nName       Completeness  Contamination  Genome_Size  GC_Content  Total_Coding_Sequences\nisolate01  92.51         5.19           4192535      0.48        5375\nisolate02  89.72         4.0            4243727      0.48        5039\nisolate03  85.39         3.52           4074948      0.47        4741\nisolate04  85.76         3.84           4232274      0.48        5024\nisolate05  85.01         2.15           4050061      0.48        5728\nisolate06  82.81         4.58           4244765      0.48        5306\nisolate07  82.8          5.93           4189945      0.48        5267\nisolate08  82.82         5.07           4087172      0.48        5078\nisolate09  91.59         5.67           4200084      0.48        5724\nisolate10  86.19         3.99           4217448      0.48        5133\nThese columns indicate:\n\nName - our sample name.\nCompleteness - how complete our genome was inferred to be as a percentage; this is based on the machine learning models used and the organisms present in the database.\nContamination - the percentage of the assembly estimated to be contaminated with other organisms (indicating our assembly isn’t “pure”).\nGenome_Size - how big the genome is estimated to be, based on other similar genomes present in the database. The O1 El Tor reference N16961 is 4 Mb in total, so these values make sense.\nGC_Content - the percentage of G’s and C’s in the genome, which is relatively constant within a species. The V. cholerae GC content is 47%, so again these values make sense.\nTotal_Coding_Sequences - the total number of coding sequences (genes) that were identified by CheckM2. The reference genome indicates 3,761 annotated genes, so the values obtained could be overestimated.\n\nFrom this analysis, we can assess that our genome assemblies are good quality, with around 90% of the genome assembled. Despite earlier assessing that some of the isolates have more fragmented assemblies (e.g. isolates 5 and 9), they still have &gt;85% completeness according to CheckM2.\nIt is worth noting that the assessment from CheckM2 is an approximation based on other genomes in the database. In diverse species such as Vibrio cholerae the completeness may be underestimated, because each individual strain will only carry part of the pangenome for that species.\nWe are therefore ready to proceed with downstream analysis, where we will further investigate how our samples relate to other V. cholerae strains and whether we find evidence for antibiotic resistance."
  },
  {
    "objectID": "materials/02-assembly/04-assembly_quality.html#accuracy",
    "href": "materials/02-assembly/04-assembly_quality.html#accuracy",
    "title": "9  Assembly quality",
    "section": "9.4 Accuracy",
    "text": "9.4 Accuracy\nAssessing the accuracy of our genome includes addressing issues such as:\n\nRepeat resolution: the ability of the assembly to accurately distinguish and represent repetitive regions within the genome.\nStructural variations: detecting large-scale changes, such as insertions, deletions, or rearrangements in the genome structure.\nSequencing errors: identifying whether errors from the sequencing reads have persisted in the final assembly, including single-nucleotide errors or minor insertions/deletions.\n\nAssessing these aspects of a genome assembly can be challenging, primarily because the true state of the organism’s genome is often unknown, especially in the case of new genome assemblies. However, two considerations can aid in this assessment:\n\nConsistency: ensuring that different assemblies from the same data produce similar results. For more details on this approach, refer to the “Assembling the perfect bacterial genome” tutorial by Wick et al. (2023).\nValidation: confirming the assembly’s accuracy by cross-referencing it with additional data or external resources. Further exploration of this aspect will be covered in the phylogeny section."
  },
  {
    "objectID": "materials/02-assembly/04-assembly_quality.html#exercises",
    "href": "materials/02-assembly/04-assembly_quality.html#exercises",
    "title": "9  Assembly quality",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data. You also need to have completed the genome assembly exercise in Section 8.4.\n\n\n\n\n\n\nAssembly contiguity\n\n\n\n\n\n\n\nAfter completing the genome assembly exercise (Section 8.4):\n\nOpen the file summary_metrics.csv in the results directory of your assemblies. This should open the file in Excel.\nAnswer the following questions:\n\nDid you have enough sequences to achieve 100x coverage in each sample?\nWas the assembly length what you expect for Vibrio cholerae?\nDid your samples have good contiguity (number of fragments)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nFor the “Ambroise 2023” dataset we are using, we had our file in results/assemblies/summary_metrics.csv. We opened this table in Excel and this is what we obtained:\nsample     total_reads  downsampled_reads  assembly_length  fragments  n50      largest  coverage\nCTMA_1402  98596        86402              4116722          2          3057497  3057497  97\nCTMA_1421  113252       112436             4158219          4          3072504  3072504  96\nCTMA_1427  87929        87929              4099953          2          3054801  3054801  64\nCTMA_1432  112216       112216             4101365          2          3056233  3056233  59\nCTMA_1473  163440       131585             4144965          4          2845520  2845520  96\nFrom these results we can see that samples 1427 and 1432 did not have enough reads to achieve 100x. The total number of reads was the same as the downsampled reads, meaning that Rasusa just kept all reads as it didn’t have enough to achieve that coverage. In the last column we can also confirm that the final coverage for those two samples was only ~60x. For the remainder of the samples the coverage was as desired.\nThe assembly lengths obtained were all around 4 Mb, which is expected for this species.\nThe contiguity of our assemblies seemed excellent, with only up to 4 fragments. Some samples even had 2 fragments assembled, suggesting we achieved chromosome-level assemblies (recall that V. cholerae has 2 chromosomes).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssembly graph\n\n\n\n\n\n\n\nBased on the contiguity metrics you evaluated in the previous exercise, investigate the assembly graphs for the samples with highest and lowest number of fragments.\n\nOpen the Bandage software.\nGo to File &gt; Load graph.\nLocate the Flye output folder (using our script, this is located in results/assemblies/02-flye/SAMPLE-ID/assembly_graph.gfa).\nClick Draw graph.\n\nBased on the graphs you get, answer the following questions:\n\nCan you identify two very large fragments, which possibly include most of your two chromosomes?\nDo you have many “loops” in the assembly graph indicating a highly-unresolved assembly?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nBased on our previous analysis of the “Ambroise 2023” data, most of our assemblies were highly contiguous, with only 2 to 4 fragments.\nFor one of the best assemblies (CTMA_1402), the assembly graph looked as follows:\n\nWe can clearly see two circular fragments, which corresponds very well to our expectation of 2 chromosomes for Vibrio cholerae.\nHere is the graph for one of the assemblies with 4 fragments (CTMA_1421):\n\nIn this case we can see that:\n\nWe had two larger fragments with size ~3Mb and ~1Mb, which likely correspond to Chr1 and Chr2, respectively.\nOne small fragment that was not connected to the larger fragments. This could be due to missing reads that bridge the connection between this fragment and the rest of the assembly.\nOne small fragment which might have been connected to either of the larger circular fragments. This could be due to a duplicated sequence, which occurs in both chromosomes. Possibly having longer reads would have helped resolve this ambiguity better.\n\nOverall though, these assemblies look extremely good, with two large fragments likely corresponding to most of the two chromosomes of Vibrio cholerae.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssembly completeness\n\n\n\n\n\n\n\nTo assess the completeness of your assembly, run the CheckM2 software on your assembly files.\n\nOpen the script provided in scripts/03-checkm.sh.\nFix the code where indicated (see Section 9.3.2 if you need some help).\nActivate the software environment: mamba activate checkm2.\nRun the script using bash scripts/03-checkm.sh\nOnce the analysis completes, go to the output folder and open the quality_report.tsv file in Excel. Answer the following questions:\n\nDid you manage to achieve &gt;90% completeness for your genomes?\nWas there evidence of substantial contamination in your assemblies?\nWas the GC content what you expected for this species?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe fixed code for our script is:\n# make output directory\nmkdir results/checkm2\n\n# run checkm\ncheckm2 predict \\\n  --input results/assemblies/*.fasta \\\n  --output-directory results/checkm2 \\\n  --database_path resources/CheckM2_database/uniref100.KO.1.dmnd \\\n  --lowmem --threads 12\nWe then ran the script using bash scripts/03-checkm.sh, which printed several messages to the screen while it was running.\nAfter it completed, we opened the file results/checkm2/quality_report.tsv in Excel and this is what we obtained:\nName       Completeness  Contamination  Completeness_Model_Used          Translation_Table_Used  Coding_Density  Contig_N50  Average_Gene_Length  Genome_Size  GC_Content  Total_Coding_Sequences  Additional_Notes\nCTMA_1402  89.38         5.08           Neural Network (Specific Model)  11                      0.855           3059251     266.3945823927765    4119063      0.47        4430                    None\nCTMA_1421  91.73         1.4            Neural Network (Specific Model)  11                      0.851           3074388     265.5238308346386    4160736      0.47        4469                    None\nCTMA_1427  88.43         5.34           Neural Network (Specific Model)  11                      0.855           3056317     260.31857427496124   4101983      0.47        4517                    None\nCTMA_1432  90.67         4.41           Neural Network (Specific Model)  11                      0.854           3057687     255.32609167933956   4103331      0.47        4603                    None\nCTMA_1473  90.89         2.58           Neural Network (Specific Model)  11                      0.854           2847175     269.16424682395643   4147467      0.47        4408                    None\nWe can see from this that:\n\nWe achieved ~90% completeness (according to CheckM2’s database) for all our samples.\nThere was no evidence of strong contamination affecting our assemblies (all ~5% or below).\nThe GC content was 47%, which is what is expected for Vibrio cholerae."
  },
  {
    "objectID": "materials/02-assembly/04-assembly_quality.html#summary",
    "href": "materials/02-assembly/04-assembly_quality.html#summary",
    "title": "9  Assembly quality",
    "section": "9.6 Summary",
    "text": "9.6 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nKey aspects to evaluate an assembly quality include:\n\nContiguity: how continuous the final assembly is (the best assembly would be chromosome-level).\nCompleteness: whether the entire genome of the species was captured.\n\nCommon indicators for evaluating the contiguity of a genome assembly include metrics like N50, fragment count and total assembly size.\nExamination of assembly graphs reveals the contiguity of an assembly by illustrating how fragments connect, helping identify regions of potential fragmentation or misassembly.\nSpecialised software tools, like CheckM2, enable the assessment of genome completeness and contamination by comparing the assembly to known reference genomes and identifying missing or unexpected genes and sequences."
  },
  {
    "objectID": "materials/03-typing/01-pathogenwatch.html#pathogenwatch",
    "href": "materials/03-typing/01-pathogenwatch.html#pathogenwatch",
    "title": "10  Pathogenwatch",
    "section": "10.1 Pathogenwatch",
    "text": "10.1 Pathogenwatch\nPathogenwatch is a web-based platform for common genomic surveillance analysis tasks, including:\n\nIdentifying strains for pathogens of concern.\nCluster sequences using phylogenetic analysis.\nIdentifying the presence of antibiotic resistance genes.\n\nPathogenwatch is designed to be user-friendly, supporting the analysis of over 100 species, including Vibrio cholerae, which is our organism of focus. In this chapter, we will cover the basics of loading genomes and creating collections for analysis on this platform. The details of the Pathogenwatch analysis will then be covered in following chapters.\nIn order to use this platform you will first need to create an account (or sign-in through your existing Google, Facebook or Twitter)."
  },
  {
    "objectID": "materials/03-typing/01-pathogenwatch.html#uploading-fasta-files",
    "href": "materials/03-typing/01-pathogenwatch.html#uploading-fasta-files",
    "title": "10  Pathogenwatch",
    "section": "10.2 Uploading FASTA files",
    "text": "10.2 Uploading FASTA files\nOnce you have logged in to Pathogenwatch, you can load the FASTA files with the sequences you want to analyse. In our case, we will load the assemblies we produced with our genome assembly script.\n\nClick the Upload link in the top-right corner of the page:\n\n\n\nClick in the Upload FASTA(s) button, on the “Single Genome FASTAs” section:\n\n\n\nIf your internet connection is slow and/or unstable, you can tick “Compress files” and “Upload files individually”.Click the + button on the bottom-right corner to upload the sequences:\n\n\n\nThis will open a file browser, where you can select the FASTA files from your local machine. Go to the results/assemblies folder where you have the results from your earlier genome assembly analysis. You can upload several files at once by clicking and selecting several FASTA files while holding the Ctrl key. Click open on the dialogue window after you have selected all of your FASTA files.\n\n\n\nA new page will open showing the progress of the samples being uploaded and processed.\n\n\n\nClick in the VIEW GENOMES button, which will take you to a tabular view of your samples:\n\n\nPathogenwatch performs the following major analyses useful for genomic surveillance: sequence typing (ST), antimicrobial resistance (AMR) analysis, phylogenetics, as well as reporting general statistics about your samples (such as genome completeness, which we also assessed with checkM2). We will detail several of these analyses in the coming chapters, but here is a brief description of each column:\n\nName - the names of the uploaded samples.\nOrganism - the species that was detected for our samples, in this case Vibrio cholerae.\nType and Typing schema - the sequence type assigned to each sample, based on MLST analysis (detailed in the next chapter)\nCountry and Date - the country and date of collection, respectively; only shown if we provided that information as metadata.\nAccess - whether these samples are private or public; in this case, because they were uploaded by us, they are private (only we can see them).\n\n\n\n\n\n\n\nMetadata\n\n\n\nIf you have metadata files associated with your sequenced samples, you can upload those files following these instructions. Make sure all metadata files are in CSV format, with five recommended columns named ‘latitude’, ‘longitude’, ‘year’, ‘month’, and ‘day’. You can also use the template provided by Pathogenwatch on the upload page, to help you prepare your metadata files before the analysis.\nHaving this type of information is highly recommended, as it will allow you to visualise your samples on a map, which is useful if you want to match particular strains to the geographic locations where outbreaks occur."
  },
  {
    "objectID": "materials/03-typing/01-pathogenwatch.html#collections",
    "href": "materials/03-typing/01-pathogenwatch.html#collections",
    "title": "10  Pathogenwatch",
    "section": "10.3 Collections",
    "text": "10.3 Collections\nA useful feature of Pathogenwatch is to group our samples into collections. This allows us to manage and analyse samples in batches of our choice. The same sample can exist in different collections. For example you might create a collection with only the genomes you sequenced recently, another collection with all the genomes you ever sequenced in your facility, or even a collection that includes your samples together with public samples available online (if you want to compare them with each other).\nTo create a collection from your sequences, check the box next to the “Name” header to select all of the uploaded genomes. Then, from the top-rigth of the table, click Selected Genomes –&gt; Create Collection:\n\nIn the next window give a name and description to your collection:\n\nIt is highly recommended to provide details for your collection:\n\nTitle - give your collection a title that is meaningful to you, for example: “Cholera workshop 2023”.\nDescription - give a brief description of your samples, for example: “Culture-based sequencing of Vibrio samples using a Nanopore platform. Samples were collected from patients in the outbreak in  on . De novo assembly was performed using Flye followed by Medaka polishing.”\nIf your data come from a published study, provide a DOI of the study.\n\nFinally, click Create Now button to create your collection. You will be shown a table and map, with the samples you just added to the collection:\n\nThis table contains several columns:\n\nPurple  download button - download the assembled genome in FASTA format. This is the same file that you just uploaded, so it’s not very useful in our case. It can be useful if you want to download the public sequences available from within Pathogenwatch.\nGreen  download button - download the gene annotation performed by Pathogenwatch in GFF format. Note that our assembly script already performed gene annotation using Bakta, so this feature is also not so useful for us. But again, if you were using public sequences from Pathogenwatch, you could download their GFF files.\nNAME - your sample name.\nREFERENCE - the reference lineage that your sequence was closest to. Vibriowatch defines a set of 17 ‘reference genomes’, 14 of which belong to current pandemic lineages (7PET). Their reference genomes are named ‘Wi_Tj’ where ‘W’ stands for a Wave and ‘T’ stands for a Transmission event. Our samples are closest to W3_T13, which is a recent lineage. This makes sense, as our samples are from isolates collected in 2023.\nINC TYPES - identification of plasmids relevant for enterobacterial strains (“inc” stands for “incompatibility”, refering to plasmid incompatibility groups). Inc plasmids often carry antibiotic resistance and virulence genes, making them of particular relevance for public health (e.g. Foley et al 2021).\nST and PROFILE - these columns refer to the “sequence type” (ST) assigned to each of our samples. We will detail this analysis in the next chapter.\nBIOTYPE and SEROGROUP - refer to the biotype and serogroup our samples likely belong to, based on the genes present in their genomes (we detailed how V. cholerae strains are classified in an earlier chapter)\n\nWe will further analyse these results in the following chapters."
  },
  {
    "objectID": "materials/03-typing/01-pathogenwatch.html#sec-ex-pathogenwatch",
    "href": "materials/03-typing/01-pathogenwatch.html#sec-ex-pathogenwatch",
    "title": "10  Pathogenwatch",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data. You also need to have completed the genome assembly exercise in Section 8.4.\n\n\n\n\n\n\nLoading data into Pathogenwatch\n\n\n\n\n\n\n\n\nLoad your newly assembled sequences to Pathogenwatch\nCreate a new collection from your sequences with a name of your choice.\nAre they identified as a known biotype? Do you think these are pathogenic strains?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nFor the “Ambroise 2023” data, we have 5 genomes saved in results/assemblies/, from the assembly pipeline we ran in sec-ex-assembly. We loaded all the FASTA files from that folder into Pathogenwatch:\n\nClicking on “View Genomes” takes us to the table of genomes, where we can select the genomes to create a new collection:\n\nWe name our collection accordingly, and in this case we even added the associated publication DOI:\n\nAfter clicking on “Create Now” we are shown the default analysis table:\n\nWe will analyse some of these results in a later section, but we can see from the last two columns that our samples were assigned biotype “O1 pathogenic”, confirming that these are pathogenic strains circulating in the population."
  },
  {
    "objectID": "materials/03-typing/01-pathogenwatch.html#summary",
    "href": "materials/03-typing/01-pathogenwatch.html#summary",
    "title": "10  Pathogenwatch",
    "section": "10.5 Summary",
    "text": "10.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nPathogenwatch is a web-based platform designed for genomic surveillance of bacterial pathogens. It assists in the analysis and interpretation of genomic data to monitor disease outbreaks and track pathogen evolution.\nYou can upload genome assemblies in FASTA format and accompanying metadata as CSV files.\nAssemblies can be organized into collections, making it simpler to manage and analyze multiple samples together.\nPathogenwatch’s interface offers an intuitive user experience designed for users with varying levels of expertise, providing results such as biotype/serogroup, strain classification, antimicrobial resistance (AMR) and phylogenetic placement."
  },
  {
    "objectID": "materials/03-typing/02-mlst.html#multilocus-sequence-typing-mlst",
    "href": "materials/03-typing/02-mlst.html#multilocus-sequence-typing-mlst",
    "title": "11  MLST",
    "section": "11.1 Multilocus Sequence Typing (MLST)",
    "text": "11.1 Multilocus Sequence Typing (MLST)\nMLST has become a common method for identifying and characterising pathogenic bacterial strains. It uses a specific set of “housekeeping genes” linked to the bacteria of interest. These genes help categorize isolates into groups based on the changes they carry within these genes. MLST aids in identifying changes in the genetic sequences of isolates during new outbreaks, which adds to other methods like serotyping for classifying strains. This is why public health labs often rely on MLST to assist officials in comprehending and controlling disease outbreaks.\n\n\n\nHaplotype tree constructed from Vibrio cholerae gene sequences used for typing, with haplotype groups color-coded by their respective countries of origin, illustrating distinct sequence types prevalent in various countries. Image source: Fig. 5 in Liang et al. 2020\n\n\nGroups of isolates that share similar mutation patterns in these housekeeping genes are called sequence types. The PubMLST project curates and maintains these sequence types. For example, sequence type 69 (ST69) is a common type linked to O1 El Tor strains in the current pandemic (7PET). Although this method might seem a bit old-fashioned in the age of genomic analysis (as it focuses on only 7 genes), it offers a uniform and comparable way to categorize strains across different labs and locations.\nThis sequence typing approach helps determine whether any of our tested isolates are harmful. V. cholerae typing is done from seven housekeeping genes (adk, gyrB, metE, mdh, pntA, purM, and pyrC). Using this approach, we aim to determine which pandemic strain types dominate our isolates or whether we’ve stumbled upon new strain types. This analysis can be accomplished through Pathogenwatch or by using a separate command line tool called mlst."
  },
  {
    "objectID": "materials/03-typing/02-mlst.html#mlst-with-pathogenwatch",
    "href": "materials/03-typing/02-mlst.html#mlst-with-pathogenwatch",
    "title": "11  MLST",
    "section": "11.2 MLST with Pathogenwatch",
    "text": "11.2 MLST with Pathogenwatch\nPathogenwatch uses PubMLST to run its typing analysis (details here) and the results can be seen on the collection view page (Figure 11.1).\n\n\n\nFigure 11.1: Example table from the collection view from Pathogenwatch (top), with example of an individual report (bottom)\n\n\nAll of the sequence types determined by MLST in our samples seem to be novel (this is indicated by an * before the name). This seems very surprising, as there must be other strains similar to ours identified in recent outbreaks. There are several reasons why we may have obtained this result:\n\nThe PubMLST database may not contain up-to-date sequence types for most recent Vibrio lineages circulating worldwide.\nEven if only one of the 7 genes used for typing contains a mutation, MLST considers it to be a different type from the one in the database.\nBecause we are using Nanopore data, which has relatively high error rates, we may have some errors in our assemblies, which now affects this analysis.\n\nWe start our analysis by activating our software environment, to make all the necessary tools available:\nmamba activate typing"
  },
  {
    "objectID": "materials/03-typing/02-mlst.html#sec-mlst-cli",
    "href": "materials/03-typing/02-mlst.html#sec-mlst-cli",
    "title": "11  MLST",
    "section": "11.3 MLST with command line",
    "text": "11.3 MLST with command line\nRunning MLST from the command line can be done with a single command:\n# create output directory\nmkdir results/mlst\n\n# run mlst\nmlst --scheme vcholerae results/assemblies/*.fasta &gt; results/mlst/mlst_typing.tsv\nThis command outputs a tab-delimited file (TSV), which we can open in a spreadsheet software such as Excel. Here is the result for our samples:\nresults/assemblies/isolate01.fasta  vcholerae  -  adk(7)  gyrB(11)    mdh(~133)  metE(37)   pntA(12?)   purM(1)     pyrC(20?)\nresults/assemblies/isolate02.fasta  vcholerae  -  adk(7)  gyrB(~11)   mdh(4)     metE(37)   pntA(227?)  purM(1)     pyrC(20?)\nresults/assemblies/isolate03.fasta  vcholerae  -  adk(7)  gyrB(11)    mdh(~121)  metE(37)   pntA(227?)  purM(1)     pyrC(20?)\nresults/assemblies/isolate04.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(~4)    metE(37)   pntA(227?)  purM(1)     pyrC(20?)\nresults/assemblies/isolate05.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(~121)  metE(37)   pntA(227?)  purM(1)     pyrC(~20)\nresults/assemblies/isolate06.fasta  vcholerae  -  adk(7)  gyrB(195?)  mdh(~121)  metE(37)   pntA(227?)  purM(1)     pyrC(20?)\nresults/assemblies/isolate07.fasta  vcholerae  -  adk(7)  gyrB(195?)  mdh(~121)  metE(37)   pntA(-)     purM(1)     pyrC(20?)\nresults/assemblies/isolate08.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(209?)  metE(37)   pntA(227?)  purM(172?)  pyrC(20?)\nresults/assemblies/isolate09.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(~121)  metE(~37)  pntA(12?)   purM(176?)  pyrC(20?)\nresults/assemblies/isolate10.fasta  vcholerae  -  adk(7)  gyrB(195?)  mdh(~121)  metE(37)   pntA(227?)  purM(1)     pyrC(20?)\nWe get a column for each of the 7 genes used for Vibrio sequence typing, with the gene name followed by the allele number in parenthesis. The allele number is an identifier used by PubMLST, and it means that allele has a specific sequence with a certain set of variants (search for alleles here). For example, adk(7) corresponds to allele 7 of the adk gene.\nThe command line version of mlst also reports when an allele has an inexact match to the allele in the database, with the following notation (copied from the README documentation):\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nLength\nIdentity\n\n\n\n\nn\nExact intact allele\n100%\n100%\n\n\n~n\nNovel full length allele similar to n\n100%\n≥ --minid\n\n\nn?\nPartial match to known allele\n≥ --mincov\n≥ --minid\n\n\n-\nAllele missing\n&lt; --mincov\n&lt; --minid\n\n\nn,m\nMultiple alleles\n\n\n\n\n\nThe third column of the output (which in our case is - for all samples) would indicate if our samples matched a known sequence type from the PubMLST database. It seems like all our samples are “new” sequence types, i.e. their profile doesn’t exactly match any of the curated types. We can further interpret our results by searching for their closest-matching profiles on the PubMLST website. If we search on that page for the profiles of each isolate, we will find that they always come closest to the sequence type ST69, which includes the pathogenic O1 El Tor strain. For example, “isolate02” matches ST69 for six out of the seven genes.\nWhile somewhat inconclusive about an exact sequence type, this analysis further reinforces that we are faced with pathogenic strains of Vibrio cholerae. In the next chapter we will turn to phylogenetic analysis to further investigate the relationship between these strains and other previously sequenced strains of this species."
  },
  {
    "objectID": "materials/03-typing/02-mlst.html#exercises",
    "href": "materials/03-typing/02-mlst.html#exercises",
    "title": "11  MLST",
    "section": "11.4 Exercises",
    "text": "11.4 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data. You also need to have completed the genome assembly exercise in Section 8.4.\n\n\n\n\n\n\nMLST (command line)\n\n\n\n\n\n\n\nPerform MLST analysis from the command line:\n\nOpen the script provided in scripts/04-mlst.sh.\nFix the code where indicated (see Section 11.3 if you need some help).\nActivate the software environment: mamba activate typing.\nRun the script using bash scripts/04-mlst.sh\n\nAfter the analysis runs, answer the following questions:\n\nDid any of your assemblies get assigned to known types?\nGo to the PubMLST allelic profile search and search for the closest profile to your samples.\nDo you think all your samples are likely from the same outbreak event or transmission chain?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nHere is the fixed shell script:\n#!/bin/bash\n\n# create output directory\nmkdir results/mlst\n\n# run mlst\nmlst --scheme vcholerae results/assemblies/*.fasta &gt; results/mlst/mlst_typing.tsv\n\nAs input we specified all our FASTA files, using the * wildcard to match them all automatically.\n\nWe then opened the output file in Excel, which contained the following:\nresults/assemblies/CTMA_1402.fasta  vcholerae  -  adk(7)  gyrB(~11)   mdh(121?)  metE(206)  pntA(12)  purM(1)     pyrC(20)\nresults/assemblies/CTMA_1421.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(~45)   metE(206)  pntA(12)  purM(176?)  pyrC(~20)\nresults/assemblies/CTMA_1427.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(223?)  metE(37)   pntA(12)  purM(1)     pyrC(20)\nresults/assemblies/CTMA_1432.fasta  vcholerae  -  adk(7)  gyrB(~11)   mdh(223?)  metE(37)   pntA(12)  purM(1)     pyrC(20)\nresults/assemblies/CTMA_1473.fasta  vcholerae  -  adk(7)  gyrB(120?)  mdh(209?)  metE(206)  pntA(12)  purM(176?)  pyrC(20)\nNone of our samples were assigned to standard sequence types (third column, with -).\nWhen we search for these profiles on PubMLST, some of them seem to have closest resemblance to ST515. Others have some resemblance to ST69. Both of these sequence types are associated with O1 El Tor pathogenic strains, confirming we are dealing with pathogenic strains.\nThe fact that our samples seem to have different profiles may suggest they represent different outbreak events or sources of transmission. We would need to investigate this more thoroughly by considering any metadata associated with these samples (such as date and place of origin).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLST (Pathogenwatch)\n\n\n\n\n\n\n\nFollowing from the Pathogenwatch exercise in Section 10.4, open the collection that you created for your samples and answer the following questions:\n\nWhich sequence type were your sequences assigned to?\nHow do the results compare with output from the command line version of the software? (You need to complete the previous exercise to answer this question)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nFirst, we make sure to be on the collection view screen for our “AWD Workshop - Ambroise 2023” collection created earlier (Section 10.4). In case you don’t have it open, you can get it from the account menu (top-left button, as shown on image below) and click on “My Collections”. Your collection should be listed and you can click on “View Colection”.\n\nOnce inside the collection view, the MLST table is usually the one shown by default, or you can choose it from the drop-down menu on the top-left of the table:\n\nFrom this table we can see that:\n\nAll of our samples were assigned a new type. This is indicated by the * in the ST column.\nThe results are very similar to the command line version of the tool, except we don’t get information about similar alleles that were matched to the new gene alleles found."
  },
  {
    "objectID": "materials/03-typing/02-mlst.html#summary",
    "href": "materials/03-typing/02-mlst.html#summary",
    "title": "11  MLST",
    "section": "11.5 Summary",
    "text": "11.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nMLST (Multilocus Sequence Typing) is a genotyping method used to identify and categorize bacterial strains based on the sequences of specific housekeeping genes.\nIt aids in tracking and monitoring the spread of bacterial pathogens, understanding their genetic diversity, and detecting outbreaks.\nMLST can be performed using web-based tools like Pathogenwatch and PubMLST, which offer user-friendly interfaces for sequence type assignment.\nDedicated command-line software such as mlst allows for automation and give a more detailed output.\nMLST results reveal the sequence types (STs) of bacterial strains, which can help in identifying clonal groups and their relatedness.\nThe validity of MLST results depends on the quality of input data, as errors in the assembly may result in wrongly classifying our assemblies as a new ST."
  },
  {
    "objectID": "materials/03-typing/03-phylogeny.html#pathogen-phylogenetics",
    "href": "materials/03-typing/03-phylogeny.html#pathogen-phylogenetics",
    "title": "12  Building phylogenetic trees",
    "section": "12.1 Pathogen Phylogenetics",
    "text": "12.1 Pathogen Phylogenetics\nPhylogenetic analysis aims to determine evolutionary relationships among organisms. In the context of pathogenic organisms, it is used to study the origin of human-infecting strains. This involves analyzing sequences from various bacterial/viral species infecting humans and other species. For instance, during the COVID-19 pandemic, inter-species phylogeny was used to trace the origin of the SARS-CoV-2 virus that adapted to infect humans.\nPathogen surveillance mostly focuses on intra-species phylogenies. Here, phylogenies are constructed from sequences within the same pathogenic species. The objective is to understand how specific strains and lineages relate and evolve. By analyzing genetic differences, we can track pathogen spread and identify outbreak sources. This section aims to answer: How do our Vibrio cholerae isolates relate to each other and to previously sequenced strains? This sheds light on relationships among isolates and their evolutionary context.\nTo construct a phylogeny, two primary steps are necessary:\n\nMultiple sequence alignment: to account for variations in sequence lengths due to insertions/deletions, aligning homologous residues of each sequence is the first step.\nTree inference: with the aligned sequences, statistical models of sequence evolution are used to infer the most probable relationship between these sequences, based on observed substitutions.\n\nMultiple sequence alignment is straightforward when dealing with a single gene or closely related species (e.g. clonal bacterial species). However, bacterial species can exhibit substantial differences due to factors like horizontal gene transfer (e.g. through conjugation or bacteriophages), gene duplication, and gene loss. In such cases, alignment focuses on the core genome, the gene set present in most species members, enabling inference of evolutionary relationships. This contrasts with the “accessory genome,” consisting of genes present only in some members of the species. The complete collection of core and accessory genomes in a species is referred to as the pangenome.\nGiven the diversity of Vibrio cholerae, which often acquires resistance genes through horizontal transfer, the phylogenetic process begins with generating a core genome alignment, forming the foundation for tree inference. We will explore phylogenetic analysis of Vibrio cholerae using Pathogenwatch, followed by a guide to constructing your own phylogenies using command line tools."
  },
  {
    "objectID": "materials/03-typing/03-phylogeny.html#sec-phylo-pathogenwatch",
    "href": "materials/03-typing/03-phylogeny.html#sec-phylo-pathogenwatch",
    "title": "12  Building phylogenetic trees",
    "section": "12.2 Phylogenies with Pathogenwatch",
    "text": "12.2 Phylogenies with Pathogenwatch\nIn the “collection view” screen for your samples, Pathogenwatch shows you a phylogenetic tree on the top-left panel, showing the relationship between the sequences in your collection. This shows how tightly our samples cluster with each other, and outliers may indicate assembly issues. For example, we can see from Figure 12.1 that “isolate05” clusters separately from the rest of the samples. From our previous analysis of assembly quality we saw that this sample had poorer sequencing coverage, which may have led to a higher error rate (because fewer sequencing reads were available to identify a consensus sequence).\n\n\n\nFigure 12.1: Phylogenetic view of the samples in our collection. You can view the sample names by clicking on the buttons shown.\n\n\nWe can also show how your samples relate to the “reference genomes” available in Pathogenwatch (Figure 12.2). These are genomes curated by the Vibriowatch project, which include several genomes that have been assigned to different phylogenetic lineages, and associated with transmission waves/events.\n\n\n\nFigure 12.2: Phylogenetic view of the “Population” samples used in the default phylogenetic analysis from Pathogenwatch. You can choose this view from the top-left dropdown menu as illustrated. The clades on which your samples were clustered with appear in purple.\n\n\nIn Figure 12.2 we can see that all our 10 samples fall in the “W3_T13” clade (wave 3, transmission event 13). This reference clade corresponds to strains from the most recent transmission wave in Africa determined by Weill et al. 2017. This agrees with other pieces of evidence we aquired so far, that our strains are closely related to the most recent pathogenic strains circulating worldwide.\nYou can further “zoom in” on the tree to see how your samples relate to the reference samples present in this clade (Figure 12.3). This shows that although our samples are related to “W3 T3” reference samples, they have accumulated enough mutations that makes them cluster apart from the reference panel. Note that some of the lineage classification used by Pathogenwatch may be slightly outdated, as new strains emerge that were not part of the 2017 study mentioned above.\n\n\n\nFigure 12.3: Clicking on the “W3_T13” clade shows our samples in the context of the other reference genomes in this clade.\n\n\n\n\n\n\n\n\nSequence divergence or sequencing error?\n\n\n\nPhylogenetic analysis is also useful to assess our assembly quality with regards to validity/correctness of our sequences. If our sequences are very diverged from other sequences (i.e. several mutations separate them from other sequences), this may indicate a high error rate in our assemblies. However, this is not always easy to assess, as the public collection such as the one from Pathogenwatch (Figure 12.3) may have older sequences, so this may represent true divergence since the time those samples were collected.\nTo fully address if our samples likely have sequencing errors, we can do any of the following:\n\nCompare your samples with similar (or even the same) samples assembled with higher accuracy data. For example, following the advice given on the “perfect bacterial genome” pipeline (using a “hybrid assembly” approach with both Illumina and ONT data).\nUsing more recent ONT chemistry, flowcells and basecalling software, which provide higher accuracy sequences.\nCompare our assemblies with recent samples from the same region/outbreak."
  },
  {
    "objectID": "materials/03-typing/03-phylogeny.html#phylogenies-with-local-software",
    "href": "materials/03-typing/03-phylogeny.html#phylogenies-with-local-software",
    "title": "12  Building phylogenetic trees",
    "section": "12.3 Phylogenies with local software",
    "text": "12.3 Phylogenies with local software\nIn the previous section we used Pathogenwatch to perform a phylogenetic analysis, using its inbuilt collection. While Pathogenwatch is user-friendly, it relies on a web-based service that might not always be accessible. Therefore, we introduce an alternative using command line tools suitable for local execution on your computer. The methodology we use here is similar to Pathogenwatch’s, but it employs distinct tools for generating the core genome and performing tree inference. Our toolkit consists of four software components:\n\nPanaroo - used to identify a set of “core genes” (genes occurring in most samples) and generate a multiple sequence alignment from them.\nSNP-sites - used to extract variable sites from the alignment (to save computational time).\nIQ-TREE - used to infer a tree from the aligned core genes.\nFigtree - used to visualise and/or annotate our tree.\n\nWe start our analysis by activating our software environment, to make all the necessary tools available:\nmamba activate phylogeny\n\n12.3.1 Core genome alignment: panaroo\nThe software Panaroo was developed to analyse bacterial pangenomes. It is able to identify orthologous sequences between a set of sequences, which it uses to produce a multiple sequence alignment of the core genome. The output alignment it produces can then be used to build our phylogenetic trees in the next step.\nAs input to Panaroo we will use:\n\nThe gene annotations for our newly assembled genomes, which were produced during the assembly pipeline using Bakta.\nAnnotations from 31 public genomes downloaded from NCBI (see Section 6.5). These annotations had to be processed to be compatible with Panaroo, which we detail in the information box below.\n\nTo run Panaroo on our samples we can use the following commands:\n# create output directory\nmkdir results/panaroo\n\n# run panaroo\npanaroo \\\n  --input results/assemblies/*.gff resources/vibrio_genomes/*.gff \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\nThe options used are:\n\n--input - all the input annotation files, in the Panaroo-compatible GFF format. Notice how we used the * wildcard to match all the files in each folder: the results/assemblies folder contains the annotations for our own genomes; the resources/vibrio_genomes/ folder contains the public annotations (suitably converted to Panaroo-compatible format - see information box below).\n--out_dir - the output directory we want to save the results into.\n--clean-mode - determines the stringency of Panaroo in including genes within its pangenome graph for gene clustering and core gene identification. The available modes are ‘strict’, ‘moderate’, and ‘sensitive’. These modes balance eliminating probable contaminants against preserving valid annotations like infrequent plasmids. In our case we used ‘strict’ mode, as we are interested in building a core gene alignment for phylogenetics, so including rare plasmids is less important for our downstream task.\n--alignment - whether we want to produce an alignment of core genes or all genes (pangenome alignment). In our case we want to only consider the core genes, to build a phylogeny.\n--core_threshold - the fraction of input genomes where a gene has to be found to be considered a “core gene”. In our case we’ve set this to a very high value, to ensure most of our samples have the gene.\n--remove-invalid-genes - this is recommended to remove annotations that are incompatible with the annotation format expected by Panaroo.\n--threads - how many CPUs we want to use for parallel computations.\n\nPanaroo takes a long time to run, so be prepared to wait a while for its analysis to finish .\nOnce if finishes, we can see the output it produces:\nls results/panaroo\naligned_gene_sequences/                core_alignment_header.embl        gene_presence_absence_roary.csv\nalignment_entropy.csv                 core_gene_alignment.aln           pan_genome_reference.fa\ncombined_DNA_CDS.fasta                core_gene_alignment_filtered.aln  pre_filt_graph.gml\ncombined_protein_CDS.fasta            final_graph.gml                   struct_presence_absence.Rtab\ncombined_protein_cdhit_out.txt        gene_data.csv                     summary_statistics.txt\ncombined_protein_cdhit_out.txt.clstr  gene_presence_absence.Rtab\ncore_alignment_filtered_header.embl   gene_presence_absence.csv\nThere are several output files generated, which can be generated for more advanced analysis and visualisation (see Panaroo documentation for details). For our purpose of creating a phylogeny from the core genome alignment, we need the file core_gene_alignment_filtered.aln, which is a file in FASTA format. We can take a quick look at this file:\nhead results/panaroo/core_gene_alignment_filtered.aln\n&gt;GCF_015482825.1_ASM1548282v1_genomic\natggctatttatctgactgaattatcgccggaaacgttgacattcccctctccttttact\ngcgttagatgaccctaacggcctgcttgcatttggcggcgatctccgtcttgaacgaatt\ntgggcggcttatcaacaaggcattttcccttggtatggccctgaagacccgattttgtgg\ntggagcccttccccacgtgccgtgtttgaccctactcggtttcaacctgcc-aaaagcgt\ngaagaagttccaacgtaaacatcagtatcgggttagcgtcaatcacgcgacgtcgcaagt\ngattgagcagtgcgcgctcactcgccctgcggatcaacgttggctcaatgactcaatgcg\nccatgcgtatggcgagttggcgaaacaaggtcgttgccattctgttgaggtgtggcaggg\ncgaacaactggtgggtgggctttatggcatttccgttggccaactgttttgtggcgaatc\ncatgtttagcctcgcaaccaatgcctcgaaaattgcgctttggta-tttttgcgaccatt\nWe can see this contains a sequence named “GCF_015482825.1_ASM1548282v1_genomic”, which corresponds to one of the NCBI genomes we downloaded. We can look at all the sequence names in the FASTA file:\ngrep \"&gt;\" results/panaroo/core_gene_alignment_filtered.aln\n&gt;GCF_015482825.1_ASM1548282v1_genomic\n&gt;GCF_019704235.1_ASM1970423v1_genomic\n&gt;GCF_013357625.1_ASM1335762v1_genomic\n&gt;GCF_017948285.1_ASM1794828v1_genomic\n&gt;GCF_009763825.1_ASM976382v1_genomic\n&gt;isolate01\n&gt;GCF_013357665.1_ASM1335766v1_genomic\n&gt;GCF_009762915.1_ASM976291v1_genomic\n&gt;GCF_009762985.1_ASM976298v1_genomic\n\n... more output omitted to save space ...\nWe can see each input genome appears once, including the “isolateXX” genomes assembled and annotated by us.\n\n\n\n\n\n\nPreparing GFF files for Panaroo (click to see details)\n\n\n\n\n\nPanaroo requires GFF files in a non-standard format. They are similar to standard GFF files, but they also include the genome sequence itself at the end of the file. By default, Bakta (which we used earlier to annotate our assembled genomes) already produces files in this non-standard GFF format.\nHowever, GFF files downloaded from NCBI will not be in this non-standard format. To convert the files to the required format, the Panaroo developers provide us with a Python script that can do this conversion:\npython3 convert_refseq_to_prokka_gff.py -g annotation.gff -f genome.fna -o new.gff\n\n-g is the original GFF (for example downloaded from NCBI).\n-f is the corresponding FASTA file with the genome (also downloaded from NCBI).\n-o is the name for the output file.\n\nThis is a bit more advanced, and is included here for interested users. We already prepared all the files for performing a phylogeny, so you don’t need to worry about this for the workshop.\n\n\n\n\n\n12.3.2 Extracting variable sites: snp-sites\nAlthough you could use the alignment generated by Panaroo directly as input to IQ-TREE, this would be quite computationally heavy, because the core genome alignments tend to be quite big. Instead, what we can do is extract the variable sites from the alignment, such that we reduce our FASTA file to only include those positions that are variable across samples.\nHere is a small example illustrating what we are doing. For example, take the following three sequences, where we see 3 variable sites (indicated with an arrow):\nseq1  C G T A G C T G G T\nseq2  C T T A G C A G G T\nseq3  C T T A G C A G A T\n        ↑         ↑   ↑\nFor the purposes of phylogenetic tree construction, we only use the variable sites to look at the relationship between our sequences, so we can simplify our alignment by extract only the variable sites:\nseq1  G T G\nseq2  T A G\nseq3  T A A\nThis example is very small, but when you have a 4Mb genome, this can make a big difference. To extract variable sites from an alignment we can use the SNP-sites software:\n# create output directory\nmkdir results/snp-sites\n\n# run SNP-sites\nsnp-sites results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/core_gene_alignment_snps.aln\nThis command simply takes as input the alignment FASTA file and produces a new file with only the variable sites - which we redirect (&gt;) to an output file. This is the file we will use as input to constructing our tree.\nHowever, before we move on to that step, we need another piece of information: the number of constant sites in the initial alignment (sites that didn’t change). Phylogenetically, it makes a difference if we have 3 mutations in 10 sites (30% variable sites, as in our small example above) or 3 mutations in 1000 sites (0.3% mutations). The IQ-TREE software we will use for tree inference can accept as input 4 numbers, counting the number of A, C, G and T that were constant in the alignment. For our small example these would be, respectively: 1, 2, 2, 2.\nFortunately, the snp-sites command can also produce these numbers for us (you can check this in the help page by running snp-sites -h). This is how you would do this:\n# count invariant sites\nsnp-sites -C results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/constant_sites.txt\nThe key difference is that we use the -C option, which produces these numbers. Again, we redirect (&gt;) the output to a file.\nWe can see what these numbers are by printing the content of the file:\ncat results/snp-sites/constant_sites.txt\n635254,561931,623994,624125\nAs we said earlier, these numbers represent the number of A, C, G, T that were constant in our original alignment. We will use these numbers in the tree inference step detailed next.\n\n\n12.3.3 Tree inference: iqtree\nThere are different methods for inferring phylogenetic trees from sequence alignments. Regardless of the method used, the objective is to construct a tree that represents the evolutionary relationships between different species or genetic sequences. Here, we will use the IQ-TREE software, which implements maximum likelihood methods of tree inference. Phylogenetic tree inference using maximum likelihood is done by identifying the tree that maximizes the likelihood of observing the given DNA sequences under a chosen evolutionary model.\nIn this process, DNA substitution models describe how DNA sequences change over time due to mutations. These models consider how frequently different bases (A, T, C, G) are replaced by each other. Another parameter these models can include is rate heterogeneity, which accounts for the fact that different DNA sites may evolve at different rates. Some sites might change rapidly, while others remain more stable.\nMaximum likelihood aims to find the tree topology and branch lengths that make the observed DNA sequences most probable, given the chosen model. It does this by exploring various tree shapes and lengths to calculate the likelihood of the observed sequences. The tree with the highest likelihood is considered the best representation of the evolutionary relationships among the sequences. The process involves making educated guesses about the tree’s parameters, calculating the likelihood of the data under these guesses, and refining the parameters iteratively to find the optimal tree that best explains the observed genetic variations.\nIQ-TREE offers various sequence evolution models, allowing researchers to match their analyses to different types of data and research questions. Conveniently, this software can identify the most fitting substituion model for a dataset (using a tool called ModelFinder), while considering the complexity of each model.\nWe run IQ-TREE on the output from SNP-sites, i.e. using the variable sites extracted from the core genome alignment:\n# create output directory\nmkdir results/iqtree\n\n# run iqtree2\niqtree -s results/snp-sites/core_gene_alignment_snps.aln -fconst 635254,561931,623994,624125 --prefix results/iqtree/awd -nt AUTO -m GTR+F+I\nThe options used are:\n\n-s - the input alignment file, in our case using only the variable sites extracted with snp-sites.\n--prefix - the name of the output files. This will be used to name all the files with a “prefix”. In this case we are using the “awd” prefix, which is very generic. In your own analysis you may want to use a more specific prefix (for example, the name of the collection batch).\n-fconst - these are the counts of invariant sites we estimated in the previous step with snp-sites (see previous section).\n-nt AUTO - automatically detect how many CPUs are available on the computer for parallel processing (quicker to run).\n-m - specifies the DNA substitution model we’d like to use. We give more details about this option below.\n\nWhen not specifying the -m option, iqtree employs ModelFinder to pinpoint the substitution model that best maximizes the data’s likelihood, as previously mentioned. Nevertheless, this can be time-consuming (as iqtree needs to fit trees numerous times). An alternative approach is utilizing a versatile model, like the one chosen here, “GTR+F+I,” which is a generalized time reversible (GTR) substitution model. This model requires an estimate of the base frequencies within the sample population, determined in this instance by tallying the base frequencies from the alignment (indicated by “+F” in the model name). Lastly, the model accommodates variations in rates across sites, including a portion of invariant sites (noted by “+I” in the model name).\nWe can look at the output folder:\nls results/iqtree\nawd.bionj  awd.ckp.gz  awd.iqtree  awd.log  awd.mldist  awd.model.gz  awd.treefile\nThere are several files with the following extension:\n\n.iqtree - a text file containing a report of the IQ-Tree run, including a representation of the tree in text format.\n.treefile - the estimated tree in NEWICK format. We can use this file with other programs, such as FigTree, to visualise our tree.\n.log - the log file containing the messages that were also printed on the screen.\n.bionj - the initial tree estimated by neighbour joining (NEWICK format).\n.mldist - the maximum likelihood distances between every pair of sequences.\n.ckp.gz - this is a “checkpoint” file, which IQ-Tree uses to resume a run in case it was interrupted (e.g. if you are estimating very large trees and your job fails half-way through).\n.model.gz - this is also a “checkpoint” file for the model testing step.\n\nThe main files of interest are the report file (.iqtree) and the tree file (.treefile) in standard Newick format.\n\n\n12.3.4 Visualising trees: FigTree\nThere are many programs that can be used to visualise phylogenetic trees. In this course we will use FigTree, which has a simple graphical user interface. You can open FigTree from the terminal by running the command figtree.\nTo open the tree:\n\nGo to File &gt; Open… and browse to the folder with the IQ-TREE output files.\nSelect the file with .treefile extension and click Open.\nYou will be presented with a visual representation of the tree.\n\nWe can also import a “tab-separated values” (TSV) file with annotations to add to the tree, if you have any available (e.g. country of origin, date of collection, etc.). To add annotations:\n\nGo to File &gt; Import annotations… and open the annotation file. This file has to be in tab-delimited format.\nOn the menu on the left, click Tip Labels and under “Display” choose one of the fields of our metadata table.\n\nThere are many ways to further configure the tree, including highlighting clades in the tree, and change the labels. See the Figure 12.4 for an example.\n\n\n\nFigure 12.4: Annotated phylogenetic tree obtained with FigTree. We identified a clade in the tree that corresponded to our samples, and used the “Highlight” function to give them a distinct colour (pink). We also highlighted the broader clade they are in (purple), which is composed of O1 biotype strains. To do this, change the “Selection Mode” at the top to “Clade”, then select the branch at the base of the clade you want to highlight, and press the “Highlight” button on the top to pick a colour."
  },
  {
    "objectID": "materials/03-typing/03-phylogeny.html#exercises",
    "href": "materials/03-typing/03-phylogeny.html#exercises",
    "title": "12  Building phylogenetic trees",
    "section": "12.4 Exercises",
    "text": "12.4 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data. You also need to have completed the genome assembly exercise in Section 8.4.\n\n\n\n\n\n\nPathogenwatch phylogeny\n\n\n\n\n\n\n\nFollowing from the Pathogenwatch exercise in Section 10.4, open the “Ambroise 2023” collection that you created and answer the following questions:\n\nDoes any of your sequences look like an outlier (i.e. a very long branch) in the sample tree view?\nChange the tree view to “Population”. Which transmission wave do your samples cluster with?\nLooking within the population(s) where your samples cluster in, do your samples cluster together or are they interspersed between other samples from the Pathogenwatch collection?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nLooking at our samples’ tree doesn’t reveal any sample as a particular outlier.\nLooking at the “Population” view (from the dropdown on the top-left, as shown below), we can see that all of the “Ambroise 2023” samples fall within the “W3_T10” clade. This is a recent transmission wave, confirming our strains are pathogenic and related to other recent strains (Weill et al. 2017).\n\nLooking inside the clade, we can see that our samples cluster somewhat apart from the rest, suggesting they are more similar to each other than they are with the samples from the Pathogenwatch collection. This might be because our samples are from 2023, whereas the collection from Pathogenwatch is from 2017, so it is likely that these strains have accumulated new mutations since then.\n\nNote that in the image above we changed our tree layout to a “circle”. Sometimes this view is helpful when we have too many sequences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCore genome alignment\n\n\n\n\n\n\n\nUsing Panaroo, perform a core genome alignment for your assembled sequences together with the public genomes we provide in resources/vibrio_genomes/.\n\nActivate the software environment: mamba activate phylogeny.\nFix the script we provide in scripts/05-panaroo.sh. See Section 12.3.1 if you need a hint of how to fix the code in the script.\nRun the script using bash scripts/05-panaroo.sh.\n\nWhen the analysis starts you will get several messages and progress bars print on the screen.\n This analysis takes a long time to run (several hours), so you can leave it running, open a new terminal and continue to the next exercise.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe fixed code for our script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/panaroo/\n\n# Run panaroo\npanaroo \\\n  --input results/assemblies/*.gff resources/vibrio_genomes/*.gff \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\nWe have specified two sets of input files:\n\nresults/assemblies/*.gff specifies all the GFF annotation files for our assembled genomes.\nresources/vibrio_genomes/*.gff specifies the GFF annotation files for the public genomes downloaded from NCBI.\n\nIn both cases we use the * wildcard to match all the files with .gff extension.\nAs it runs, Panaroo prints several messages to the screen. The analysis took 5h to run on our computers! It’s quite a long time, so it is advisable to run it on a high performance computing cluster, if you have one available. Otherwise, you will have to leave it running on your computer overnight.\nWe specified results/panaroo as the output file for the core genome aligner Panaroo. We can see all the output files it generated:\nls results/panaroo\naligned_gene_sequences                core_alignment_header.embl        gene_presence_absence_roary.csv\nalignment_entropy.csv                 core_gene_alignment.aln           pan_genome_reference.fa\ncombined_DNA_CDS.fasta                core_gene_alignment_filtered.aln  pre_filt_graph.gml\ncombined_protein_CDS.fasta            final_graph.gml                   struct_presence_absence.Rtab\ncombined_protein_cdhit_out.txt        gene_data.csv                     summary_statistics.txt\ncombined_protein_cdhit_out.txt.clstr  gene_presence_absence.Rtab\ncore_alignment_filtered_header.embl   gene_presence_absence.csv\nThe script included other code - which we didn’t need to fix - that extracted the variable sites from our alignment (as detailed in Section 12.3.2). These steps took as input the alignment file generated by Panaroo:\n# extract variable sites\nsnp-sites results/panaroo/core_gene_alignment.aln &gt; results/snp-sites/core_gene_alignment_snps.aln\n\n# count invariant sites\nsnp-sites -C results/panaroo/core_gene_alignment.aln &gt; results/snp-sites/constant_sites.txt\nThese two steps of the analysis produce the two files that we want to use in the next exercise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTree inference\n\n\n\n\n\n\n\n Because Panaroo takes a long time to run, we provide pre-processed results in the folder preprocessed/, which you can use as input to IQ-TREE in this exercise.\nProduce a tree from the core genome alignment from the previous step.\n\nActivate the software environment: mamba activate phylogeny.\nFix the script provided in scripts/06-iqtree.sh. See Section 12.3.3 if you need a hint of how to fix the code in the script.\nRun the script using bash scripts/06-iqtree.sh. Several messages will be printed on the screen while iqtree runs.\n\nOnce the IQ-TREE finishes running:\n\nLoad the generated tree into FigTree.\nImport the annotations we provide in resources/vibrio_genomes/public_genomes_metadata.tsv.\nYou can change the annotations shown on the tip labels from the menu panel on the left.\n\nAnswer the following questions:\n\nDo all your samples cluster together?\nAre they quite distinct (long branch lengths) compared to their closest public sequences?\nDo your samples cluster with the pathogenic O1 biotype?\nWhich transmission wave are your samples most closely related to?\nDo these results agree with the result you obtained on Pathogenwatch?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nFor IQ-TREE:\n\nThe constant sites can be obtained by looking at the output of the snp-sites in results/snp-sites/constant_sites.txt (or in the preprocessed folder if you are still waiting for you Panaroo analysis to finish).\nThe input alignment should be the output from the snp-sites program in results/snp-sites/core_gene_alignment_snps.aln (or in the preprocessed folder if you are still waiting for you Panaroo analysis to finish).\n\nFor FigTree:\n\nTo open a tree go to File &gt; Open… and load the .treefile generated by IQ-TREE.\nTo import sample metadata go to File &gt; Import annotations….\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/iqtree/\n\n# Run iqtree\niqtree -s results/snp-sites/core_gene_alignment_snps.aln -fconst 712485,634106,704469,704109 --prefix results/iqtree/ambroise -nt AUTO\n\nWe specify as input the core_gene_alignment_snps.aln produced in the previous exercise by running Panaroo followed by SNP-sites.\nWe specify the number of constant sites, also generated from the previous exercise. We ran cat results/snp-sites/constant_sites.txt to obtain these numbers.\nWe use as prefix for our output files “ambroise” (since we are using the “Ambroise 2023” data), so all the output file names will be named as such.\nWe automatically detect the number of threads/CPUs for parallel computation.\n\nAfter the analysis runs we get several output files in our directory:\nls results/iqtree/\nambroise.bionj  ambroise.ckp.gz  ambroise.iqtree  \nambroise.log    ambroise.mldist  ambroise.treefile\nThe main file of interest is ambroise.treefile, which contains our tree in the standard Newick format. We can load this tree into FigTree from File &gt; Open…. We also import the annotations provided for the public genomes from File &gt; Import Annotations….\nThe resulting tree file is shown in the image below:\n\nWe have highlighted the the clades showing our samples (pint) and the main clade they fall into (purple). We can see that:\n\nOur samples do cluster together, suggesting they are more similar to each other than to any other samples in the dataset.\nThey are quite distant from other public sequences in their clade. This could be true divergence if, for example, the public sequences were collected a long time ago and/or from distant locations or outbreaks (we have no information about this). Alternatively, this could be due to sequencing errors, in particular as we are working with an older chemistry of ONT data (R9.4.1 flowcells) with high sequencing error rates.\nUsing FigTree, we changed the tip labels to show the “biotype” of the public samples, which confirmed that the closest sequences to ours belong to O1 biotype.\nBy changing the tip labels to “clade” we can see the closest sequences are from “W3 T10” transmission event.\n\nThese results suggest that our samples are closely related to other “O1” strains, confirming their classification as also being that strain type. The results are also compatible with the analysis from Pathogenwatch."
  },
  {
    "objectID": "materials/03-typing/03-phylogeny.html#summary",
    "href": "materials/03-typing/03-phylogeny.html#summary",
    "title": "12  Building phylogenetic trees",
    "section": "12.5 Summary",
    "text": "12.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nPhylogenetic inference is a crucial tool in pathogen surveillance, enabling us to understand the relatedness of pathogen strains, trace transmission routes, and identify the sources of outbreaks.\nPhylogenetic tree analysis helps identify the closest relatives or lineages of your pathogen samples.\nFor Vibrio cholerae, it can crucially identify whether our samples belong to the highly transmissible and virulent 7PET lineage.\nA core genome alignment is essential for Vibrio cholerae due to its high genetic diversity, as it captures conserved genomic regions shared across strains and provides a robust basis for phylogenetic analysis.\nCreating a core genome alignment involves identifying shared genes across genomes and aligning these sequences. This task can be achieved using the panaroo software.\nA phylogenetic tree is constructed from a core genome alignment using algorithms such as maximum likelihood to infer the most likely relationship among strains. This task can be achieved using the iqtree software.\nTo reduce the computational burden of the analysis, we can extract variable sites from our alignment using the snp-sites software. Its output can be used with the iqtree mentioned above.\nPhylogenetic trees can be visualized and annotated using software like FigTree.\nPhylogenetic trees can serve as a valuable tool to assess assembly quality by revealing inconsistencies, such as unexpected placements of samples within the tree or high divergence relative to other samples from similar outbreaks. These may indicate potential sequencing errors affecting the accuracy of our assembly."
  },
  {
    "objectID": "materials/03-typing/04-amr.html#antimicrobial-resistance-amr-analysis",
    "href": "materials/03-typing/04-amr.html#antimicrobial-resistance-amr-analysis",
    "title": "13  AMR analysis",
    "section": "13.1 Antimicrobial Resistance (AMR) analysis",
    "text": "13.1 Antimicrobial Resistance (AMR) analysis\nAntimicrobial resistance (AMR) is a phenomenon where microorganisms, such as bacteria, evolve in a way that reduces the effectiveness of antimicrobial drugs, including antibiotics. This occurs due to the overuse and misuse of these drugs, which exerts selective pressure on the microorganisms. As a result, bacteria may develop or acquire genetic changes that enable them to survive exposure to antimicrobial agents, making the drugs less effective or entirely ineffective. AMR poses a significant global health threat, as it can lead to infections that are challenging to treat, potentially causing increased morbidity and mortality. Efforts to combat AMR include responsible antibiotic use, developing new drugs, and enhancing infection prevention and control measures.\n\n\n\nFigure 13.1: History of antibiotic resistance in Vibrio cholerae and associated resistance genes. These are often acquired through horizontal gene transfer of mobile genetic elements. Image: Fig. 1 in Das et al. (2020)\n\n\nAccording to the WHO, antimicrobial resistance (AMR) has evolved into a global concern for public health. This stems from various harmful bacterial strains developing resistance to antimicrobial medications, including antibiotics. As part of our analysis, we will now focus on identifying AMR patterns connected to our V. cholerae isolates.\nNumerous software tools have been created to predict the presence of genes linked to AMR in genome sequences. Estimating the function of a gene or protein solely from its sequence is complex, leading to varying outcomes across different software tools. It is advisable to employ multiple tools and compare their findings, thus increasing our confidence in identifying which antimicrobial drugs might be more effective for treating patients infected with the strains we’re studying.\nIn this section we will introduce a workflow aimed at combining the results from various AMR tools into a unified analysis. We will compare its results with AMR analysis performed by Pathogenwatch.\n\n\n\n\n\n\nWhat is Nextflow?\n\n\n\nNextflow is a program designed for building and running complex workflows (also known as pipelines). It simplifies the process of orchestrating complex computational pipelines that involve various tasks, inputs and outputs, and parallel processing. Nextflow is particularly well-suited for bioinformatics, where workflows often involve many steps, tools, and data transformations. It’s designed to work across different environments, including local machines, clusters, and cloud platforms.\nThere are many publicly-available Nextflow pipelines available, which users can take advantage of. In particular, nf-core is an community-driven, open-source project aimed at providing high-quality bioinformatics pipelines for a wide range of applications. The project’s goal is to offer standardized and well-documented workflows, allowing researchers to more easily set up and run complex analyses while following best practices and ensuring reproducibility.\nBy using nf-core pipelines, researchers can save time and effort that would otherwise be spent developing and fine-tuning their own analysis workflows."
  },
  {
    "objectID": "materials/03-typing/04-amr.html#sec-funcscan",
    "href": "materials/03-typing/04-amr.html#sec-funcscan",
    "title": "13  AMR analysis",
    "section": "13.2 Funcscan workflow",
    "text": "13.2 Funcscan workflow\nHere, we introduce an automated workflow called nf-core/funcscan (Figure 13.2), which uses Nextflow to manage all the software and analysis steps (see information box above). This pipeline uses five different AMR screening tools: ABRicate, AMRFinderPlus (NCBI Antimicrobial Resistance Gene Finder), fARGene (Fragmented Antibiotic Resistance Gene idENntifiEr), RGI (Resistance Gene Identifier), and DeepARG. This is convenient, as we can obtain the results from multiple approaches in one step.\n\n\n\nFigure 13.2: Overview of the nf-core/funcscan workflow. In our case we will run the “Antimicrobial Resistance Genes (ARGs)” analysis, shown in yellow. Image source: https://nf-co.re/funcscan/1.1.2\n\n\nThis pipeline requires us to prepare a samplesheet CSV file with information about the samples we want to analyse. Two columns are required:\n\nsample –&gt; a sample name of our choice (we will use the same name that we used for the assembly).\nfasta –&gt; the path to the FASTA file corresponding to that sample.\n\nYou can create this file using a spreadsheet software such as Excel, making sure to save the file as a CSV. Here is an example of our samplesheet, which we saved in a file called samplesheet_funcscan.csv:\nsample,fasta\nisolate01,results/assemblies/isolate01.fasta\nisolate02,results/assemblies/isolate02.fasta\nisolate03,results/assemblies/isolate03.fasta\nisolate04,results/assemblies/isolate04.fasta\nisolate05,results/assemblies/isolate05.fasta\nisolate06,results/assemblies/isolate06.fasta\nisolate07,results/assemblies/isolate07.fasta\nisolate08,results/assemblies/isolate08.fasta\nisolate09,results/assemblies/isolate09.fasta\nisolate10,results/assemblies/isolate10.fasta\nOnce we have the samplesheet ready, we can run the nf-core/funcscan workflow using the following commands:\n# activate the environment\nmamba activate nextflow\n\n# create output directory\nmkdir results/funcscan\n\n# run the pipeline\nnextflow run nf-core/funcscan -profile singularity \\\n  --max_memory 16.GB --max_cpus 8 \\\n  --input samplesheet_funcscan.csv \\\n  --outdir results/funcscan \\\n  --run_arg_screening \\\n  --arg_skip_deeparg\nThe options we used are:\n\n-profile singularity - indicates we want to use the Singularity program to manage all the software required by the pipeline (another option is to use docker). See Data & Setup for details about their installation.\n--max_memory and --max_cpus - sets the available RAM memory and CPUs. You can check this with the commands free -h and nproc --all, respectively.\n--input - the samplesheet with the input files, as explained above.\n--outdir - the output directory for the results.\n--run_arg_screening - indicates we want to run the “antimicrobial resistance gene screening tools”. There are also options to run antimicrobial peptide and biosynthetic gene cluster screening (see documentation).\n--arg_skip_deeparg - this skips a step in the analysis which uses the software DeepARG. We did this simply because this software takes a very long time to run. But in a real analysis you may want to leave this option on.\n\nWhile the pipeline runs, you will get a progress printed on the screen, and then a message once it finishes. Here is an example from our samples:\n[4a/77ac77] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_funcscan.csv) [100%] 1 of 1 ✔\n[-        ] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:GUNZIP_FASTA_PREP                                        -\n[97/6d505c] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:BIOAWK (isolate09)                                       [100%] 10 of 10 ✔\n[4f/aa117b] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:AMRFINDERPLUS_UPDATE (update)                        [100%] 1 of 1 ✔\n[9b/ba9bf2] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:AMRFINDERPLUS_RUN (isolate10)                        [100%] 10 of 10 ✔\n[15/0eaa8c] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_AMRFINDERPLUS (isolate10)              [100%] 10 of 10 ✔\n[d8/3ed91e] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:FARGENE (isolate10)                                  [100%] 100 of 100 ✔\n[2f/ab7c5c] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_FARGENE (isolate10)                    [100%] 102 of 102 ✔\n[4a/7116be] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:RGI_MAIN (isolate09)                                 [100%] 10 of 10 ✔\n[34/82b92c] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_RGI (isolate09)                        [100%] 10 of 10 ✔\n[5d/c349c5] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:ABRICATE_RUN (isolate09)                             [100%] 10 of 10 ✔\n[93/162a59] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_ABRICATE (isolate09)                   [100%] 10 of 10 ✔\n[79/725687] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_SUMMARIZE                              [100%] 1 of 1 ✔\n[4a/1f36ad] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:CUSTOM_DUMPSOFTWAREVERSIONS (1)                          [100%] 1 of 1 ✔\n[be/0d7355] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:MULTIQC                                                  [100%] 1 of 1 ✔\n-[nf-core/funcscan] Pipeline completed successfully-\nCompleted at: 10-Aug-2023 11:52:22\nDuration    : 34m 54s\nCPU hours   : 3.0\nSucceeded   : 277\n\n13.2.1 funcscan outputs\nThe main output of interest from this pipeline is a CSV file, which contains a summary of the results from all the AMR tools used This summary is produced by a software called hAMRonization and the corresponding CSV file is saved in results/funcscan/reports/hamronization_summarize/hamronization_combined_report.tsv. You can open this file using any standard spreadsheet software such as Excel (Figure 13.3).\nThis file is quite large, containing many columns and rows (we detail these columns in the information box below). The easiest way to query this table is to filter the table based on the column “antimicrobial_agent” to remove rows where no AMR gene was detected (Figure 13.3). This way you are left with only the results which were positive for the AMR analysis.\n\n\n\nFigure 13.3: To analyse the table output by hAMRonization in Excel you can go to “Data” –&gt; “Filter”. Then, select the dropdown button on the “antimicrobial_agent” column and untick the box “blank”. This will only show the genes associated with resistance to antimicrobial drugs.\n\n\n\n\n\n\n\n\nhAMRonization report columns (click to expand)\n\n\n\n\n\nTODO\n\n\n\nYou can also look at the detailed results of each individual tool, which can be found in the directory results/funcscan/arg. This directory contains sub-directories for each of the 5 AMR tools used (in our case only 4 folders, because we skipped the DeepARG step):\nls results/funcscan/arg\nabricate  amrfinderplus  fargene  hamronization  rgi\nFor each individual tool’s output folder shown above, there is a report, which is associated with the predicted AMRs for each of our samples. In most cases, the report is in tab-delimited TSV format, which can be opened in a standard spreadsheet software such as Excel. For instance, the AMR report from Abricate for one of our samples looks like this:\nless -S results/funcscan/arg/abricate/isolate02/isolate02.txt\n#FILE            SEQUENCE  START    END      STRAND  GENE      COVERAGE     COVERAGE_MAP     GAPS  %COVERAGE  %IDENTITY  DATABASE  ACCESSION    PRODUCT                                                         RESISTANCE\nisolate02.fasta  contig_2  1696     2623     -       blaPER-7  1-927/927    ========/======  1/1   100.00     99.89      ncbi      NG_049966.1  class A extended-spectrum beta-lactamase PER-7                  CEPHALOSPORIN\nisolate02.fasta  contig_2  4895     5738     -       sul1      1-840/840    ========/======  4/4   100.00     98.93      ncbi      NG_048091.1  sulfonamide-resistant dihydropteroate synthase Sul1             SULFONAMIDE\nisolate02.fasta  contig_2  6243     7036     -       aadA2     1-792/792    ========/======  2/2   100.00     99.50      ncbi      NG_047343.1  ANT(3'')-Ia family aminoglycoside nucleotidyltransferase AadA2  STREPTOMYCIN\nisolate02.fasta  contig_3  966452   967081   +       catB9     1-630/630    ===============  0/0   100.00     99.84      ncbi      NG_047621.1  type B-5 chloramphenicol O-acetyltransferase CatB9              CHLORAMPHENICOL\nisolate02.fasta  contig_4  778899   780023   +       varG      1-1125/1125  ===============  0/0   100.00     100.00     ncbi      NG_057468.1  VarG family subclass B1-like metallo-beta-lactamase             CARBAPENEM\nisolate02.fasta  contig_4  2573875  2574348  -       dfrA1     1-474/474    ===============  0/0   100.00     100.00     ncbi      NG_047676.1  trimethoprim-resistant dihydrofolate reductase DfrA1            TRIMETHOPRIM\nisolate02.fasta  contig_7  4178     5099     -       mph(A)    1-921/921    ========/======  1/1   100.00     99.35      ncbi      NG_047986.1  Mph(A) family macrolide 2'-phosphotransferase                   MACROLIDE\nisolate02.fasta  contig_7  6594     8069     +       msr(E)    1-1476/1476  ===============  0/0   100.00     100.00     ncbi      NG_048007.1  ABC-F type ribosomal protection protein Msr(E)                  MACROLIDE\nisolate02.fasta  contig_7  8125     9009     +       mph(E)    1-885/885    ===============  0/0   100.00     100.00     ncbi      NG_064660.1  Mph(E) family macrolide 2'-phosphotransferase                   MACROLIDE\nisolate02.fasta  contig_7  131405   132197   +       aadA2     1-792/792    ========/======  1/1   100.00     99.62      ncbi      NG_047343.1  ANT(3'')-Ia family aminoglycoside nucleotidyltransferase AadA2  STREPTOMYCIN\nFor this sample there were several putative AMR genes detected by Abricate, with their associated drugs. These genes were identified based on their similarity with annotated sequences from the NCBI database. For example, the gene varG was detected in our sample, matching the NCBI accession NG_057468.1. This is annotated as as a reference for antimicrobial resistance, in this case to the drug “CARBAPENEM”.\n\n\n\n\n\n\nCommand line trick \n\n\n\nHere is a trick using standard commands to count how many times each drug was identified by funcscan:\ncat results/funcscan/reports/hamronization_summarize/hamronization_combined_report.tsv | cut -f 10 | sort | uniq -c\n\ncat prints the content of the file\ncut extracts the 10th column from the file\nsort and uniq -c are used in combination to count unique output values\n\nThe result of the above command is:\n 9 CARBAPENEM\n 8 CEPHALOSPORIN\n 8 CHLORAMPHENICOL\n27 MACROLIDE\n13 QUATERNARY AMMONIUM\n10 STREPTOMYCIN\n 1 SULFONAMIDE\n10 TRIMETHOPRIM"
  },
  {
    "objectID": "materials/03-typing/04-amr.html#amr-with-pathogenwatch",
    "href": "materials/03-typing/04-amr.html#amr-with-pathogenwatch",
    "title": "13  AMR analysis",
    "section": "13.3 AMR with Pathogenwatch",
    "text": "13.3 AMR with Pathogenwatch\nPathogenwatch also performs AMR prediction using its own algorithm and curated gene sequences. The results from this analysis can be seen from the individual sample report, or summarised in the collection view.\n\n\n\nFigure 13.4: AMR analysis from Pathogenwatch. The summary table (top) can be accessed from the sample collections view, by selecting “Antibiotics” from the drop-down on the top-left. The table summarises resistance to a range of antibiotics (red = resistant; yellow = intermediate). More detailed results can be viewed for each individual sample by clicking on its name and opening the sample report (bottom)."
  },
  {
    "objectID": "materials/03-typing/04-amr.html#which-amr-do-my-isolates-have",
    "href": "materials/03-typing/04-amr.html#which-amr-do-my-isolates-have",
    "title": "13  AMR analysis",
    "section": "13.4 Which AMR do my isolates have?",
    "text": "13.4 Which AMR do my isolates have?\nAt this stage you may notice that different tools will give you a different answer to this question and it is therefore recommended to compare the results across multiple tools. For example, Pathogenwatch generally detects AMR for comparatively more antimicrobial drugs compared to the funcscan analysis. However, some of the drugs detected by funcscan were either not reported by Pathogenwatch (possibly because they are not part of its database) or have a disagreeing result.\nLet’s take a specific example. Pathogenwatch determined that none of our isolates were resistant to Streptomycin. However, in the hAMRonization summary table (output by funcscan) we can see that this drug was reported for several of our samples. Upon closer inspection, however, we can see that we only had partial matches to the reference NCBI sequence (WP_001206356.1), or in the case of one sample with a higher match the sequence identity was less than 100% (table below, showing some of the columns from the hAMRonization table).\ninput_file_name               gene_symbol   reference_accession   antimicrobial_agent   coverage_percentage   sequence_identity  \nisolate01.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate02.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate02.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate04.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate05.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate06.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate07.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate08.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate09.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          68.44                 100                \nisolate10.tsv.amrfinderplus   aadA2         WP_001206356.1        STREPTOMYCIN          100                   92.05              \nIt is also important to take into consideration our earlier assembly quality assessments as they may result in false negative results. For example, we can see that “isolate05” has the lowest AMR detection of all samples. However, this was the sample with the lowest genome coverage (only 21x) and with a resulting highly fragmented genome (229 fragments). Therefore, it is very possible that we missed parts of its genome during assembly, and that some of those contained AMR genes or plasmids.\nIn conclusion, always be critical of the analysis of your results at this stage, comparing the output from different tools as well as considering the quality of your assemblies. Ultimately, the safest way to assess AMR is with experimental validation, by testing those strains against the relevant antimicrobial agents in the lab. However, computational analysis such as what we did can help inform these experiments and treatment decisions."
  },
  {
    "objectID": "materials/03-typing/04-amr.html#exercises",
    "href": "materials/03-typing/04-amr.html#exercises",
    "title": "13  AMR analysis",
    "section": "13.5 Exercises",
    "text": "13.5 Exercises\n For these exercises, you can either use the dataset we provide in Data & Setup, or your own data. You also need to have completed the genome assembly exercise in Section 8.4.\n\n\n\n\n\n\nFuncscan workflow\n\n\n\n\n\n\n\nRun the nf-core/funcscan workflow on the assembled genomes for your samples.\n\nUsing Excel, create a samplesheet CSV file for your samples, required as input for this pipeline. See Section 13.2 if you need to revise the format of this samplesheet.\nActivate the software environment: mamba activate nextflow.\nFix the script provided in scripts/07-amr.sh.\nRun the script using bash scripts/07-amr.sh.\n\nOnce the workflow is running it will print a progress message on the screen. This will take a while to run, so you can do the next exercise, and then continue with this one.\nOnce the analysis finishes, open the output file results/funcscan/reports/hamronization_summarize/hamronization_combined_report.tsv in Excel. Answer the following questions:\n\nDid this analysis find evidence for antimicrobial resistance to any drugs?\nDid all your samples show evidence for AMR?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nWe created a samplesheet for our samples in Excel, making sure to “Save As…” CSV format. The raw file looks like this:\nsample,fasta\nCTMA_1402,results/assemblies/CTMA_1402.fasta\nCTMA_1421,results/assemblies/CTMA_1421.fasta\nCTMA_1427,results/assemblies/CTMA_1427.fasta\nCTMA_1432,results/assemblies/CTMA_1432.fasta\nCTMA_1473,results/assemblies/CTMA_1473.fasta\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir results/funcscan\n\n# run the pipeline\nnextflow run nf-core/funcscan -profile singularity \\\n  --max_memory 16.GB --max_cpus 8 \\\n  --input samplesheet_funcscan.csv \\\n  --outdir results/funcscan \\\n  --run_arg_screening \\\n  --arg_skip_deeparg\nWhile the script was running we got a progress of the analysis printed on the screen. Once it finished we got a message like this (yours will look slightly different):\nCompleted at: 12-Aug-2023 12:24:03\nDuration    : 44m 54s\nCPU hours   : 3.0\nSucceeded   : 277\nWe opened the hAMRonization output report file in Excel and filtered it for the column “antimicrobial_agent”. We identified the following (only a few columns shown for simplicity):\ninput_file_name              gene_symbol  antimicrobial_agent\nCTMA_1402.tsv.amrfinderplus  aph(6)-Id    STREPTOMYCIN\nCTMA_1402.tsv.amrfinderplus  catB9        CHLORAMPHENICOL\nCTMA_1402.tsv.amrfinderplus  dfrA1        TRIMETHOPRIM\nCTMA_1402.tsv.amrfinderplus  floR         CHLORAMPHENICOL/FLORFENICOL\nCTMA_1402.tsv.amrfinderplus  sul2         SULFONAMIDE\nCTMA_1402.tsv.amrfinderplus  varG         CARBAPENEM\nCTMA_1421.tsv.amrfinderplus  aph(3'')-Ib  STREPTOMYCIN\nCTMA_1421.tsv.amrfinderplus  aph(6)-Id    STREPTOMYCIN\nCTMA_1421.tsv.amrfinderplus  catB9        CHLORAMPHENICOL\nCTMA_1421.tsv.amrfinderplus  dfrA1        TRIMETHOPRIM\nCTMA_1421.tsv.amrfinderplus  floR         CHLORAMPHENICOL/FLORFENICOL\nCTMA_1421.tsv.amrfinderplus  sul2         SULFONAMIDE\nCTMA_1421.tsv.amrfinderplus  varG         CARBAPENEM\nCTMA_1427.tsv.amrfinderplus  aph(3'')-Ib  STREPTOMYCIN\nCTMA_1427.tsv.amrfinderplus  aph(6)-Id    STREPTOMYCIN\nCTMA_1427.tsv.amrfinderplus  catB9        CHLORAMPHENICOL\nCTMA_1427.tsv.amrfinderplus  dfrA1        TRIMETHOPRIM\nCTMA_1427.tsv.amrfinderplus  floR         CHLORAMPHENICOL/FLORFENICOL\nCTMA_1427.tsv.amrfinderplus  sul2         SULFONAMIDE\nCTMA_1427.tsv.amrfinderplus  varG         CARBAPENEM\nCTMA_1432.tsv.amrfinderplus  aph(3'')-Ib  STREPTOMYCIN\nCTMA_1432.tsv.amrfinderplus  aph(6)-Id    STREPTOMYCIN\nCTMA_1432.tsv.amrfinderplus  catB9        CHLORAMPHENICOL\nCTMA_1432.tsv.amrfinderplus  dfrA1        TRIMETHOPRIM\nCTMA_1432.tsv.amrfinderplus  floR         CHLORAMPHENICOL/FLORFENICOL\nCTMA_1432.tsv.amrfinderplus  sul2         SULFONAMIDE\nCTMA_1432.tsv.amrfinderplus  varG         CARBAPENEM\nCTMA_1473.tsv.amrfinderplus  aph(3'')-Ib  STREPTOMYCIN\nCTMA_1473.tsv.amrfinderplus  aph(6)-Id    STREPTOMYCIN\nCTMA_1473.tsv.amrfinderplus  catB9        CHLORAMPHENICOL\nCTMA_1473.tsv.amrfinderplus  dfrA1        TRIMETHOPRIM\nCTMA_1473.tsv.amrfinderplus  floR         CHLORAMPHENICOL/FLORFENICOL\nCTMA_1473.tsv.amrfinderplus  sul2         SULFONAMIDE\nCTMA_1473.tsv.amrfinderplus  varG         CARBAPENEM\nAll 5 of our samples show evidence of AMR to different antimicrobial drugs. All of them are quite similar, with resistance to a similar set of drugs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAMR with Pathogenwatch\n\n\n\n\n\n\n\nFollowing from the Pathogenwatch exercise in Section 10.4, open the “Ambroise 2023” collection that you created and answer the following questions:\n\nOpen the antibiotics summary table.\nDo all your samples have evidence for antibiotic resistance?\nIf any samples have resistance to much fewer antibiotics compared to the others, do you think this could be related to assembly quality?\nHow do the results from Pathogenwatch compare to those from nf-core/funcscan?\n\nHow do the results compare with Pathogenwatch?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nWe can open the “Antibiotics” table from the top-left dropdown menu, as shown in the image below.\n\nWe can see that Pathogenwatch identified resistance to several antibiotics. All samples are similar, except “1432” doesn’t have resistance to furazolidone and nitrofurantoin. This sample had high completeness, according to CheckM2. However, it was also the sample with the lowest sequencing coverage (from our analysis during the genome assembly in Section 8.4), so this could be a “false negative” result due to the inability to cover some of the genes that confer AMR.\nAll of the drugs identified by funcscan were also identified by Pathogenwatch (note that sulfamethoxazole and sulfisoxazole identified by Pathogenwatch are both sulfonamide-derived drugs, reported by funcscan). However, Pathogenwatch identified resistance to several other drugs: ampicilin, ceftazidime, cephalosporins, ceftriazone, cefepime, furazolidone and nitrofurantoin."
  },
  {
    "objectID": "materials/03-typing/04-amr.html#summary",
    "href": "materials/03-typing/04-amr.html#summary",
    "title": "13  AMR analysis",
    "section": "13.6 Summary",
    "text": "13.6 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nAMR poses significant global public health threats by diminishing the effectiveness of antibiotics, making it challenging to treat infectious diseases effectively.\nAMR software aims to identify specific genes or mutations known to confer resistance to antimicrobial agents. These tools compare input genetic sequences to known resistance genes or patterns in their associated databases.\nThe nf-core/funcscan workflow performs AMR analysis using several software tools and producing a summary of their results as a CSV file.\nPathogenwatch is a more user-friendly application, which performs AMR using its own curated database.\nAMR prediction can result in false results (either false positives or false negatives). One way to overcome this limitation is to compare the results from multiple tools and, whenever possible, complement it with validation assays in the lab."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#genomic-surveillance-report",
    "href": "materials/04-report/01-reporting.html#genomic-surveillance-report",
    "title": "14  Reporting",
    "section": "14.1 Genomic surveillance report",
    "text": "14.1 Genomic surveillance report\nAfter completing your analysis, it is advisable to write a report summarising your main findings. This report is intended to provide public health officials with well-informed insights for decisions concerning strain identification, outbreak source, transmission, control, mitigation measures, and treatment strategies for diseases like cholera or other infectious diseases you might be facing.\nIn this chapter, we give an outline of the report’s essential sections. We took inspiration from a published report of a cholera outbreak in Zambia in 2023. We highly recommend that you also read that report to learn from it.\n\n\n\n\n\n\nReport template\n\n\n\nWe provide a report template as a shared GDoc, which you can download and adapt for your own analysis. However, we are not accredited public health officials, so please do not consider our template as an “official” document."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#background",
    "href": "materials/04-report/01-reporting.html#background",
    "title": "14  Reporting",
    "section": "14.2 Background",
    "text": "14.2 Background\nThis segment offers a contextual backdrop for the report:\n\nThe current knowledge about the disease from sources such as the scientific literature and organisations such as the World Health Organisation, Centers for Disease Control and Prevention, amongst others.\nDetails about the outbreak seen in the region, answering questions such as:\n\nWhen and where was the first case reported?\nDoes this outbreak follow other outbreaks in geographically close areas?\nHow many cases have been reported so far?\nHave any treatments been used or any other preemptive measures put in place in the affected areas?\n\nHow Whole Genome Sequencing (WGS) was integrated into the disease surveillance.\n\nEnd this section by summarising the main questions that your report will address, such as:\n\nAre the acute watery diarrhoea cases in the region caused by Vibrio cholerae?\nAre they pathogenic strains of Vibrio cholerae (7PET lineage)?\nAre they similar to previously sequenced strains in the region?\nDo these strains carry AMR genes and, if so, for which antimicrobial agents?"
  },
  {
    "objectID": "materials/04-report/01-reporting.html#methods",
    "href": "materials/04-report/01-reporting.html#methods",
    "title": "14  Reporting",
    "section": "14.3 Methods",
    "text": "14.3 Methods\nThis section should describe the methodologies used from sample collection to analysis. This section should address the following:\n\nWhen (date, time), where (location, GPS coordinates) and how were the samples collected from the patients?\nWhat kind of disease symptoms did the patients show?\nHow were the samples processed in the lab - were they grown in plates for isolating colonies, or did you use a metagenomic approach?\nWere the strains characterised in the lab using assays such as serotyping and antibiotic susceptibility?\nHow was the DNA extracted and sequencing libraries prepared? Detail the approach used (e.g. whole genome sequencing, amplicon-based, metagenomic) and details about kits and reagents used.\nWhat platform was used for sequencing (ONT, Illumina, PacBio)? Give details about the instrument models and any software versions in those platforms.\nFor ONT data, how was basecalling performed, including the Guppy version used and the basecalling mode (fast, high accuracy, super high accuracy)?\nHow was the bioinformatic analysis performed? This should include the names of all the software used and their versions.\n\nFor bioinformatic analysis, which is the focus of these materials, we give a detailed description of the analysis covered in previous chapters in our template document."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#results",
    "href": "materials/04-report/01-reporting.html#results",
    "title": "14  Reporting",
    "section": "14.4 Results",
    "text": "14.4 Results\nThis is where you provide the main results of your analysis. This is the most crucial section of your report, as its content may inform subsequent decision-making by public health professionals. It covers essential results relevant to public health, including antimicrobial resistance (AMR), transmission source, and typing. The outputs from all the analysis tools we used in previous chapters contribute to this section. Inclusion of tables and plots from individual tool outputs is recommended.\nFor the bioinformatics component in particular, you can include several sub-sections within the results:\n\nAssembly quality - the table output by our assembly script, as well as the results from the Mash screening (for example, were any unexpected contaminants found?).\nMultilocus sequence typing - the results from MLST analysis and importantly whether your strains are pathogenic (O1 El Tor or related). Also report whether the cholera toxin genes ctxA and ctxB are present in your assemblies.\nPhylogenetic relatedness - a phylogenetic tree showing how your sequences relate to other sequences collected in the region or other public sequences.\nAntimicrobial resistance - the results from AMR analysis including results from multiple tools and which antibiotic drugs your strains are likely to be resistant to."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#references",
    "href": "materials/04-report/01-reporting.html#references",
    "title": "14  Reporting",
    "section": "14.5 References",
    "text": "14.5 References\nThe report concludes with citations from relevant literature sources utilized in its preparation."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#publishing",
    "href": "materials/04-report/01-reporting.html#publishing",
    "title": "14  Reporting",
    "section": "14.6 Publishing",
    "text": "14.6 Publishing\nAlthough reports may often be used for sharing with public health officials, you should also consider publishing and sharing your genomes. You can make a small publication, even including a single genome. Every piece of information helps when dealing with pathogens that cause pandemic outbreaks.\nAs inspiration, here are some examples of small “announcement” publications:\n\nMalki et al. 2021 reporting seven genomes from patients in Qatar.\nOsama et al. 2012 reporting a single genome from a patient in Malaysia.\nThompson et al. 2011 reporting a single genome from an isolate collected during an outbreak in Brazil in 1991."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#exercises",
    "href": "materials/04-report/01-reporting.html#exercises",
    "title": "14  Reporting",
    "section": "14.7 Exercises",
    "text": "14.7 Exercises\n\n\n\n\n\n\nWriting a report\n\n\n\n\n\n\n\nDownload a copy of the report template to your computer (File &gt; Download &gt; Microsoft Word).\nWe already drafted some key points in the “Background” and “Methods” section. Read these carefully to understand what you should include in your own reports.\nComplete the results section with your own results (tasks highlighted in yellow).\nDiscuss and compare the results with your colleagues."
  },
  {
    "objectID": "materials/04-report/01-reporting.html#summary",
    "href": "materials/04-report/01-reporting.html#summary",
    "title": "14  Reporting",
    "section": "14.8 Summary",
    "text": "14.8 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nKey questions to address in a genomic surveillance report include:\n\nWhich pathogen was identified in your isolates?\nWas it a pathogenic strain?\nWhat is the relatedness among your samples and with other samples from the region?\nDo they carry potential antimicrobial resistance?\n\nTypical sections to include in a report are: introduction, methods, results, and conclusions. Using a clear and concise style is recommended.\nInclude as much contextual information about your samples as possible, such as when, where and how the samples were collected.\nInclude tables and figures from your bioinformatic analysis, including: assembly quality metrics, sequence typing, phylogenetic trees and detected AMR genes.\nWhenever possible, publish your findings as a small paper, as this helps improve databases and services such as Pathogenwatch."
  },
  {
    "objectID": "materials/appendices/01-file_formats.html",
    "href": "materials/appendices/01-file_formats.html",
    "title": "Appendix A — Common file formats",
    "section": "",
    "text": "This page lists some common file formats used in Bioinformatics (listed alphabetically). The heading of each file links to a page with more details about each format.\nGenerally, files can be classified into two categories: text files and binary files.\n\nText files can be opened with standard text editors, and manipulated using command-line tools (such as head, less, grep, cat, etc.). However, many of the standard files listed in this page can be opened with specific software that displays their content in a more user-friendly way. For example, the NEWICK format is used to store phylogenetic trees and, although it can be opened in a text editor, it is better used with a software such as FigTree to visualise the tree as a graph.\nBinary files are often used to store data more efficiently. Typically, specific tools need to be used with those files. For example, the BAM format is used to store sequences aligned to a reference genome and can be manipulated with dedicated software such as samtools.\n\nVery often, text files may be compressed to save storage space. A common compression format used in bioinformatics is gzip with has extension .gz. Many bioinformatic tools support compressed files. For example, FASTQ files (used to store NGS sequencing data) are often compressed with format .fq.gz.\n\nBAM (“Binary Alignment Map”)\n\nBinary file.\nSame as a SAM file but compressed in binary form.\nFile extensions: .bam\n\n\n\nBED (“Browser Extensible Data”)\n\nText file.\nStores coordinates of genomic regions.\nFile extension: .bed\n\n\n\nCSV (“Comma Separated Values”)\n\nText file.\nStores tabular data in a text file. (also see TSV format)\nFile extensions: .csv\n\nThese files can be opened with spreadsheet programs (such as Microsoft Excel). They can also be created from spreadsheet programs by going to File &gt; Save As… and select “CSV (Comma delimited)” as the file format.\n\n\nFAST5\n\nBinary file. More specifically, this is a Hierarchical Data Format (HDF5) file.\nUsed by Nanopore platforms to store the called sequences (in FASTQ format) as well as the raw electrical signal data from the pore.\nFile extensions: .fast5\n\n\n\nFASTA\n\nText file.\nStores nucleotide or amino acid sequences.\nFile extensions: .fa or .fas or .fasta\n\n\n\nFASTQ\n\nText file, but often compressed with gzip.\nStores sequences and their quality scores.\nFile extensions: .fq or .fastq (compressed as .fq.gz or .fastq.gz)\n\n\n\nGFF (“General Feature Format”)\n\nText file.\nStores gene coordinates and other features.\nFile extension: .gff\n\n\n\nNEWICK\n\nText file.\nStores phylogenetic trees including nodes names and edge lengths.\nFile extensions: .tree or .treefile\n\n\n\nSAM (“Sequence Alignment Map”)\n\nText file.\nStores sequences aligned to a reference genome. (also see BAM format)\nFile extensions: .sam\n\n\n\nTSV (“Tab-Separated Values”)\n\nText file.\nStores tabular data in a text file. (also see CSV format)\nFile extensions: .tsv or .txt\n\nThese files can be opened with spreadsheet programs (such as Microsoft Excel). They can also be created from spreadsheet programs by going to File &gt; Save As… and select “Text (Tab delimited)” as the file format.\n\n\nVCF (“Variant Calling Format”)\n\nText file but often compressed with gzip.\nStores SNP/Indel variants\nFile extension: .vcf (or compressed as .vcf.gz)"
  }
]